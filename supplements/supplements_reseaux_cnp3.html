<!DOCTYPE html>
<html lang="fr">
    <head>
        <meta charset="UTF-8">
        <meta name="author" content="Choquet Olivier & Vandenheede Driss">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>CNP3 - Réseaux - Suppléments - e-Syllabus HTML5 et CSS3</title>
        <!-- une icône dans la barre du navigateur -->
        <link rel="icon" href="../images/favicon.png" type="image/png">
        <link rel="stylesheet" href="../style.css">
    </head>
    <body>
        <!-- liens d'évitement -->
        <ul class="a11y-nav">
            <li>
                <a href="#container">Passer directement au contenu</a>
            </li>
            <li>
                <a href="#menu">Accéder au menu secondaire de la page</a>
            </li>
            <li>
                <a href="#aside">Accéder aux liens utiles</a>
            </li>
        </ul>
        <!-- bannière et menu -->
        <header role="banner">
            <div class="logoTitle">
                <h1>Cours d'introduction au Web</h1>
                <h2>HTML5 et CSS3</h2>
            </div>
            <nav role="navigation" aria-label="Menu principal de navigation">
                <ul>
                    <li><a href="../index.html">Le cours</a></li>
                    <li><a href="../internet.html">Internet</a></li>
                    <li><a href="../html5.html">HTML5</a></li>
                    <li><a href="../css3.html">CSS3</a></li>
                    <li><a href="../methodologie.html">Méthodologie</a></li>
                    <li><a href="../lexique.html">Lexique</a></li>
                    <li><a href="../supplements.html" class="current">Suppléments</a></li>
                </ul>
            </nav>
        </header>
        <main id="container" role="main" tabindex="-1">
            <div id="menu">
                <nav role="navigation">
                    <ol>
                        <li><a href="../supplements_reseaux.html">Retour au sommaire</a></li>
                        <li><a href="#preface">Préface</a></li>
                        <li><a href="#introduction">Introduction</a></li>
                        <li><a href="#couche_application">La couche application</a></li>
                        <li><a href="#couche_transport">La couche transport</a></li>
                        <li><a href="#couche_reseau">La couche réseau</a></li>
                        <li><a href="#couche_liaison_donnees_LAN">La couche de liaison de données et les LAN</a></li>
                        <li><a href="#annexes">Annexes</a></li>
                    </ol>
                </nav>
                <!-- aside lié à la page et pas à l'article donc pas à l'intérieur de article -->
                <aside id="aside">
                    <p>Liens utiles :</p>
                    <ul>
                        <li><a href="https://www.youtube.com/c/LeDesignerduWeb/videos">Le Designer du web</a></li>
                    </ul>
                </aside>
            </div>
            <section id="mainSection">
                <article>
                    <h1>Réseaux informatiques : principes, protocoles et pratiques version 0.25 :</h1>
                    <p>Ce texte est traduit du livre "Computer Networking : Principles, Protocols and Practice Release 0.25" et écrit par Olivier Bonaventure.</p>
                    <p><a href="../files/cnp3.pdf">Le texte original se trouve ici sous forme PDF et en anglais.</a></p>
                    <img src="../images/cnp3.jpg" alt="">
                </article>
                <article>
                    <h2 id="preface">Préface :</h2>
                    <p>Ce manuel est né d'une frustration de son auteur principal. De nombreux auteurs ont choisi d'écrire un manuel parce qu'il n'y a pas de manuels dans leur domaine ou parce qu'ils ne sont pas satisfaits des manuels existants. Cette frustration a produit plusieurs excellents manuels dans la communauté des réseaux. À une époque où les manuels de réseaux étaient principalement théoriques, Douglas Corner a choisi d'écrire un manuel entièrement axé sur la suite de protocoles TCP/IP [Comer1988], un choix difficile à l'époque. Plus tard, il a étendu son manuel en décrivant une implémentation complète de TCP/IP, ajoutent des considérations pratiques aux descriptions théoriques de [Comer1988]. Richard Stevens a abordé Internet comme un explorateur et a expliqué le fonctionnement des protocoles en examinant tous les paquets échangés sur le fil [Stevens1994]. Jim Kurose et Keith Ross ont réinventé les manuels de réseaux en partant des applications utilisées par les étudiants et ont ensuite expliqué les protocoles Internet en supprimant une couche après l'autre [KuroseRoss09].</p>
                    <p>Les frustrations qui ont motivé ce livre sont différentes. Lorsque j'ai commencé à enseigner les réseaux à la fin des années 1990, les étudiants étaient déno des utilisateurs d'Internet, mais leur utilisation était limitée. Les étudiants utilisaient encore des manuels de référence et passaient du temps à la bibliothèques. Les étudiants d'aujourd'hui sont complètement différents. Ce sont des utilisateurs du web avides et expérimentés qui trouvent beaucoup d'informations sur le web. C'est une attitude positive car ils sont probablement plus curieux que leurs prédécesseurs. Grâce aux informations disponibles sur Internet, ils peuvent vérifier ou obtenir des informations supplémentaires sur les sujets expliqués par leurs professeurs. Cette abondance d'informations crée plusieurs défis pour un enseignant. Jusqu'à la fin du XIXe siècle, un enseignant par définition plus compétent que ses étudiants et il était très difficile pour les étudiants de vérifier les leçons données par leurs enseignants. Aujourd'hui, étant donné la quantité d'informations disponibles au bout des doigts de chaque étudiant via Internet, la vérification d'une leçon ou l'obtention de plus d'informations sur un sujet donné est parfois à quelques clics seulement. Des sites web tels que Wikipédia fournissent beaucoup d'informations sur divers sujets et les étudiants les consultent souvent. Malheureusement, l'organisation des informations sur ces sites web n'est pas adaptée pour permettre aux étudiants d'apprendre à partir d'eux. De plus, il existe de grandes différences dans la qualité et la profondeur des informations disponibles pour différents sujets.</p>
                    <p>La deuxième raison est que la communauté du réseau informatique est un participant fort dans le mouvement open-source. Aujourd'hui, il existe des implémentations open-source de haute qualité et largement utilisées pour la plupart des protocoles réseau. Cela inclut les implémentations TCP/IP qui font partie de Linux, FreeBSD ou la pile uIP fonctionnant sur des contrôleurs 8 bits, mais aussi des serveurs tels que Bind, Unbound, Apache ou Sendmail, ainsi que des implémentations de procoles de routage tels que XORP ou Quagga. De plus, les documents qui définissent presque tous les protocoles Internet ont été développés au sein de l'Internet Engineering Task Force (IETF) en utilisant un processus ouvert. L'IETF publie ses spécifications de protocole dans les RFC disponibles publiquement et les nouvelles propositions sont décrites dans les projets Internet.</p>
                    <p>Cet ouvrage pédagogique vise à combler le fossé entre les implémentations open source et les spécifications open source des réseaux en fournissant un description détaillée mais pédagogique des principes clés qui guident le fonctionnement d'Internet. Le livre est publié sous une licence Creative Commons. Cette licence open source est motivée par deux raisons. La première est que nous espérons que cela permettra à de nombreux étudiants d'utiliser le livre pour apprendre les réseaux informatiques. La seconde est que j'espère que d'autres enseignants le réutiliseront, l'adapteront et l'amélioreront. Le temps dira s'il est possible de construire une communauté de contributeurs pour améliorer et développer davantage le livre. En tant que point de départ, la première édition contient tout le matériel pour un semestre de premier cycle supérieur ou de cours de réseau des cycles supérieurs.</p>
                    <p>À l'heure actuelle où ces lignes sont écrites, la plupart du texte a été rédigé par Olivier Bonaventure. Laurent Vanbever, Virginie Van den Schriek, Damien Saucez et Mickael Hoerdt ont contribué aux exercices. Pierre Reinbold a conçu les icônes utilisées pour représenter les commutateurs et Nipaul Long a redessiné de nombreuses figures au format SVG. Stéphane Bortzmeyer a envoyé de nombreuses suggestions et corrections au texte. Des informations supplémentaires sur le manuel sont disponsibles sur le site <a href="http://inl.info.ucl.ac.be/CNP3" target="_blank">http://inl.info.ucl.ac.be/CNP3</a>.</p>
                    <h3>1.1. À propos de l'auteur :</h3>
                    <p>Olivier Bonaventure est actuellement professeur à l'Univesité Catholique de Louvain (Belgique) où il dirige le laboratoire IP Networking et est vice-président de l'ICTEAM Institute. Sa recherche est axée sur les protocoles Internet depuis plus de vingt ans. Avec ses doctorants, il a développé des techniques d'ingénierie de trafic, effectué divers types de mesures Internet, amélioré les performances des protocoles de routage tels que BGP est IS-IS et participé au développement de nouveaux protocoles Internet, notamment shim6, LISP et Multipath TCP. Il contribue fréquemment à la normalisation au sein de l'IETF. Il était membre du commité de rédaction de la revue IEEE/ACM Transactions on Networking et est directeur de ACM SIGCOMM.</p>
                </article>
                <article>
                    <h2 id="introduction">Partie 1 : Introduction :</h2>
                    <h3>2.1. Introduction :</h3>
                    <p>Lorsque les premiers ordinateurs ont été construits pendant la Seconde Guerre mondiale, ils étaient coûteux et isolés. Cependant, après environ 20 ans, à mesure que leurs prix diminuaient progressivement, les premières expériences ont commencé à connecter des ordinateurs entre eux. Au début des années 1960, des chercheurs tels que Paul Baran, Donald Davies ou Joseph Licklider ont publié indépendamment les premiers articles décrivant l'idée de construire des réseaux informatiques [Baran] [Licklider1963]. Étant donné le coût des ordinateurs, le partage sur une longue distance était une idée intéressante. Aux États-Unis, l'ARPANET a démarré en 1960 et s'est poursuivi jusqu'au milieu des années 1980 [LCCD09]. En France, Louis Pouzin a développé le réseau Cyclades [Pouzin1975]. De nombreux autres réseaux de recherche ont été construits au cours des années 1970 [Moore]. En même temps, l'industrie des télécommunications et des ordinateurs s'est intéressée aux réseaux informatiques. L'industrie des télécommunications a misé sur le X25. L'industrie informatique a adopté une approche complètement différente en concevant des réseaux locaux (LAN). De nombreuses technologies LAN telles qu'Ethernet ou Token Ring ont été conçues à cette époque. Au cours des années 1980, la nécessité d'interconnecter de plus en plus d'ordinateurs a conduit la plupart des vendeurs d'ordinateurs à développer leur propre suite de protocoles de réseau. Xerox a développé [XNS], DEC a choisi DECNet [Malamud1991], IBM a développé SNA [McFadyen1976], Microsoft a introduit NetBIOS [Winston2003], Apple a misé sur Appletalk [SAO1990]. Dans la communauté de recherche, l'ARPANET a été mise hors service et remplacée par TCP/IP [LCCD09] et l'implémentation de référence a été développée à l'intérieur de BSD Unix [McKusick1999]. Les universités qui utilisaient déjà Unix ont ainsi pu adopter facilement TCP/IP et les vendeurs de stations de travail Unix tels que Sun ou Silicon Graphics ont inclus TCP/IP dans leur variante d'Unix. En parallèle, l'ISO, avec le soutien des gouvernements, a travaillé sur le développement d'une suite ouverte de protocoles réseau ("Open" dans les termes de l'ISO était en contraste avec les suites de protocoles propriétaires dont les spécifications n'étaient pas toujours disponibles publiquement. Le gouvernement américain a même ordonnée l'utilisation des protocoles OSI (voir RFC 1169), mais cela n'a pas suffi à encourager tous les utilisateurs à passer à la suite de protocoles OSI qui était considérée par beaucoup comme trop complexe par rapport à d'autres suites de protocoles.). Au final, TCP/IP est devenu la norme de facto qui n'est pas seulement utilisée au sein de la communauté de recherche. Au cours des années 1990 et du début des années 2000, la croissance de l'utilisation TCP/IP a continué, et aujourd'hui, les protocoles propriétaires sont rarement utilisés. Comme le montre la figure ci-dessous, qui fournit une estimation du nombre d'hôtes connectés à Internet, Internet a connu une forte croissance au cours des 20 dernières années.</p>
                    <p>Les estimations récentes du nombre de machines connectées à Internet montrent une croissance continue depuis plus de 20 ans. Cependant, bien que le nombre de machines connectées à Internet soit élevé, il devrait être comparé au nombre de téléphones mobiles en utilisation aujourd'hui. De plus en plus de ces téléphones mobiles seront connectés à Internet. De plus, grâce à la disponibilité des implémentations de TCP/IP nécessitant des ressources limitées telles que uIP [Dunkels2003], nous pouvons nous attendre à voir une croissance des appareils intégrés compatibles avec TCP/IP.</p>
                    <p>Avant d'examiner les services fournis par les réseaux informatiques, il est utile de se mettre d'acoord sur certains termes largement utilisés dans la littérature sur les réseaux. Tout d'abord, les réseaux informatiques sont souvent classés en fonction de la zone géographique qu'ils couvrent :</p>
                    <ul>
                        <li>
                            <p><span class="html">LAN</span> (Local Area Network) : un réseau local interconnecte généralement des hôtes qui sont quelques kilomètres ou peut-être quelques dizaines de kilomètres de distance.</p>
                        </li>
                        <li>
                            <p><span class="html">MAN</span> (Metropolitan Area Network) : un réseau métropolitain interconnecte généralement des appareils qui sont jusqu'à quelques centaines de kilomètre de distance.</p>
                            <figure>
                                <img src="../images/estimation_nombre_hotes_sur_internet.png" alt="">
                                <figcaption>Figure 2.1 : Estimation du nombre d'hôtes sur Internet</figcaption>
                            </figure>
                            <figure>
                                <img src="../images/estimation_nombre_telephones_mobiles.png" alt="">
                                <figcaption>Figure 2.2 : Estimation du nombre de téléphones mobiles</figcaption>
                            </figure>
                        </li>
                        <li>
                            <p><span class="html">WAN</span> (Wide Area Network) : un réseau étendu interconnecte des hôtes qui peuvent être situés n'importe où sur Terre.</p>
                            <p>Dans ce livre, nous nous concentrons sur les réseaux utilisés sur Terre. Ces réseaux incluent parfois des liaisons satellites. En plus des technologies de réseau utilisées sur Terre, les chercheurs développent des techniques de mise en réseau qui pourraient être utilisées entre des noeuds situés sur des planètes différentes. Un tel Internet interplanétaire nécessite des techniques différentes de celles discutées dans ce livre. Pour plus d'informations sur ce techniques, consultez la RFC 4838 et les références qui y sont mentionnées.</p>
                        </li>
                    </ul>
                    <p>Une autre classification des réseaux informatiques est basée sur leur topologie physique. Dans les figures suivantes, les liens physiques sont représentés par des lignes tandis que les boîtes montrent des ordinateurs ou d'autres types d'équipements de réseau.</p>
                    <p>Les réseaux informatiques sont utilisés pour permettre à plusieurs hôtes d'échanger des informations entre eux. Pour permettre à n'importe quel hôte d'envoyer des messages à n'importe quel autre hôte du réseau, la solution la plus simple est de les organiser en maillage complet, avec un lien direct et dédié entre chaque paire d'hôtes. Cette topologie physique est parfois utilisée, en particulier lorsqu'une haute performance et une grande redondance sont requises pour un petit nombre d'hôtes. Cependant, elle présente deux inconvénients majeurs :</p>
                    <ul>
                        <li>
                            <p>pour un réseau contenant <span class="em">n</span> hôtes, chaque hôte doit avoir <span class="em">n - 1</span> interfaces physiques. En pratique, le nombre d'interfaces physiques sur un noeud limitera la taille d'un réseau en maillage complet qui peut être construit.</p>
                        </li>
                        <li>
                            <p>pour un réseau contenant <span class="em">n</span> hôtes, <span class="em">(n<sup>2</sup> - n) / 2</span> liens nécessaires. Ceci est possible lorsqu'il y a quelques noeuds dans la même pièce, mais rarement lorsqu'ils sont situés à plusieurs kilomètres de distance.</p>
                        </li>
                    </ul>
                    <figure>
                        <img src="../images/reseau_en_maillage_complet.PNG" alt="">
                        <figcaption>Figure 2.3 : Un réseau en maillage complet</figcaption>
                    </figure>
                    <p>La deuxième organisation physique possible, qui est également utilisée à l'intérieur des ordinateurs pour connecter différentes cartes d'utilisation, est le bus. Dans un réseau de bus, tous les hôtes sont cconnectés à un support partagé, généralement un câble, via une seule interface. Lorsqu'un hôte envoie un signal électrique sur le bus, le signal est reçu par tous les hôtes connectés au bus. Un inconvénient des réseaux en bus est que si le bus est physiquement coupé, le réseau est divisé en deux réseaux isolés. Pour cette raison, les réseaux en bus sont parfois considérés comme difficiles à exploiter et à entretenir, surtout lorsque le câble est long et qu'il y a de nombreux endroits où il peut se rompre. Une telle topologie en bus a été utilisée dans les premiers réseaux Ethernet.</p>
                    <figure>
                        <img src="../images/reseau_en_bus.PNG" alt="">
                        <figcaption>Figure 2.4 : Un réseau organisé en bus</figcaption>
                    </figure>
                    <p>Une troisième organisation d'un réseau informatique est une topologie en étoile. Dans de telles topologies, les hôtes disposent d'une seule interface physique et il y a un lien physique entre chaque hôte et le centre de l'étoile. Le noeud au centre de l'étoile peut être soit un équipement qui amplifie un signal électrique, soit un appareil actif, tel qu'un équipement qui comprend le format des messages échangés à travers le réseau. Bien sûr, la défaillance du noeud central implique la défaillance du réseau. Cependant, si un lien physique échoue (par exemple, parce que le câble a été coupé), alors un seul noeud est déconnecté du réseau. En pratique, les réseaux en forme d'étoile sont plus faciles à exploiter et à entretenir que les réseaux en forme de bus. De nombreux administrateurs réseau apprécient également le fait qu'ils peuvent contrôler le réseau à partir d'un point central. Administré à partir d'une interface Web ou via une connexion de type console, le centre de l'étoile est un point de contrôle utile (activation ou désactivation d'appareils) et un excellent point d'observation (statiques d'utilisation).</p>
                    <figure>
                        <img src="../images/reseau_en_etoile.PNG" alt="">
                        <figcaption>Figure 2.5 : Un réseau organisé en étoile</figcaption>
                    </figure>
                    <p>Une quatrième organisation physique d'un réseau est la topologie en anneau. Comme pour l'organisation en bus, chaque hôte a une seule interface physique qui le relie à l'anneau. Tout signal envoyé par un hôte sur l'anneau sera reçu par tous les hôtes connectés à l'anneau. Du point de vue de la redondance, un seul anneau n'est pas la meilleure solution, car le signal ne se déplace que dans une seule direction sur l'anneau. Ainsi, si l'un des liens qui composent l'anneau est coupé, l'ensemble du réseau tombe en panne. En pratique, de tels anneaux ont été utilisés dans des réseaux locaux, mais sont maintenant souvent remplacés par des réseaux en étoile. Dans les réseaux métropolitains, les anneaux sont souvent utilisés pour interconnecter plusieurs sites. Dans ce cas, deux liens parallèles, composés de câbles différents, sont souvent utilisés par la redondance. Avec un tel anneau double, lorsque l'un des anneaux échoue, tout le trafic peut être rapidement basculé vers l'autre anneau.</p>
                    <figure>
                        <img src="../images/reseau_en_anneau.PNG" alt="">
                        <figcaption>Figure 2.6 : Un réseau organisé en anneau</figcaption>
                    </figure>
                    <p>Une cinquième organisation physique d'un réseau est l'arborescence. De tels réseaux sont généralement utilisés lorsqu'un grand nombre de clients doivent être connectés de manière très rentable. Les réseaux de télévision par câble sont souvent organisés en arborescence.</p>
                    <figure>
                        <img src="../images/reseau_en_arbre.PNG" alt="">
                        <figcaption>Figure 2.7 : Un réseau organisé en arbre</figcaption>
                    </figure>
                    <p>En pratique, la plupart des réseaux réels combinent une partie de ces topologies. Par exemple, un réseau de campus peut être organisé en anneau entre les bâtiments clés, tandis que les bâtiments plus petits sont rattachés en tant qu'arbre ou étoile aux bâtiments importants. Ou un réseau ISP peut avoir un maillage complet d'appareils dans le coeur de son réseau, et des arbres pour connecter les utilisateurs distants.</p>
                    <p>Tout au long de ce livre, notre objectif sera de comprendre les protocoles et les mécanismes nécessaires pour un réseau tel que celui-illustré ci-dessous.</p>
                    <figure>
                        <img src="../images/inter-reseau_simple.PNG" alt="">
                        <figcaption>Figure 2.8 : Un inter-réseau simple</figcaption>
                    </figure>
                    <p>La figure ci-dessus illustre un inter-réseau, c'est-à-dire un réseau interconnecte d'autres réseaux. Chaque réseau est représenté par une ellipse contenant quelques appareils. Nous expliquerons tout au long du livre les différents types d'appareils et leurs rôles respectifs permettant à tous les hôtes d'échanger des informations. En outre, nous discuterons de la manière dont les réseaux sont interconnectés et des règles qui régissent ces interconnexions. Nous analyserons également comment les topologies en bus, en anneau et en maillage sont utilisées pour construire des réseaux réels.</p>
                    <p>Le dernier point de terminolgie que nous devons aborder est les modes de transmission. Lorsque nous échangeons des informations à travers un réseau, nous distinguons trois modes de transmission. Dans la transmission TV et radio, la diffusion (<span class="html">broadcast</span>) est souvent utilisée pour indiquer une technologie qui envoie un signal vidéo ou radio à tous les récepteurs dans une zone géographique donnée. La diffusion est parfois utilisée dans les réseaux informatiques, mais uniquement dans les réseaux locaux où le nombre de destinataires est limité.</p>
                    <p>Le premier et le mode de transmission le plus répondu s'appelle <span class="html">unicast</span>. Dans le mode de transmission unicast, l'information est envoyée par un émetteur à un récepteur. La plupart des applications Internet d'aujourd'hui reposent sur le mode de transmission unicast. L'exemple ci-dessous montre un réseau avec deux types d'appareils : des hôtes (dessinés comme des ordinateurs) et des noeuds intermédiaires (dessinés comme des cubes). Les hôtes échangent des informations via les noeuds intermédiaires. Dans l'exemple ci-dessous, lorsque l'hôte S utilise unicast pour envoyer des informations, il les envoie via trois noeuds intermédiaires. Chacun de ces noeuds reçoit l'information de son noeud ou hôte en amont, la traite et la transmet à son noeud ou hôte en aval. Cela s'appelle <span class="em">store and forward</span> et nous verrons plus tard que ce concept est clé dans les réseaux informatiques.</p>
                    <figure>
                        <img src="../images/transmission_unicast.PNG" alt="">
                        <figcaption>Figure 2.9 : transmission unicast</figcaption>
                    </figure>
                    <p>Un deuxième mode de transmission est le mode de transmission <span class="html">multicast</span>. Ce mode est utilisé lorsque la même information doit être envoyée à un ensemble de destinataires. Il a d'abord été utilisé dans les réseaux locaux (LAN), mais est devenu plus tard pris en charge dans les réseaux étendus. Lorsqu'un expéditeur utilise le multicast pour envoyer des informations à N destinataires, l'expéditeur envoie une seule copie des informations et les noeuds du réseau dupliquent ces informations chaque fois que cela est nécessaire, de sorte qu'elles puissent atteindre tous les destinataires appartenant au groupe de destination.</p>
                    <p>Pour comprendre l'importance de la transmission en mode multicast, considérons la source S qui envoie les mêmes informations aux destinations A, C et E. Avec le mode unicast, les mêmes informations passent trois fois sur les noeuds intermédiaires 1 et 2 et deux fois sur le noeud 4. C'est une perte de ressources pour les noeuds intermédiaires et les liens entre eux. Avec la transmission en mode multicast, l'hôte S envoie les informations au noeud 1 qui le transmet en aval au noeud 2. Ce noeud crée une copie des informations reçues en envoie une copie directement à l'hôte E et l'autre en aval vers le noeud 4. À la reception des informations, le noeud 4 produit une copie et envoie une copie à la fois au noeud A et une autre au noeud C. Grâce à la transmission en mode multicast, les mêmes informations peuvent atteindre un grand nombre de destinataires tout en étant envoyées une seule fois sur chaque lien.</p>
                    <figure>
                        <img src="../images/transmission_multicast.PNG" alt="">
                        <figcaption>Figure 2.10 : transmission multicast</figcaption>
                    </figure>
                    <p>Le dernier mode de transmission est le mode de transmission <span class="html">anycast</span>. Il a été initialement défini dans le RFC 1542. Dans ce mode de transmission, un ensemble de récepteurs est identifié. Lorsqu'une source envoie des informations à cet ensemble de récepteurs, le réseau s'assyre que les informations sont délivrées à un récepteur qui appartient à cet ensemble. En général, le récepteur le plus proche de la source est celui qui reçoit les informations envoyées par cette source particulière. Le mode de transmission anycast est utile pour assurer la redondance, car lorsque l'un des récepteurs échoue, le réseau s'assure que les informations seront délivrées à un autre récepteur appartenant au même groupe. Cependant, en pratique, la prise en charge du mode de transmission anycast peut être difficile.</p>
                    <figure>
                        <img src="../images/transmission_anycast.PNG" alt="">
                        <figcaption>Figure 2.11 : transmission anycast</figcaption>
                    </figure>
                    <p>Dans l'exemple ci-dessus, les trois hôtes marqués d'un astérisque (*) font partie du même groupe anycast. Lorsque l'hôte S envoie des informations à ce groupe anycast, le réseau s'assure qu'elles parviendront à l'un des membres du groupe anycast. Les lignes en pointillés montrent une livraison possible via les noeuds 1, 2 et 4. Une transmission anycast ultérieure de l'hôte S vers le même groupe anycast pourrait atteindre l'hôte attaché au noeud intermédiaire 3, comme indiqué par la ligne pleine. Une transmission anycast atteint un membre du groupe anycast choisi par le réseau en fonction des conditions actuelles du réseau.</p>
                    <h3>2.2. Services et protocoles :</h3>
                    <p>Un aspect important à comprendre avant d'étudier les réseaux informatiques est la différence entre un service et un protocole.</p>
                    <p>Pour comprendre la différence entre les deux, il est utile de commencer par des exemples du monde réel. La Poste traditionnelle fournit un service dans lequel un facteur livre des lettres aux destinataires. La Poste définit précisément quels types de lettres (taille, poids, etc.) peuvent être livrés en utilisant le service du courrier standard. De plus, le format de l'enveloppe est spécifié (position des adresses de l'expéditeur et du destinataire, position du timbre). Qyelqu'un qui veut envoyer une lettre doit soit la déposer dans un bureau de poste ou dans l'une des boîtes aux lettres dédiées. La lettre sera ensuite collectée et livrée à son destinataire final. Notez que pour le service régulier, La Poste ne garantit généralement pas la livraison de chaque lettre particulière, certaines lettres peuvent être perdues et d'autres peuvent être livrées à la mauvaise boîte aux lettres. Si une lettre est importante, l'expéditeur peut utiliser le service recommandé pour s'assurer que la lettre sera livrée à son destinataire. Certains services de La Poste proposent également un service avec accusé de réception ou un service de courrier express qui est plus rapide que le service régulier.</p>
                    <p>Dans les réseaux informatiques, la notion de service est définie de manière plus formelle dans [X2000]. Elle peut être mieux comprise en considérant un réseau informatique, quelle que soit sa taille ou sa complexité, comme une boîte noire qui fournit un service aux utilisateurs, comme le montre la figure ci-dessous. Ces utilisateurs pourraient être des utilisateurs humains ou des processus s'exécutant sur un système informatique.</p>
                    <p>De nombreux utilisateurs peuvent être connectés au même fournisseur de services. Par l'intermédiaire de ce fournisseur, chaque utilisateur doit être en mesure d'échanger des messages avec n'importe quel autre utilisateur. Pour pouvoir livrer ces messages, le fournisseur de services doit être en mesure d'identifier de manière univoque chaque utilisateur. Dans les réseaux informatiques, chaque utilisateur est identifié par une adresse unique, nous discuterons plus tard de la façon dont ces adresses sont construites et utilisées. À ce stade, et lors de la considération de la transmission en unicast, la principale caractéristique de ces adresses est qu'elles sont uniques. Deux utilisateurs différents connectés au réseau ne peuvent pas utiliser la même adresse.</p>
                    <figure>
                        <img src="../images/utilisateurs_fournisseur_services.PNG" alt="">
                        <figcaption>Figure 2.12 : utilisateurs et fournisseur de services</figcaption>
                    </figure>
                    <p>Tout au long de ce livre, nous définirons un service comme un ensemble de capacités fournies par un système (et ses éléments sous-jacents) à son utilisateur. Un utilisateur interagit avec un service via un point d'accès au service. Notez que, comme indiqué dans la figure ci-dessus, les utilisateurs interagissent avec un seul fournisseur de services. En pratique, le fournisseur de services est réparti sur plusieurs hôtes, mais ce sont des détails d'implémentation qui ne sont pas importants à ce stade. Ces interactions entre un utilisateur et un fournisseur de services sont exprimées dans [X200] en utilisant des primitives, comme le montre la figure ci-dessous. Ces primitives sont une représentation abstraite des interactions entre un utilisateur et un fournisseur de services. En pratique, ces interactions pourraient être implémentées sous forme d'appels système, par exemple.</p>
                    <figure>
                        <img src="../images/quatre_types_primitives.PNG" alt="">
                        <figcaption>Figure 2.13 : les quatre types de primitives</figcaption>
                    </figure>
                    <p>Les quatres types de primitives sont définis :</p>
                    <ul>
                        <li>
                            <p><span class="html">X.request</span> : Ce type de primitive correspond à une demande émise par un utilisateur à un fournisseur de service.</p>
                        </li>
                        <li>
                            <p><span class="html">X.indication</span> : Ce type de primitive est généré par le fournisseur de réseau et livré à un utilisateur (souventt lié à une primitive <span class="em">X.request</span> antérieure et distante).</p>
                        </li>
                        <li>
                            <p><span class="html">X.response</span> : Ce type de primitive est généré par un utilisateur pour répondre à une primitive <span class="em">X.indication</span> antérieure.</p>
                        </li>
                        <li>
                            <p><span class="html">X.confirm</span> : Ce type de primitive est livré par le fournisseur de service pour confirmer à un utilisateur q'une primitive <span class="em">X.request</span> précédente a été traitée avec succès.</p>
                        </li>
                    </ul>
                    <p>Les primitives peuvent être combinées pour modéliser différents types de services. Le service le plus simple dans les réseaux informatiques est appelé <span class="html">le service sans connexion</span>. Ce service est appelé sans connexion car il n'est pas nécessaire de créer une connexion avant de transmettre des données contrairement au service orienté connexion. Ce service peut être modélisé en utilisant deux primitives :</p>
                    <ul>
                        <li>
                            <p><span class="html">Data.request(source, destination, SDU)</span> : Cette primitive est émise par un utilisateur qui spécifie, en tant que paramètres, son adresse source, l'adresse du destinataire du message et le message lui-même. Nous utiliserons <span class="html">l'unité de données de service (SDU ou Service Data Unit)</span> pour nommer le message échangé de manière transparente entre deux utilisateurs d'un service.</p>
                        </li>
                        <li>
                            <p><span class="html">Data.indication(source, destination, SDU)</span> : Cette primitive est délivrée par un fournisseur de service à un utilisateur. Elle contient comme paramètres une unité de données de service (SDU) ainsi que les adresses des utilisateurs émetteur et destinataire.</p>
                        </li>
                    </ul>
                    <p>Lorsqu'on discute du service fourni dans un réseau informatique, il est souvent utile de pouvoir décrire graphiquement les interactions entre les utilisateurs et le fournisseur. Une représentation fréquemment utilisée est le diagramme de séquence temporelle. Dans ce chapitre et tout au long du livre, nous utiliserons souvent des diagrammes tels que celui présenté ci-dessous. Un diagramme de séquence temporelle décrit les interactions entre deux utilisateurs et un fournisseur de services. Par convention, les utilisateurs sont représentés dans les parties gauche et droite du diagramme tandis que le fournisseur de services occupe le milieu du diagramme. Dans un tel diagramme de séquence temporelle, le temps d'écoule de haut en bas du diagramme. Chaque primitive est représentée par une flèche horizontale simple, à laquelle le nom de la primitive est attaché. Les lignes en pointillé sont utilisées pour représenter la relation possible entre deux (ou plusieurs) primitives. Un tel diagramme fournit des informations sur l'ordre des différentes primitives, mais la distance entre deux primitives ne représente pas une quantité de temps précise.</p>
                    <p>La figure ci-dessous fournit une représentation du service sans connexion sous forme de <span class="html">diagramme de séquence temporelle</span>. L'utilisateur à gauche, ayant l'adresse S, émet une primitive <span class="em">Data.request</span> contenant la SDU <span class="em">M</span> qui doit être livré par le fournisseur de services à la destination  <span class="em">D</span>. La ligne en pointillé entre les deux primitives indique que la primitive <span class="em">Data.indication</span> qui est délivrée à l'utilisateur de droite correspond à la primitive <span class="em">Data.request</span> envoyée par l'utilisateur de gauche.</p>
                    <figure>
                        <img src="../images/service_sans_connection_simple.PNG" alt="">
                        <figcaption>Figure 2.14 : Un service sans connexion simple</figcaption>
                    </figure>
                    <p>Il existe plusieurs implémentations possibles du service sans connexion, que nous étudierons plus tard dans ce livre. Avant d'examiner ces réalisations, il est utile de discuter des caractéristiques possibles du service sans connexion. Un <span class="html">service sans connection fiable</span> est un service dans lequel le fournisseur de service garantit que tous les SDU soumis dans les <span class="em">Data.requests</span> par un utilisateur seront finalement livrés à leur destination. Un tel service serait très utile pour les utilisateurs, mais garantir une livraison parfaite est difficile en pratique. Pour cette raison, les réseaux informatiques prennent généralement en charge un <span class="html">service sans connexion non fiable</span>.</p>
                    <p>Un <span class="em">service sans connexion peu fiable</span> peut présenter différents types de problèmes par rapport à un <span class="em">service sans connexion fiable</span>. Tout d'abord, un <span class="em">service sans connexion peu fiable</span> ne garantit pas la livraison de toutes les unités de donénes de service (SDU). Cela peut être exprimé graphiquement en utilisant le diagramme de séquence temporelle ci-dessous.</p>
                    <p>En pratique, un <span class="em">service sans connexion peu fiable</span> livrera généralement une partie des SDU. Cependant, comme la livraison des SDU n'est pas garantie, l'utilisateur doit être capable de la perte de tout SDU.</p>
                    <figure>
                        <img src="../images/service_sans_connection_peu_fiable_perdre_SDU.PNG" alt="">
                        <figcaption>Figure 2.15 : Un service sans connexion peut perdre des SDU</figcaption>
                    </figure>
                    <p>Une seconde imperfection qui peut affecter un service non fiable sans connexion est qu'il peut dupliquer des SDU. Certains founisseurs de services sans connexion peu fiables peuvent livrer une SDU envoyée par un utilisateur deux fois ou même plus. Cela est illustré par le diagramme de séquence temporelle ci-dessous.</p>
                    <figure>
                        <img src="../images/service_sans_connection_peu_fiable_dupliquer_SDU.PNG" alt="">
                        <figcaption>Figure 2.16 : Un service sans connection peu fiable peut dupliquer des SDU</figcaption>
                    </figure>
                    <p>Enfin, certains fournisseurs de services sans connexion peu fiables peuvent livrer à une destination un SDU différent de celui qui a été fourni dans le <span class="em">Data.request</span>. Cela est illustré dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/service_sans_connection_peu_fiable_livrer_SDU_erronees.PNG" alt="">
                        <figcaption>Figure 2.17 : Un service sans connection peu fiable peut livrer des SDU erronnées</figcaption>
                    </figure>
                    <p>Lorsqu'un utilisateur interagit avec un fournisseur de service, il doit connaître précisément les limitations du service sous-jacent afin de pouvoir surmonter tout problème qui pourrait survenir. Cela nécessite une définition précise des caractéristiques du service sous-jacent.</p>
                    <p>Une autre caractéristique importante du service sans connexion est de savoir s'il préserve l'ordre des SDU envoyés par un utilisateur. Du point de vue de l'utilisateur, c'est souvent une caractéristique souhaitable. Cela est illustré dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/service_sans_connection_preserve_ordre_SDU_envoyees.PNG" alt="">
                        <figcaption>Figure 2.18 : Un service sans connexion qui préserve l'ordre des SDU envoyées par un utilisateur donné</figcaption>
                    </figure>
                    <p>Cependant, de nombreux services sans connexion, en particulier les services non fiables, ne garantissent pas qu'ils préserveront toujours l'ordre des SDU envoyés par chaque utilisateur. Cela est illustré dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/service_sans_connection_preserve_pas_ordre_SDU_envoyees.PNG" alt="">
                        <figcaption>Figure 2.19 : Un service sans connexion qui ne préserve pas l'ordre des SDU envoyées par un utilisateur donné</figcaption>
                    </figure>
                    <p>Le service sans connexion est largement utilisé dans les réseaux informatiques, comme nous le verrons plus tard dans ce livre. Plusieurs variantes de ce service de base ont été proposées. L'une d'entre elles est le <span class="html">service sans connexion confirmé</span>. Ce service utilise une primitive <span class="em">Data.confirm</span> en plus des primitives <span class="em">Data.request</span> et <span class="em">Data.indication</span> classiques. Cette primitive est émise par le fournisseur de services pour confirmer à un utilisateur la livraison d'un SDU précédemment envoyé à son destinataire. Notez que, comme le service enregistré de la poste, <span class="em">Data.confirm</span> indique uniquement que le SDU a été livré à l'utilisateur de destination. La primitive <span class="em">Data.confirm</span> ne permet pas de savoir si le SDU a été traité par l'utilisateur de destination. Ce service sans connexion confirmé est illustré dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/service_sans_connection_confirme.PNG" alt="">
                        <figcaption>Figure 2.20 : Un service sans connexion confirmé</figcaption>
                    </figure>
                    <p>Une invocation du <span class="html">service orienté-connexion</span> est divisée en trois phases. La première phase est l'établissement d'une <span class="html">connexion</span>. Une <span class="em">connexion</span> est une association temporaire entre deux utilisateurs via un fournisseur de service. Plusieurs connexions peuvent exister en même temps entre une paire d'utilisateurs. Une fois établie, la connexion est utilisée pour transférer des SDU. Les connexions fournissent généralement un flux bidirectionnel permettant l'échange de SDU entre les deux utilisateurs associés par la connexion. Ce flux est utilisé pour transférer des données pendant la deuxième phase de la connexion appelée phase de <span class="html">transfert de données</span>. La troisième phase est la <span class="html">terminaison de la connexion</span>. Une fois que les utilisateurs ont fini d'échanger des SDU, ils demandent au fournisseur de service de terminer la connexion. Comme nous le veroons plus tard, il y a également des cas où le fournisseur de service peut avoir besoin de terminer une connexion lui-même.</p>
                    <p>La mise en place d'une connexion peut être modélisée en utilisant quatre primitives : <span class="em">Connect.request</span>, <span class="em">Connect.indication</span>, <span class="em">Connect.response</span> et <span class="em">Connect.confirm</span>. La primitive <span class="em">Connect.request</span> est utilisée pour demander l'établissement d'une connexion. Le paramètre principal de cette primitive est l'adresse de l'utilisateur de destination. Le fournisseur de service délivre une primitive <span class="em">Connect.indication</span> pour informer l'utilisateur de destination de la tentative de connexion. S'il accepte d'établir une connexion, il répond avec une primitive <span class="em">Connect.response</span>. À ce stade, la connexion est considérée comme ouverte et l'utilisateur de destination peut commencer à envoyer des SDU sur la connexion. Le fournisseur de service traite la <span class="em">Connect.response</span> et délivrera une primitive <span class="em">Connect.confirm</span> à l'utilisateur qui a initié la connexion. La livraison de cette primitive met fin à la phase d'établissement de la connexion. À ce stade, la connexion est considérée comme ouverte et les deux utilisateurs peuvent envoyer des SDU. L'établissement réussi d'une connexion est illustré ci-dessous.</p>
                    <figure>
                        <img src="../images/etablissement_connexion.PNG" alt="">
                        <figcaption>Figure 2.21 : L'établissement de la connexion</figcaption>
                    </figure>
                    <p>L'exemple ci-dessus montre l'établissement réussi d'une connexion. Cependant, en pratique, toutes les connexions ne sont pas établies avec succès. Une raison est que l'utilisateur de destination peut ne pas être d'accord, pour des raisons de politique ou de performance, pour établir une connexion avec l'utilisateur initiateur à ce moment. Dans ce cas, l'utilisateur de destination répond à la primitive <span class="em">Connect.indication</span> par une primitive <span class="html">Disconnect.request</span> qui contient un paramètre pour indiquer la raison pour laquelle la connexion a été refusée. Le fournisseur de service délivrera alors une primitive <span class="html">Disconnect.indication</span> pour informer l'utilisateur initiateur. Une deuxième raison est lorsque le fournisseur de service est incapable d'atteindre l'utilisateur de destination. Cela peut arriver parce que l'utilisateur de destination n'est pas actuellement connecté au réseau ou en raison de la congestion. Dans ces cas, le fournisseur de service répond à la primitive <span class="em">Connect.request</span> avec une primitive <span class="em">Disconnect.indication</span> dont le paramètre de raison contient des informations supplémentaires sur l'échec de la connexion.</p>
                    <figure>
                        <img src="../images/types_rejet_tentative_etablissement_connexion.PNG" alt="">
                        <figcaption>Figure 2.22 : Deux types de rejet pour une tentative d'établissement de connexion</figcaption>
                    </figure>
                    <p>Une fois que la connexion est établie, le fournisseur de services fournit deux flux de données aux utilisateurs communicants. Le premier flux de données peut être utilisé par l'utilisateur initiant pour envoyer des SDU. Le deuxième flux de donénes permet à l'utilisateur répondant d'envoyer des SDU à l'utilisateur initiant. Les flux de données peuvent être organisés de différentes manières. Une première organisation est le transfert en mode message. Avec le transfert en mode message, le fournisseur de services garantit qu'un seul et unique <span class="em">Data.indication</span> sera livré au point final du flux de données pour chaque primitive <span class="em">Data.request</span> émise par l'autre point final. Le transfert en mode message est illustré dans la figure ci-dessous. Le principal avantage du mode de transfert de message est que le destinataire reçoit exactement les SDU qui ont été envoyés par l'autre utilisateur. Si chaque SDU contient une commande, l'utilisateur qui reçoit peut traiter chaque commande dès qu'il reçoit un SDU.</p>
                    <figure>
                        <img src="../images/transfert_mode_message_service_oriente_connexion.PNG" alt="">
                        <figcaption>Figure 2.23 : Transfert en mode message dans un service orienté connexion</figcaption>
                    </figure>
                    <p>Malheureusement, le transfert en mode message n'est pas largement utilisé sur Internet. Sur Internet, le service orienté connexion le plus populaire transfère des SDU en mode flux. Avec le mode flux, le fournisseur de services fournit un flux de bytes qui relie les deux utilisateurs en communication. L'utilisateur émetteur envoie des bytes en utilisant des primitives de requête de données (<span class="em">Data.request</span>) qui contiennent des séquences de bytes en tant que SDU. Le fournissur de sercices délivre des SDU contenant des bytes consécutifs à l'utilisateur récepteur en utilisant des primitives d'indication de données (<span class="em">Data.indication</span>). Le fournisseur de services garantit que tous les bytes envoyés à une extrémité du flux sont livrés correctement dans le même ordre à l'autre extrémité. Cependant, le fournisseur de services n'essaie pas de préserver les limites des SDU. Il n'y a aucune relation imposée par le fournisseur de services entre le nombre de primitives <span class="em">Data.request</span> et le nombre de primitives <span class="em">Data.indication</span>. Le mode flux est illustré dans la figure ci-dessous. En pratique, une conséquence de l'utilisation du mode flux est que si les utilisateurs veulent échanger des SDU structurés, ils devront fournir les mécanismes qui permettent à l'utilisateur récepteur de séparer les SDU successifs dans le flux de bytes qu'il reçoit. Comme nous le verrons dans le prochain chapitre, les protocoles de la couche d'application utilisent souvent des délimiteurs spécifiques tels que le caractère de fin de ligne pour délimiter des SDU dans un flux de bytes.</p>
                    <figure>
                        <img src="../images/transfert_mode_flux_service_oriente_connexion.PNG" alt="">
                        <figcaption>Figure 2.24 : Transfert en mode flux dans un service orienté connexion</figcaption>
                    </figure>
                    <p>La troisième phase d'une connexion est lorsqu'elle doit être libérée. Comme une connxion implique trois parties (deux utilisateurs et un fournisseur de service), l'une d'entre elles peut demander la résiliation de la connexion. Habituellement, les connexions sont terminées à la demande d'un utilisateur une fois le transfert de données terminé. Cependant, parfois le fournisseur de service peut être obligé de mettre fin à une connexion. Cela peut être dû à un manque de ressources à l'intérieur du fournisseur de service ou parce que l'un des utilisateurs n'est plus accessible via le réseau. Dans ce cas, le fournisseur de service émettra des primitives <span class="em">Disconnect.indication</span> à tous les deux utilisateurs. Ces primitives contiendront, en paramètre, des informations sur la raison de la résiliation de la connexion. Malheureusement, comme illustré dans la figure ci-dessous, lorsqu'un fournisseur de service est contraint de mettre fin à une connexion, il ne peut garantir que toutes les SDU envoyées par chaque utilisateur ont été livrées à l'autre utilisateur. Cette libération de connexion est dite abrupte car elle peut causer des pertes de données.</p>
                    <figure>
                        <img src="../images/liberation_abrupte_connexion_initiee_fournisseur_service.PNG" alt="">
                        <figcaption>Figure 2.25 : Libération abrupte de connexion initiée par le fournisseur de service</figcaption>
                    </figure>
                    <p>Une libération de connexion abrupte peut également être déclenchée par l'un des utilisateurs. Si un utilisateur a besoin, pour une raison quelconque, de mettre fin rapidement à une connexion, il peut émettre une primitive <span class="em">Disconnect.request</span> et demander une libération abrupte. Le fournisseur de service traitera la demande, arrêtera les deux flux de données et livrera la primitive <span class="em">Disconnect.indication</span> à l'utilisateur distant dès que possible. Comme illustré dans la figure ci-dessous, cette libération de connexion abrupte peut entraîner des pertes de SDU.</p>
                    <figure>
                        <img src="../images/liberation_abrupte_connexion_initiee_utilisateur.PNG" alt="">
                        <figcaption>Figure 2.26 : Libération abrupte de connexion initiée par un utilisateur</figcaption>
                    </figure>
                    <p>Pour assurer une livraison fiable des SDU envoyés par chaque utilisateur sur une connexion, nous devons considérer les deux flux qui composent une connexion comme indépendants. Un utilisateur devrait être en mesure de libérer le flux qu'il utilise pour envoyer des SDU une fois qu'il a envoyé tous les SDU qu'il avait prévu d'envoyer sur cette connexion, mais continuer à recevoir des SDU sur le flux opposé. Cette libération de connexion gracieuse est généralement effectuée comme indiqué dans la figure ci-dessous. Un utilisateur émet une primitive <span class="em">Disconnect.request</span> à son fournisseur une fois qu'il a émis toutes ses primitives <span class="em">Data.request</span>. Le fournisseur de service attendra que toutes les primitives <Span class="em">Data.indication</Span> aient été livrées à l'utilisateur destinataire avant d'émettre la primitive <span class="em">Disconnect.indication</span>. Cette primitive informe l'utilisateur destinataire qu'il ne recevra plus de SDU sur cette connexion, mais il peut toujours émettre des primitives <span class="em">Data.request</span> sur le flux dans la direction opposée. Une fois que l'utilisateur a émis toutes ses primitives <span class="em">Data.request</span>, il émet une primitive <span class="em">Disconnect.request</span> pour demander la terminaison du flux restant. Le fournisseur de service traitera la demande et livrera la <span class="em">Disconnect.indication</span> correspondante à l'autre utilisateur une fois qu'il aura livré toutes les primitives <span class="em">Data.indication</span> en attente. À ce stade, toutes les données ont été libérés avec succès et la connexion est complètement fermée.</p>
                    <figure>
                        <img src="../images/liberation_gracieuse_connexion.PNG" alt="">
                        <figcaption>Figure 2.27 : Libération gracieuse de connexion</figcaption>
                    </figure>
                    <hr>
                    <p>Note : Fiabilité du service orienté connexion :</p>
                    <p>Un point important à noter concernant le service orienté connexion est sa fiabilité. Un service orienté connexion ne peut garantir la bonne livraison de toutes les unités de données de service (SDU) que si la connexion est relâchée de manière contrôlée. Cela implique que tant que la connexion est active, il n'y aucune garantie de la livraison effective des SDU échangées car la connexion peut être libérée de manière abrupte à tout moment.</p>
                    <hr>
                    <h3>2.3 Les modèles de référence :</h3>
                    <p>Face à la complexité croissante des réseaux informatiques, au cours des années 1970, les chercheurs en réseau ont proposé différents modèles de référence pour faciliter la description des protocoles et des services de réseau. Parmi ceux-ci, le modèle de référence de l'Interconnexion de Systèmes Ouverts (ISO) [Zimmermann80] a été probablement le plus influent. Il a servi de base aux travaux de normalisation effectués au sein de l'ISO pour développer des normes mondiales de réseau informatique. Le modèle de référence que nous utilisons dans ce livre peut être considéré comme une version simplifiée du modèle de référence OSI. Une discussion historique intéressante sur le débat OSI-TCP/IP peut être trouvée dans [Russel06].</p>
                    <h4>2.3.1 Les cinq couches du modèle de référence :</h4>
                    <p>Notre modèle de référence est divisé en cinq couches, comme le montre la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/cinq_couches_modele_reference.PNG" alt="">
                        <figcaption>Figure 2.28 : Les cinq couches du modèle de référence</figcaption>
                    </figure>
                    <p>En commençant par la base, la première couche est <span class="html">la couche physique</span>. Deux appareils communicants sont reliés par un support physique. Ce support physique est utilisé pour transférer un signal électrique ou optique entre deux appareils directement connectés. Plusieurs types de supports physiques sont utilisés en pratique :</p>
                    <ul>
                        <li>
                            <p><span class="html">câble électrique</span> : Les informations peuvent être transmises sur différents types de câbles électriques. Les plus courants sont les paires torsadées utilisées dans le réseau téléphonique, mais également dans les réseaux de télévision par câble, mais ne sont plus utilisés dans les réseaux d'entreprise. Certaines technologies de réseau fonctionnent sur le câble électrique classique.</p>
                        </li>
                        <li>
                            <p><span class="html">fibre optique</span> : Les fibres optiques sont fréquemment utilisées dans les réseaux publics et d'entreprise lorsque la distance entre les appareils de communication est supérieure à un kilomètre. Il existe deux principaux types de fibres optiques : <span class="html">multimode</span> et <span class="html">monomode</span>. Le multimode est beaucoup moins cher que la fibre monomode car une LED peut être utilisée pour envoyer un signal sur une fibre multimode tandis qu'une fibre monomode doit être pilotée par un laser. En raison des différents modes de propagation de la lumière, les fibres monomodes sont limitées à des distances de quelques kilomètres tandis que les fibres multimodes peuvent être utilisées sur des distances supérieures à plusieurs dizaines de kilomètres. Dans les deux cas, des répéteurs peuvent être utilisées pour régénérer le signal optique à une extrémité d'une fibre pour l'envoyer sur une autre fibre.</p>
                        </li>
                        <li>
                            <p><span class="html">sans fil</span> : Dans ce cas, un signal radio est utilisé pour coder les informations échangées entre les appareils de communication. De nombreuses techniques de modulation sont utilisées pour envoyer des informations sur un canal sans fil et il y a beaucoup d'innovation dans ce domaine avec de nouvelles techniques apparaissant chaque année. Bien que la plupart des réseaux sans fil reposent sur des signaux radio, certains utilisent un laser qui envoie des impulsions lumineuses à un détecteur distant. Ces techniques optiques permettent de créer des liaisons point à point tandis que les techniques basées sur la radio, en fonction de la directionnalité des antennes, peuvent être utilisées pour construire des réseaux contenant des appareils répartis sur une petite zone géographique.</p>
                        </li>
                    </ul>
                    <p>Un point important à noter à propos de la couche physique est le service qu'elle fournit. Ce service est généralement un service orienté connexion peu fiable qui permet aux utilisateurs de la couche physique d'échanger des bits. L'unité de transfert d'information dans la couche physique est le bit. Le service de la couche physique est peu fiable parce que :</p>
                    <ul>
                        <li>
                            <p>la couche physique peut changer, par exemple en raison d'interférences électromagnétiques, la valeur d'un bit en cours de transmission</p>
                        </li>
                        <li>
                            <p>la couche physique peut livrer plus de bits au récepteur que les bits envoyés par l'émetteur</p>
                        </li>
                        <li>
                            <p>la couche physique peut livrer moins de bits au récepteur que les bits envoyés par l'émetteur</p>
                        </li>
                    </ul>
                    <p>Les deux derniers points peuvent sembler étranges à première vue. Lorsque deux périphériques sont connectés via un câble, comment est-il possible que des bits soient créés ou perdus sur ce câble ?</p>
                    <p>Cela est principalement dû au fait que les appareils communicants utilisent leur propre horloge pour transmettre des bits à une vitesse de transmission donnée. Considérons un émetteur ayant une horloge qui bat un million de fois par seconde et envoie un bit à chaque battement. Chaque microseconde, l'émetteur envoie un signal électrique ou optique qui code un bit. Le débit binaire de l'émetteur est donc de 1 Mbps. Si l'horloge du récepteur bat exactement 5 fois chaque microseconde, il délivrera également 1 Mbps à son utilisateur. Cependant, si l'horloge du récepteur est légèrement plus rapide (ou plus lente), il délivrera légèrement plus (ou moins) d'un million de bits chaque seconde. Cela explique pourquoi la couche physique peut perdre ou créer des bits. Il est très difficile en pratique d'avoir des horloges parfaitement synchronisées fonctionnant à haute fréquence. Cependant, certaines couches physiques introduisent une boucle de rétroaction qui permet à l'horloge du récepteur de se synchroniser automatiquement à l'horloge de l'émetteur. Cependant, toutes les couches physiques n'incluent pas ce type de synchronisation.</p>
                    <hr>
                    <p>Note : Débit binaire :</p>
                    <p>Dans les réseaux informatiques, le débit binaire de la couche physique est toujours exprimé en bits par seconde. Un Mbps est évalement à un million de bits par seconde et un Gbps est équivalent à un milliard de bits par seconde. Cela contraste avec les spécifications de mémoire qui sont habituellement exprimées en octets (8 bits), Kilo-octets (1024 octets) ou Méga-octets (1048576 octets). Ainsi, transférer un MByte à travers une liaison de 1 mbps prend 8,39 secondes.</p>
                    <table class="tableBalises"role="presentation">
                        <thead>
                            <tr>
                                <th>Débit binaire</th>
                                <th>Bits par seconde</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>1 Kbps</td>
                                <td>10<sup>3</sup></td>
                            </tr>
                            <tr>
                                <td>1 Mbps</td>
                                <td>10<sup>6</sup></td>
                            </tr>
                            <tr>
                                <td>1 Gbps</td>
                                <td>10<sup>9</sup></td>
                            </tr>
                            <tr>
                                <td>1 Tbps</td>
                                <td>10<sup>12</sup></td>
                            </tr>
                        </tbody>
                    </table>
                    <hr>
                    <figure>
                        <img src="../images/couche_physique.PNG" alt="">
                        <figcaption>Figure 2.29 : La couche physique</figcaption>
                    </figure>
                    <p>La couche physique permet ainsi à deux ou plusieurs entités directement connectées au même support de transmission d'échanger des bits. Pouvoir échanger des bits est important car pratiquement toutes les informations peuvent être encodées sous forme de séquences de bits. Les ingénieurs électriciens ont l'habitude de traiter des flux de bits, mais les informaticiens préfèrent généralement traiter des concepts de plus haut niveau. Un problème similaire se pose avec le stockage de fichiers. Les dispositifs de stockage tels que les disques durs stockent également des flux de bits. Il existe des dispositifs matériels qui traitent le flux de bits produit par un disque dur, mais les informaticiens ont conçu des systèmes de fichiers pour permettre aux applications d'accéder facilement à ces dispositifs de stockage. Ces syst!mes de fichiers sont généralement également divisés en plusieurs couches. Les disques durs stockent des secteurs de 512 octets ou plus. Les systèmes de ficheirs Unix regroupent des secteurs en blocs plus larges qui peuvent contenir des données ou des inodes représentant la structure du système de fichiers. Enfin, les applications manipulent des fichiers et des répertoires qui sont traduits en blocs, en secteurs et finalement en bits par le système d'exploitation.</p>
                    <p>Les réseaux informatiques utilisent une approche similaire. Chaque couche fournit un service qui est construit au-dessus de la couche sous-jacente et est plus proche des besoins des applications.</p>
                    <p><span class="html">La couche liaison de données</span> repose sur le service fourni par la couche physique sous-jacente. La couche liaison de donénes permet à deux hôtes directement connectés par la couche physique d'échanger des informations. L'unité d'information échangée entre deux entités dans la couche liaison de données est une trame. Une trame est une séquence finie de bits. Certaines couches liaison de données fournissent un service orienté connexion tandis que d'autres fournissent un service sans connexion. Certaines couches liaison de données garantissent la livraison fiable des informations tandis que d'autres ne garantissent pas la livraison correcte de l'information.</p>
                    <p>Un point important à noter à propos de la couche liaison de données est que bien que la figure ci-dessous indique que deux entités de la couche liaison de données échangent directement des trames, en réalité c'est légèrement différent. Lorsque l'entité de la couche liaison de données à gauche doit transmettre une trame, elle émet autant de primitives <span class="em">Data.request</span> à la couche physique sous-jacente qu'il y a de bits dans la trame. La couche physique convertira alors la séquence de bits en un signal électromagnétique ou optique qui sera envoyé sur le support physique. La couche physique du côté droit de la figure décodera le signal reçu, récupérera les bits et émettra les primitives <span class="em">Data.indication</span> correspondantes à son entité de la couche liaison de données. S'il n'y a pas d'erreurs de transmission, cette entité recevra la trame envoyée précédemment.</p>
                    <figure>
                        <img src="../images/couche_liaison_donnees.PNG" alt="">
                        <figcaption>Figure 2.30 : La couche liaison de données</figcaption>
                    </figure>
                    <p>La couche liaison de données permet à des hôtes directement connectés d'échanger de l'information, mais il est souvent nécessaire d'échanger de l'information entre des hôtes qui ne sont pas connectés au même support physique. C'est la tâche de la <span class="html">couche réseau</span>. La couche réseau est construire au-dessus de la couche liaison de données. Les entités de la couche réseau échangent des <span class="html">paquets</span>. Un paquet est une séquence finie d'octets transportée par la couche liaison de données à l'intérieur d'une ou plusieurs trames. Un paquet contient généralement des informations sur son origine et sa destination, et passe habituellement par plusieurs appareils intermédiaires appelés routeurs en chemin de son origine à sa destination.</p>
                    <figure>
                        <img src="../images/couche_reseau.PNG" alt="">
                        <figcaption>Figure 2.31 : La couche réseau</figcaption>
                    </figure>
                    <p>La plupart des implémentations de la couche réseau, y compris Internet, ne fournissent pas de service fiable. Cependant, de nombreuses applications ont besoin d'échanger des informations de manière fiable, et l'utilisation directe du service de la couche réseau serait très difficile pour elles. Assurer la livraison fiable des données produites par les applications est la tâhce de la <span class="html">couche transport</span>. Les entités de la couche transport échangent des <span class="html">segments</span>. Un segment est une séquence finie d'octets qui sont transportés à l'intérieur d'un ou plusieurs paquets. Une entité de la couche transport émet des segments (ou parfois une partie de segments) en tant que demande de données (<span class="em">Data.request</span>) à l'entité de la couche réseau sous-jacente.</p>
                    <figure>
                        <img src="../images/couche_transport.PNG" alt="">
                        <figcaption>Figure 2.32 : le réseau transport</figcaption>
                    </figure>
                    <p>Il existe différents types de couches transport. Les couches transport les plus couramment utilisées sur Internet sont <span class="html">TCP</span>, qui fournit un service de transport orienté connexion fiable pour un flux de données en octetsn, et <span class="html">UDP</span>, qui founit un service de transprot non fiable sans connexion.</p>
                    <p>La couche supérieure de notre architecture est la <span class="html">couche application</span>. Cette couche comprend tous les mécanismes et structures de données nécessaires pour les applications. Nous utiliserons <span class="html">l'unité de données de l'application (UDA ou ADU pour "Application Data Unit")</span> pour indiquer les données échangées entre deux entités de la couche application.</p>
                    <figure>
                        <img src="../images/couche_application.PNG" alt="">
                        <figcaption>Figure 2.33 : La couche application</figcaption>
                    </figure>
                    <h4>2.3.2 Le modèle de référence TCP/IP :</h4>
                    <p>En contraste avec OSI, la communauté TCP/IP n'a pas fait beaucoup d'efforts pour définir un modèle de référence détaillé; en fait, les objectifs de l'architecture Internet ont été documentés après que TCP/IP ait été déployé [Clark88]. RFC 1122, qui définit les exigences pour les hôtes Internet, mentionne quatre couches différentes. En commençant par le haut, ce sont :</p>
                    <ul>
                        <li>
                            <p>une couche Application</p>
                        </li>
                        <li>
                            <p>une couche Transport</p>
                        </li>
                        <li>
                            <p>une couche Internet qui est équivalente à la couche réseau de notre modèle de référence</p>
                        </li>
                        <li>
                            <p>une couche Liaison qui combine les fonctionnalités des courches physique et de liaison de données de notre modèle de référence à cinq couches.</p>
                        </li>
                    </ul>
                    <p>Outre cette différence dans les couches inférieures, le modèle de référence TCP/IP est très proche des cinq couches que nous utilons tout au long de ce document.</p>
                    <h4>2.3.3 Le modèle de référence OSI :</h4>
                    <p>En comparaison avec le modèle de référence à cinq couches expliqué ci-dessus, le modèle de référence OSI défini dans [X200] est divisé en sept couches. Les quatre couches inférieures sont similaires aux quatre couches inférieures décrites ci-dessus. Le modèle de référence OSI a affiné la couche application en la divisant en trois couches :</p>
                    <ul>
                        <li>
                            <p><span class="html">La couche Session</span> contient les protocoles et mécanismes nécessaires pour organiser et synchroniser le dialogue et gérer l'échange de données entre les entités de la couche Présentation. Alors que l'une des fonctions principales de la couche transport est de faire face à l'instabilité de la couche réseau, l'objectif de la couche session est de masquer les éventuelles défaillances des connexions de niveau transport vers la couche supérieure. Pour cela, la couche session fournit des services permettant d'établir une connexion de session, de soutenir un échange de données ordonné (y compris des mécanismes permettant de récupérer d'une libération abrupte d'une connexion de transport sous-jacente) et de libérer la connexion de manière ordonnée.</p>
                        </li>
                        <li>
                            <p><span class="html">La couche Présentation</span> a été conçue pour faire face aux différentes façons de représenter l'information sur les ordinateurs. Il existe de nombreuses différences dans la façon dont les ordinateurs stockent l'information. Certains ordinateurs stockent les entiers sous forme de champs de 32 bits, d'autres utilisent des champs de 64 bits et le même problème se pose avec les nombres à virgule flottante. Pour les informations textuelles, cela est encore plus complexe avec les nombreux codes de caractères différents qui ont été utilisés. La situation est encore plus complexe lorsqu'on considère l'échange d'informations structurées telles que les enregistrements de base de données. Pour résoudre ce problème, la couche présentation prévoit une représentation commune des données transférées. La notation <span class="em">ASN.1</span> a été conçue pour la couche présentation et est encore utilisée aujourd'hui par certains protocoles.</p>
                        </li>
                        <li>
                            <p><span class="html">La couche Application</span> qui contient les mécanismes qui ne rentrent ni dans la couche présentation, ni dans la couche session. La couche application OSI était elle-même subdivisée en plusieurs éléments de service génériques.</p>
                        </li>
                    </ul>
                    <hr>
                    <p>Note : Où sont les couches manquantes dans le modèle de référence TCP/IP ?</p>
                    <p>Le modèle de référence TCP/IP place les couches présentation et session implicitement dans la couche application. Les principales motivations pour simplifier les couches supérieures das le modèle de référence TCP/IP étaient pragmatiques. La plupart des applications Internet ont commencé comme des prototypes qui ont évolué et ont ensuite été standardisés. Beaucoup de ces applications ont supposé qu'elles seraient utilisées pour échanger des informations écrites en anglais américain et pour lesquelles le code de caractères US-ASCII sur 7 bits était suffisant. C'était le cas pour le courrier électronique, mais comme nous le verrons dans le prochain chapitre, le courrier électronique a pu évoluer pour prendre en charge différentes codifications de caractères. Certaines applications considéraient explicitement les différentes représentations de données. Par exemple, <span class="em">ftp</span> contenait des mécanismes pour convertir un fichier d'un format à un autre et le langage HTML a été défini pour représenter des pages Web. D'autre part, de nombreuses spécifications ISO ont été développées par des comités composés de personnes qui n'ont pas toutes participé à des implémentations réelles. ISO a consacré beaucoup d'efforts à l'analyse des exigebces et à la définition d'une solution qui répond à toutes ces exigences. Malheureusement, certaines des spécifications étaient si complexes qu'il était difficile de les implémenter complètement et les organismes de normalisation ont défini des profils recommandés qui contenanient les ensembles d'options implémentés...</p>
                    <hr>
                    <figure>
                        <img src="../images/sept_couches_modele_reference_OSI.PNG" alt="">
                        <figcaption>Figure 2.34 : Les sept couches du modèle de référence OSI</figcaption>
                    </figure>
                    <h3>2.4 Organisation du livre :</h3>
                    <p>Ce document est organisé selon le modèle de référence TCP/IP et suit une approche descendante (top-down). La plupart des manuels classiques de réseau choisissent une approche ascendante (bottom-up), c'est-à-dire qu'ils expliquent d'abord tous les détails électriques et optiques de la couche physique avant de passer à la couche liaison de données. Cette approche a bien fonctionné pendant la première période de l'informatique en réseau et jusqu'à la fin des années 1990. À cette époque, la plupart des étudiants n'étaient pas utilisateurs de réseaux informatiques et il était utile d'expliquer les réseaux informatiques en construisant les protocoles correpondants à partir des plus simples, dans la couche physique, jusqu'à la couche application. Aujourd'hui, tous les étudiants sont des utilisateurs actifs d'application. Aujourd'hui, tous les étudiants sont des utilisateurs actifs d'applicatiion Internet et commencer à apprendre la réseautique en examinant des bits n'est pas très motivant. À partir de [KuroseRoss09], de nombreux manuels et enseignants ont choisi une approche descendante. Cette approche commence par des applications telles que le courrier électronique et le Web que les étudiants connaissent déjà et explore les différentes couches, en commençant par la couche application. Cette approche fonctionne assez bien avec les étudiants d'aujourd'hui. L'approche traditionnelle ascendante pourrait en fait être considérée comme une approche d'ingénierie car elle part du réseau le plus simple qui permet l'échange de bits et explique comment combiner différents protocoles et mécanismes pour construire les applications les plus complexes. L'approche descendante, en revanche, pourrait être considérée comme une approche scientifique. Comme les biologistes, elle part d'un système existant (construit par l'homme) et l'explore couche par couche.</p>
                    <p>En plus de l'organisation top-down versus bottom-up, les livres sur les réseaux informatiques peuvent soit viser à couvrir en profondeur un petit nombre de sujets, soit à avoir une couverture limitée d'un large éventail de sujets. Couvrir un large éventail de sujets est intéressant pour les cours d'introduction ou pour les étudiants qui n'ont pas besoin d'une connaissance détaillée des réseaux informatiques. Cela permet aux étudiants d'apprendre un peu de tout, puis de partir de cette connaissance de base plus tard s'ils ont besoin de comprendre les réseaux informatiques plus en détail. Ce livre a choisi de couvrir en détail un plus petit nombre de jets que d'autres manuels. Cela est motivé par le fait que les réseaux informatiques doivent souvent être poussés à leurs limites. Comprendre les détails des principaux protocoles de mise en réseau est important pour pouvoir comprendre pleinement le comportement d'un réseau ou l'étendre pour fournir des services innovants. Une citation populaire dit que le diable se cache dans les détails. Cette situation reflète très bien le fonctionnement de nombreux protocoles de réseau, où le changement d'un seul bit peut avoir des conséquences énormes. Dans les réseaux informatiques, comprendre tous les détails est parfois nécessaire.</p>
                    <p>Le livre est organisé comme suit : nous décrivons d'abord la couche application dans le chapitre "La couche application". Étant donné le grand nombre d'applications basées sur Internet, il est bien sûr impossible de les couvrir toutes en détail. Nous nous concentrons plutôt sur trois types d'applications basées sur Internet. Nous étudions d'abord le système de noms de domaine (DNS) puis expliquons certains des protocoles impliqués dans l'échange de courrier électronique. La discussion de la couche application se termine par une description des protocoles clés du World Wide Web.</p>
                    <p>Toutes ces applications reposent sur la couche transport qui est expliquée dans le chapitre "chapter-transport". Il s'agit d'une couche clé dans les réseaux d'aujourd'hui car elle contient tous les mécanismes nécessaires pour assurer une transmission fiable des données sur un réseau non fiable. Nous couvrons la couche transport en développant d'abord un protocole simple de couche transport fiable, puis expliquons les détails des protocoles TCP et UDP utilisés dans les réseaux TCP/IP.</p>
                    <p>Après la couche transport, nous analysons la couche réseau dans le chapitre "La couche réseau". C'est également une couche très importante car elle est responsable de la livraison de paquets de n'importe quelle source vers n'importe quelle destination via des routeurs intermédiaires. Dans la couche réseau, nous décrivons les deux organisations possibles de la couche réseau et les protocoles de routage basés sur les états de lien et les vecteurs de distance. Ensuite, nous expliquons en détail les protocoles IPv4, IPv6, RIP, OSPF et BGP qui sont effectivement utilisés dans l'Internet d'aujourd'hui.</p>
                    <p>Le dernier chapitre du livre est consacré à la couche liaison de données. Dans le chapitre "La couche liaison de données et les réseaux locaux", nous commençons par expliquer les principes de la couche liaison de données sur des liens point-à-point. Ensuite, nous nous concentrons sur les réseaux locaux. Nous considérons à la fois les techniques opportunistes et déterministes. Nous expliquons ensuite en détail deux types de LAN importants d'un point de vue de déploiement aujourdhui : <span class="html">Ethernet</span> et <span class="html">Wifi</span>.</p>
                </article>
                <article>
                    <h2 id="couche_application">Partie 3 : La couche application :</h2>
                    <h3>3.1 La couche application :</h3>
                    <p>La couche application est la couche la plus importante et la plus visible des réseaux informatiques. Les applications résident dans cette couche et les utilisateurs humains interagissent via ces applications à travers le réseau.</p>
                    <p>Dans ce chapitre, nous décrivons d'abord brièvement les principes principaux de la couche application et nous nous concentrons sur les deux modèles d'application les plus importants : les modèles client-serveur et pair-à-pair. Ensuite, nous examinons en détail deux familles de protocoles qui se sont révélées très utiles sur Internet : le courrier électronique et les protocoles qui permettent l'accès aux informations sur le World Wide Web. Nous décrivons également le système de noms de domaine qui permet aux humains d'utiliser des noms convivaux alors que les hôtes utilisent des adresses IP longues de 32 bits ou 128 bits.</p>
                    <h3>3.2 Principes :</h3>
                    <p>Il y a deux modèles importants utilisés pour organiser une application en réseau. Le premier et le plus ancien modèle est le modèle client-serveur. Dans ce modèle, un serveur fournit des services aux clients qui échangent des informations avec lui. Ce modèle est hautement asymétrique : les clients envoient des requêtes et les serveurs effectuent des actions et renvoient des réponses. Il est illustré dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/modele_client_serveur.PNG" alt="">
                        <figcaption>Figure 3.1 : Le modèle client-serveur</figcaption>
                    </figure>
                    <p>Le modèle client-serveur était le premier modèle utilisé pour développer des applications en réseau. Ce modèle découle naturellement des ordinateurs centraux et des mini-ordinateurs qui étaient les seuls ordinateurs en réseau utilisés jusqu'aux années 1980. Un mini-ordinateur est un système multi-utilisateur utilisé par des dizaines ou plus d'utilisateurs en même temps. Chaque utilisateur interagit avec le mini-ordinateur en utilisant un terminal. Ces terminaux, composés principalement d'un écran, d'un clavier et d'un câble directement connecté au mini-ordinateur.</p>
                    <p>Il existe différents types de serveurs ainsi que différents types de clients. Un serveur web fournit des informations en réponse à la requête envoyée par ses clients. Un serveur d'impression imprime des documents envoyés sous forme de requêtes par le client. Un serveur de messagerie transférera les messages électroniques envoyés sous forme de requêtes à leur destinataire, tanid qu'un serveur de musique fournira la musique demandée par le client. Du point de vue du développeur d'application, les applications cliente et serveur échangent directement des messages (les flèches horizontales étiquetées Requêtes et Réponses dans la figure ci-dessus), mais en pratique, ces messages sont échangés grpace aux couches sous-jacentes (les flèches verticales dans la figure ci-dessus). Dans ce chapitre, nous nous concentrons sur ces échanges horizontaux de messages.</p>
                    <p>Les applications en réseau n'échangent pas des messages aléatoires. Pour s'assurer que le serveur est capable de comprendre les requêtes envoyées par un client, et aussi que le client est capable de comprendre les réponses envoyées par le serveur, ils doivent tous deux convenir d'un ensemble de règles syntaxiques et sémantiques. Ces règles définissent le format des messages échangés ainsi que leur ordre. Cet ensemble de règles est appelé un protocole de niveau application.</p>
                    <p>Un <span class="html">protocole de niveau d'application</span> est similaire à une conversation structurée entre des êtres humains. Supposons qu'Alice souhauite connaître l'heure actuelle mais n'a pas de montre. Si Bob passe à proximité, la conversation suivante pourrait avoir lieu :</p>
                    <ul>
                        <li>
                            <p>Alice : Bonjour</p>
                        </li>
                        <li>
                            <p>Bob : Bonjour</p>
                        </li>
                        <li>
                            <p>Alice : Quelle heure est-il ?</p>
                        </li>
                        <li>
                            <p>Bob : 11h55</p>
                        </li>
                        <li>
                            <p>Alice : Merci</p>
                        </li>
                        <li>
                            <p>Bob : De rien</p>
                        </li>
                    </ul>
                    <p>Une telle conversation réussit si Alice et Bob parlent la même langue. Si Alice rencontre Tchang qui ne parle que chinois, elle ne pourra pas lui demander l'heure actuelle. Une conversation entre humains peut être plus complexe. Par exemple, supposons que Bob est un garde de sécurité dont la responsabilité est de n'autoriser que les agents secrets de confiance à entrer dans une salle de réunion. Si tous les agents connaissent un mot de passe secret, la conversation entre Bob et Trudy pourrait être la suivante :</p>
                    <ul>
                        <li>
                            <p>Bob : Quel est le mot de passe secret ?</p>
                        </li>
                        <li>
                            <p>Trudy : 1234</p>
                        </li>
                        <li>
                            <p>Bob : C'est le mot de passe correct, bienvenue</p>
                        </li>
                    </ul>
                    <p>Si Alice veut entrer dans la salle de réunion mais ne connaît pas le mot de passe, sa conversation pourrait être la suivante :</p>
                    <ul>
                        <li>
                            <p>Bob : Quel est le mot de passe secret ?</p>
                        </li>
                        <li>
                            <p>Alice : 3.1415</p>
                        </li>
                        <li>
                            <p>Bob : Ce n'est pas le mot de passe correct</p>
                        </li>
                    </ul>
                    <p>Les conversations humaines peuvent être très formelles, par exemple lorsque les soldats communiquent avec leur hiérarchie, ou informelles comme lorsqu'ils discutent entre amis. Les ordinateurs qui communiquent sont plus semblables à des soldats et nécessitent des règles bien définies pour assurer un échange d'informations réussi. Il existe deux types de règles qui définissent comment les informations peuvent être échangées entre les ordinateurs :</p>
                    <ul>
                        <li>
                            <p>Les règles syntaxiques précisent le format des messages échangés entre les ordinateurs. Comme les ordinateurs ne traitent que des bits, ces règles spécifient comment l'information est encodée en chaînes de bits.</p>
                        </li>
                        <li>
                            <p>Pour de nombreuses applications, le flux d'informations doit être structuré et il existe des relations de prééminence entre les différents types d'informations échangées. Dans l'exemple de l'heure donné précédemment, Alice doit saluer Bob avant de lui demander l'heure. Alice ne demanderait pas l'heure d'abord et ne saluerait Bob qu'ensuite. De telles relations de prééminence existent également dans les applications réseau. Par exemple, un serveur doit recevoir un nom d'utilisateur et un mot de passe valide avant d'accepter des commandes plus complexes de ses clients.</p>
                        </li>
                    </ul>
                    <p>Commençons par discuter des règles syntaxiques. Nous expliquerons plus tard comment le flux d'informations peut être organisé en analysant des applications réseau réelles.</p>
                    <p>Les protocoles de la couche application échangent deux types de messages. Certains protocoles tels que ceux utilisés pour prendre en charge l'échange de courrier électronique échangenet des messages exprimés sous forme de chaînes ou de lignes de caractères. Comme la couche de transport permet aux hôtes d'échanger des octets, ils doivent convenir d'une représentation commune des caractères. La première et la plus simple méthode pour encoder les caractères est d'utiliser la table ASCII. La RFC 20 fournit la table ASCII qui est utilisée par de nombreux protocoles sur Internet. Par exemple, la table définit les représentations binaires suivantes :</p>
                    <ul>
                        <li>
                            <p>A : 1000011b</p>
                        </li>
                        <li>
                            <p>0 : 0110000b</p>
                        </li>
                        <li>
                            <p>z : 1111010b</p>
                        </li>
                        <li>
                            <p>@ : 1000000b</p>
                        </li>
                        <li>
                            <p>space : 0100000b</p>
                        </li>
                    </ul>
                    <p>En outre, la table ASCII définit également plusieurs caractères non imprimables ou caractères de contrôle. Ces caractères ont été conçus pour permettre à une application de contrôler une imprimante ou un terminal. Ces caractères de contrôle comprennent CR et LF, qui sont sont utilisés pour terminer une ligne, et le caractère Bell qui provoque l'émission d'un son par le terminal.</p>
                    <ul>
                        <li>
                            <p>Retour chariot (CR pour carriage return) : 000AA0Ab</p>
                        </li>
                        <li>
                            <p>Avance de ligne (LF pour line feed) : 0001010b</p>
                        </li>
                        <li>
                            <p>Bell : 0000111b</p>
                        </li>
                    </ul>
                    <p>Les caractères ASCII sont encodés sur sept bits, mais transmis sous forme d'un octet de huit bits dont le bit de poids élevé est généralement fixé à 0. Les octets sont toujours transmis en commmençant par le bit de poids élevé ou le bit le plus significatif.</p>
                    <p>La plupart des applications échangent des chaînes de caractères composées d'un nombre fixe ou variable de caractères. Une solution courante pour définir les chaînes de caractères acceptables consiste à les définir comme une grammaire utilisant une forme de Backus-Naur (BNF) telle que la BNF augmentée définie dans le RFC 5234. Une BNF est un ensemble de règles de production qui génèrent toutes les chaînes de caractères valides. Par exemple, considérez une application en réseau qui utilise deux commandes, où l'utilisateur peut fournir un nom d'utilisateur et un mot de passe. La BNF pour cette application pourrait être définie comme indiqu" dans la figure-dessous.</p>
                    <figure>
                        <img src="../images/specification_BNF_simple.PNG" alt="">
                        <figcaption>Figure 3.2 : Une spécification BNF simple</figcaption>
                    </figure>
                    <p>L'exemple ci-dessus définit plusieurs terminaux et deux commandes : <span class="em">usercommand</span> et <span class="em">passwordcommand</span>. Le terminal <span class="em">ALPHA</span> contient toutes les lettres en majuscules et minuscules. Dans la règle <span class="em">ALPHA</span>, <span class="em">%x41</span> correspond au code de caractère ASCII <span class="em">41</span> en hexadécimal, c'est-à-dire le <span class="em">A</span> majuscule. Les terminaux <span class="em">CR</span> et <span class="em">LF</span> correspondent aux caractères de contrôle de retour de cariot et de saut de ligne. La règle <span class="em">CRLF</span> concatène ces deux terminaux pour correspondre à la fin de ligne standard. Le terminal <span class="Em">DIGIT</span> contient tous les chiffres. Le terminal <span class="em">SP</span> correspond aux caractères d'espacement. La commande <span class="em">usercommand</span> est composée de deux chaînes séparées par un espace blanc. Dans les règles <span class="em">ABNF</span> qui définissent les messages utilisés par les applications Internet, les commandes sont insensibles à la casse. La règle "user" correspond à toutes les cas possibles des lettres qui composent le mot entre crochets, par exemple user, uSeR, USER, usER, ... Un nom d'utilisateur contient au moins une lettre et jusqu'à 8 lettres. Les noms d'utilisateur sont sensibles à la casse car ils ne sont pas définis comme une chaîne entre crochets. La règle de mot de passe indique qu'un mot de passe commence par une lettre et peut contenir n'importe quel nombre de lettres ou de chiffres. Les caractères d'espacement et de contrôle ne peuvent pas apparaître dans un mot de passe défini par la règle ci-dessus.</p>
                    <p>En plus des chaînes de caractères, certaines applications ont également besoin d'échanger des champs de 16 ou 32 bits tels que des entiers. Une solution naïve aurait été d'envoyer le champ de 16 ou 32 bits tel qu'il est encodé dans la mémoire de l'hôte. Malheureusement, il existe différentes méthodes pour stocker les champs de 16 ou 32 bits en mémoire. Certains porcesseurs stockent le byte le plus significatif d'un champ de 16 bits à la première adresse du champ, tandis que d'autres stockent le byte le moins significatif à cet emplacement. Lorsque des applications en réseau s'exécutant sur des processeurs différents échangent des champs de 16 bits, il existe deux possibilités pour les transférer via le service de transport :</p>
                    <ul>
                        <li>
                            <p>envoyer le byte le plus significatif suivi du byte le moins significatif</p>
                        </li>
                        <li>
                            <p>envoyer le byte le moins significatif suivi du byte le plus significatif</p>
                        </li>
                    </ul>
                    <p>La première possibilité a été appelée <span class="html">big-endian</span> dans une note écrite par Cohen [Cohen1980], tandis que la seconde a été appelée <span class="html">little-endian</span>. Les fournisseurs de processeurs qui utilisaient <span class="em">big-endian</span> en mémoire ont insisté sur l'utilisation de l'encodage <span class="em">big-endian</span> dans les applications en réseau, tandis que les fournisseurs de processeurs qui utilisaient <span class="em">little-endian</span> ont recommandé l'inverse. Plusieurs études ont été rédigées sur les mérites relatifs de chaque type d'encodage, mais la discussion est devenue presque une question religieuse [Cohen1980]. Finalement, Internet a choisi l'encodage <span class="em">big-endian</span>, c'est-à-dire que les champs multi-octets sont toujours transmis en envoyant le byte le plus significatif en premier, le RFC 791 fait référence à cet encodage comme étant <span class="html">l'ordre des octets du réseau</span>. La plupart des bibliothèques utilisées pour écrire des applications en réseau contiennent des fonctions permettant de convertir les champs multi-octets de la mémoire à l'ordre des octets du réseau et vice versa.</p>
                    <p>Par exemple, les fonctions de la bibliothèque standard C <span class="html">htonl(3)</span> et <span class="html">ntohl(3)</span> permettent de convertir un entier non signé de 32 bits de l'ordre d'octets utilisé par le processeur à l'ordre d'octets du réseau (resp. de l'ordre d'octets du réseau à l'ordre d'octets du processeur). Des fonctions similaires existent dans d'autres langages de programmation.</p>
                    <p>En plus des mots de 16 et 32 bits, certaines applications ont besoin d'échanger des structures de données contenant des champs de bits de différentes longueurs. Par exemple, un message peut être composé d'un champ de 16 bits suivi de huit indicateurs, chacun sur un seul bit, d'un champ de 24 bits et de deux octets de 8 bits. Les spécifications des protocoles Internet définiront un tel message en utilisant une représentation telle que celle ci-dessous. Dans cette représentation, chaque ligne correpond à 32 bits et les lignes verticales sont utilisées pour délimiter les champs. Les nombres au-dessus des lignes indiquent les positions des bits dans le mot de 32 bits, avec le bit de poids fort à la position 0.</p>
                    <figure>
                        <img src="../images/format_message.PNG" alt="">
                        <figcaption>Figure 3.3 : Format message</figcaption>
                    </figure>
                    <p>Le message mentionné ci-dessus sera transmis en commençant par le premier mot de 32 bits dans l'ordre des octets du réseau. Le premier champ est encodé sur 16 bits. Il est suivi de huit indicateurs à un bit (A-H), d'un champ de 24 bits dont l'octet de poids fort est représenté sur la première ligne et les deux octets de poids faible apparaissent sur la deuxième ligne, suivis de deux champs d'un octet chacun. Cette représentation ASCII est fréquemment utilisée lors de la définition de protocoles binaires. Nous l'utiliserons pour tous les protocoles binaires discutés dans ce livre.</p>
                    <p>Nous discuterons de plusieurs exemples de protocoles au niveau de l'application dans ce chapitre.</p>
                    <h4>3.2.1 Le modèle pair-à-pair :</h4>
                    <p>Le modèle pair-à-pair est apparu au cours des dix dernières années en tant qu'architecture possible pour les applications en réseau. Dans le modèle client-serveur traditionnel, les hôtes agissent soit en tant que serveurs, soit en tant que clients et un serveur sert un grand nombre de clients. Dans le modèle pair-à-pair, tous les hôtes agissent à la fois en tant que serveurs et clients et jouent les deux rôles. Le modèle pair-à-pair a été utilisé pour développer diverses applications en réseau, allant de la téléphonie Internet au partage de fichiers ou aux systèmes de fichiers à l'échelle d'Internet. Une description détaillée des applications pair-à-pair peut être trouvée dans [BYL2008]. Des enquêtes sur les protocoles et les applications pair-à-pair peuvent être trouvées dans [AS2004] et [LCP2005].</p>
                    <h3>3.3 Les services de transport :</h3>
                    <p>Les applications en réseau sont construites sur le service de transport. Comme expliqué dans le chapitre précédent, il existe deux types principaux de services de transport :</p>
                    <ul>
                        <li>
                            <p>le service de datagramme ou sans connexion</p>
                        </li>
                        <li>
                            <p>le service orienté connexion ou de flux d'octets</p>
                        </li>
                    </ul>
                    <p>Le service sans connexion permet aux appplications d'échanger facilement des messages ou des unités de donénes de service. Sur Internet, ce service est fourni par le protocole UDP qui sera expliqué dans le prochain chapitre. Le service de transport sans connexion sur Internet est peu fiable, mais est capable de détecter les erreurs de transmission. Cela implique qu'une application ne recevra pas une unité de données de service qui a été corrompue en raison d'erreurs de transmission.</p>
                    <p>Le service de transport sans connexion permet aux applications réseau d'échanger des messages. Plusieurs applications réseau peuvent être en cours d'exécution en même temps sur un seul hôte. Chacune de ces applications doit être en mesure d'échanger des SDU avec des applications distantes. Pour permettre ces échanges de SDU, chaque application réseau s'exécutant sur un hôte est identifiée par les informations suivantes :</p>
                    <ul>
                        <li>
                            <p>l'<span class="html">hôte</span> sur lequel l'application s'exécute</p>
                        </li>
                        <li>
                            <p>le <span class="html">numéro de port</span> sur lequel l'application écoute les SDU</p>
                        </li>
                    </ul>
                    <p>Sur Internet, le numéro de port est un entier et l'hôte est identifé par son adresse réseau. Comme nous le verrons dans le chapitre sur la couche réseau, il existe deux types d'adresses Internet :</p>
                    <ul>
                        <li>
                            <p>les adresses <span class="html">IP version 4</span> qui sont larges de 32 bits</p>
                        </li>
                        <li>
                            <p>les adresses <span class="html">IP version 6</span> qui sont larges de 128 bits</p>
                        </li>
                    </ul>
                    <p>IPv4 est représenté habituellement en utilisant une représentation décimale pointée où chaque nombre décimal correspond à un octet de l'adresse, par exemple <span class="em">203.0.113.56</span>. Les adresses IPv6 sont généralement représentées sous forme de nombres hexadécimaux séparés par des deux-points, par exemple <span class="em">2011:db8:3080:2:217:f2ff:fed6:65c0</span>. AUjourd'hui, la plupart des hôtes Internet ont une adresse IPv4. Une petite fraction d'entre eux a également une adresse IPv6. À l'avenir, nous pouvons nous attendre à ce que de plus en plus d'hôtes aient des adresses IPv6 et que certains d'entre eux n'aient plus d'adresse IPv4. Un hôte qui n'a qu'une adresse IPv6. La figure ci-dessous illustre deux hôtes qui utilisent le service de datagramme fourni par UCP sur des hôtes qui utilisent des adresses IPv4.</p>
                    <figure>
                        <img src="../images/service_sans_connexion_datagramme.PNG" alt="">
                        <figcaption>Figure 3.4 : Le service sans connexion ou datagramme</figcaption>
                    </figure>
                    <p>Le deuxième service de transport est le service orienté connexion. Sur Internet, ce service est souvent appelé le <span class="html">service de flux de bytes</span> car il crée un flux de bytes fiable entre les deux applications qui sont liées par une connexion de transport. Comme le service datagramme, les applications réseau qui utilisent le service de flux de bytes sont identifiées par l'hôte sur lequel elles s'exécutent et un numéro de port. Ces hôtes peuvent être identifés par une adresse IPv4, une adresse IPv6 ou un nom. La figure ci-dessous illustre deux applications qui utilisent le service de flux de bytes fourni par le protocole TCP sur des hôtes IPv6. Le service de flux de bytes fourni par TCP est fiable et bidirectionnel.</p>
                    <h3>3.4 Protocoles de niveau applicatif :</h3>
                    <p>De nombreux protocoles ont été définis pour les applications réseau. Dans cette section, nous décrivons certaines des applications importantes utilisées sur Internet. Nous expliquons d'abord le <span class="html">système de noms de domaine (DNS pour Domain Name System)</span> qui permet aux hôtes d'être identifiés par des noms convivaux pour les humains au lieu des adresses IPv4 ou IPv6 utilisées par le réseau. Ensuite, nous décrivons le fonctionnement du courrier électronique, l'une des premières applications phares sur l'INternet mondial, et les protocoles utilisés sur le World Wide Web.</p>
                    <h4>3.4.1 Le DNS :</h4>
                    <p>Au début de l'Internet, il y avait seulement quelques hôtes (principalement des mini-ordinateurs) connectés au réseau. Les applications les plus populaires étaient la connexion à distance et le transfert de fichiers. En 1983, il y avait déjà cinq cents hôtes connectés à l'Internet. Chacun de ces hôtes était identifié par une adresse IPv4 unique. Forcer les utilisateurs à se souvenir des adresses IPv4 des hôtes distants qu'ils voulaient utiliser n'était pas convivial pour les utilisateurs. Les utilisateurs préfèrent se souvenir des noms et les utiliser en cas de besoin. Utiliser des noms comme alias pour les adresses est une technique courante en informatique. Elle simplifie le développement des applications et permet au développeur d'ignorer les détails de bas niveau. Par exemple, en utilisant un langage de programmation plutôt qu'enécrivant du code machine, un développeur peut écrire des logiciels sans savoir si les variables qu'il utilise sont stockées dans la mémoire ou dans les registres.</p>
                    <p>En raison du fait que les noms sont à un niveau supérieur aux adresses, ils permettent (aussi bien dans l'exemple de la programmation que sur Internet) de considérer les adresses comme de simples identifiants techniques, qui peuvent être modifiés à volonté. Seuls les noms sont stables. Sur l'Internet d'aujourd'hui, où le fait de passer à un autre fournisseur d'accès Internet signifie changer ses adresses IP, la convivialité des noms de domaine est moins importante (les utilisateurs ne les saisissent pas souvent), mais leur stabilité reste une propriété très importante, peut-être leur propriété la plus importante.</p>
                    <p>La première solution qui a permis aux applications d'utiliser des noms était le fichier <span class="em">hosts.txt</span>. Ce fichier est similaire à la table des symboles que l'on trouve dans le code compilé. Il contient la correspondance entre le nom de chaque hôte Internet et son adresse IP associée. Il était géré par SRI International, qui coordonnait le <span class="html">Centre d'information réseau (NIC pour Network Information Center)</span>. Lorsq'un nouvel hôte était connecté au réseau, l'administrateur système devait enregistrer son nom et son adresse IP auprès du NIC. Le NIC mettait à jour le fichier hosts.txt sur son serveur. Tous les hôtes Internet récupéraient régulièrement le fichier hosts.txt mis à jour à partir du serveur maintenu par SRI. Ce fichier était stocké à un emplacement bien connu sur chaque hôte Internet (voir RFC 952) et les applications réseau pouvaient l'utiliser pour trouver l'adresse IP correspondant à un nom.</p>
                    <p>Le fichier hosts.txt n'est plus maintenu. Une capture historique récupérée le 15 avril 1984 est disponible sur <a href="http://ftp.univie.ac.at/netinfo/netinfo/hosts.txt" target="_blank">http://ftp.univie.ac.at/netinfo/netinfo/hosts.txt</a>.</p>
                    <p>Un fichier hosts.txt peut être utilisé lorsqu'il y a jusqu'à quelques centaines d'hôtes sur le réseau. Cependant, il n'est clairement pas adapté à un réseau contenant des milliers ou des millions d'hôtes. Un problème clé dans un grand réseau est de définir un schéma de nommage approprié. L'ARPANet utilisait initialement un espace de noms plat, c'est-à-dire que chaque hôte se voyait attribuer un nom unique. Pour limiter les collisions entre les noms, ces derniers contenaient généralement le nom de l'institution et un suffixe pour identifier l'hôte à l'intérieur de l'institution (une sorte de système de nommage hiérarchique du pauvre). Sur l'ARPANet, peu d'institutions avaient plusieurs hôtes connectés au réseau.</p>
                    <p>Cependant, les limites d'un schéma de nomenclature plat sont devenues évidentes avant la fin de l'ARPANet et le RFC 819 a proposé un schéma de nomenclature hiérachique. Bien que le RFC 819 ait discuté de la possibilité d'organiser les noms en tant que graphe orienté, Internet a finalemnt opté pour une structure en arborescence capable de contenir tous les noms. Dans cette arborescence, les domaines de premier niveau sont ceux qui sont directement attachés à la racine. Le premier domaine de premier niveau était <span class="em">.arpa</span> (voir <a href="http://www.donelan.com/dnstimeline.html" target="_blank">http://www.donelan.com/dnstimeline.html</a> pour une chronologie des développements liés à DNS). Ce nom de niveau supérieur a été initialement ajouté comme suffixe aux noms des hôtes attachés à l'ARPANet et listés dans le fichier hosts.txt. EN 1984, les noms de domaine génériques de premier niveau <span class="em">.gov</span>, <span class="em">.edu</span>, <span class="em">.com</span>, <span class="em">.mil</span> et <span class="em">.org</span> ont été ajoutés et le RFC 1032 a proposé l'utilisation des codes de pays <span class="em">ISO-3166</span> à deux lettres comme noms de domaine de premier niveau. Comme ISO-3166 définit un code à deux lettres pour chaque pays reconnu par les Nations unies, cela a permis à tous les pays d'avoir automatiquement un domaine de premier niveau. Ces domaines <span class="em">.be</span> pour la Belgique, <span class="em">.fr</span> pour la France, <span class="Em">.us</span> pour les États-Unis, <span class="em">.ie</span> pour l'Irlande ou <span class="em">.tv</span> pour Tuvalu, un groupe de petites îles dans le Pacifique et <span class="em">.tm</span> pour le Turkménistan. Aujourd'hui, l'ensemble des noms ou de domaine de premier est géré par la <span class="html">Corporation pour l'attribution des noms et des numéros sur Internet (ICANN pour "Internet Corporation for Assigned Names and Numbers")</span>. Récemment, l'ICANN a ajouté une douzaine de domaines de premier niveau génériques qui ne sont pas liés à un pays et le domaine de premier niveau <span class="em">.cat</span> a été enregistré pour la langue catalane. Des discussions sont en cours au sein de l'ICANN pour augmenter le nombre de domaines de premier niveau.</p>
                    <p>Chaque domaine de premier niveau est géré par une organisation qui décide de la manière dont les noms de sous-domaines peuvent être enregistrés. La plupart des noms de domaine de premier niveau utilisent un système du premier arrivé, premier servi et permettent à quiconque d'enregistrer des noms de domaine, mais il y a des exceptions. Par exemple, <span class="em">.gov</span> est réservé au gouvernement américain, <span class="em">.int</span> est réservé aux organisations internationales et les noms en <span class="em">.ca</span> sont principalement réservés aux entreprises ou aux utilisateurs présents au Canada.</p>
                    <figure>
                        <img src="../images/arbre_noms_domaine.PNG" alt="">
                        <figcaption>Figure 3.6 : L'arbre des noms de domaine</figcaption>
                    </figure>
                    <p>RFC 1035 a recommandé la BNF suivante pour les noms de domaine complets, afin de permettre aux noms d'hôtes d'avoir une syntaxe qui fonctionne avec toutes les applications (les noms de domaine eux-mêmes ont une syntaxe beacuoup plus riche).</p>
                    <figure>
                        <img src="../images/BNF_noms_domaine_complets.PNG" alt="">
                        <figcaption>Figure 3.7 : BNF des noms de domaine complets</figcaption>
                    </figure>
                    <p>Cette grammaire spécifie qu'un nom d'hôte est une liste ordonnée d'étiquettes spéparées par le caractère point (.). Chaque étiquette peut contenir des lettres, des chiffres et le caractère tiret (-). Cette spécification a évolué ultérieurement pour prendre en charge des noms de domaine écrits en utilisant d'autres jeux de caractères que l'us-ASCII RFC 5890. Cette extension est importante pour prendre en charge des langues autres que l'anglais, mais une discussion détaillée sort du cadre de ce document. Les noms de domaine complets sont lus de gauche à droite. La première étiquette est un nom d'hôte ou un nom de domaine suivi de la hiérarchie des domaines et se terminant implicitement à droite par la racine. Le nom de domaine de premier niveau doit être l'un des TLD enregistrés. La liste officielle des noms de domaine de premier niveau est gérée par le terme : `IANA` à l'adresse <a href="http://data.iana.org/TLD/tlds-alpha-by-domain.txt" target="_blank">http://data.iana.org/TLD/tlds-alpha-by-domain.txt</a>. Des informations supplémentaires sur ces domaines peuvent être trouvées à l'adresse <a href="http://en.wikipedia.org/wiki/List_of_Internet_top-level_domains" target="_blank">http://en.wikipedia.org/wiki/List_of_Internet_top-level_domains</a>. Par exemple, dans la figure ci-dessus, <a href="http://www.whitehouse.gov/" target="_blank">http://www.whitehouse.gov/</a> correspond à un hôte nommé <span class="em">www</span> du domaine <span class="em">whitehouse</span> qui appartient au domaine de premier niveau <span class="em">gov</span>. <span class="em">info.ucl.ac.be</span> correspond au domaine <span class="em">info</span> à l'intérieur du domaine <span class="em">ucl</span> qui est inclus dans le sous-domaine <span class="em">ac</span> du domaine de premier niveau <span class="em">be</span>.</p>
                    <p>Le schéma de nommage hiérarchique est un élément clé du système de noms de domaine (DNS). Le DNS est une base de données distribuée qui contient des correspondances entre des noms de domaine complets et des adresses IP. Le DNS utilise le modèle client-serveur. Les clients sont des hôtes qui ont besoin de récupérer la correspondance pour un nom donné. Chaque serveur de noms stocke une partie de la base de données distribuée et répond aux requêtes envoyées par les clients. Il y a au moins un serveur de noms qui est responsable de chaque domaine. Dans la figure ci-dessous, les domaines sont représentés par des cercles et il y a trois hôtes dans le domaine <span class="em">dom</span> (<span class="em">h1</span>, <span class="em">h2</span> et <span class="em">h3</span>) et trois hôtes dans le domaine <span class="em">a.sdom.dom</span>. Comme le montre la figure ci-dessous, un sous-domaine peut contenir à la fois des noms d'hôtes et des sous-domaines.</p>
                    <figure>
                        <img src="../images/arbre_simple_noms_domaine.PNG" alt="">
                        <figcaption>Figure 3.8 : Un arbre simple de noms de domaine</figcaption>
                    </figure>
                    <p>Un serveur de noms qui est responsable du domaine <span class="em">dom</span> peut répondre directement aux requêtes suivantes :</p>
                    <ul>
                        <li>
                            <p>l'adresse IP de tout hôte résidant directement dans le domaine <span class="em">dom</span> (par exemple, <span class="em">h2.dom</span> dans la figure ci-dessus)</p>
                        </li>
                        <li>
                            <p>le(s) serveur(s) de noms qui sont responsables de tout sous-domaine direct du domaine <span class="em">dom</span> (c'est-à-dire <span class="em">sdom1.dom</span> et <span class="em">sdom2.dom</span> dans la figure ci-dessus, mais pas <span class="em">z.sdom1.dom</span>)</p>
                        </li>
                    </ul>
                    <p>Pour récupérer la correspondance pour l'hôte <span class="em">h2.dom</span>, un client envoie sa requête au serveur de noms qui est responsable du domaine <span class="em">.dom</span>. Le serveur de noms répond directement à la requête. Pour récupérer une correspondance pour <span class="em">h3.a.sdom1.dom</span>, un client DNS envoie d'abord une requête au serveur de noms qui est responsable du domaine <span class="em">.dom</span>. Ce serveur de noms renvoie le serveur de noms qui est responsable du domaine <span class="em">sdom1.dom</span>. Ce serveur de noms peut maintenant être contacté pour obtenir le serveur de noms qui est responsable du domaine <span class="em">a.sdom1.dom</span>. Ce serveur de noms peut être contacté pour récupérer la correspondance pour le nom <span class="em">h3.a.sdom1.dom</span>. Grâce à cette organisation des serveurs de noms, il est possible pour un client DNS d'obtenir la correspondance de n'importe quel hôte à l'intérieur du domaine <span class="em">.dom</span> ou de l'un de ses sous-domaines. Pour garantir qu'un client DNS sera en mesure de résoudre n'importe quel nom de domaine pleinement qualifié, il existe des serveurs de noms spéciaux qui sont responsables de la racine de la hiérarchie des noms de domaine. Ces serveurs de noms sont appelés serveurs de noms racine. Il existe actuellement une douzaine de serveurs de noms racine. En pratique, certains de ces serveurs de noms racine sont eux-mêmes mis en oeuvre sous la forme d'un ensemble de serveurs physiques distincts. Voir <a href="http://www.root-servers.org/" target="_blank">http://www.root-servers.org/</a> pour plus d'informations sur l'emplacement physique de ces serveurs.</p>
                    <p>Chaque serveur de noms racine maintient la liste de tous les serveurs de noms qui sont responsables de chacun des noms de domaines de premier niveau et de leurs adresses IP. Une copie des informations maintenues par chaque serveur de noms racine est disponible sur <a href="http://www.internic.net/zones/root.zone" target="_blank">http://www.internic.net/zones/root.zone</a>. Jusqu'en février 2008, les serveurs DNS racine n'avaient que des adresses IPv4. Les adresses IPv6 ont ét ajoutées lentement aux serveurs DNS racine pour éviter de créer des problèmes comme discuté dans <a href="http://www.icann.org/en/committees/security/sac018.pdf" target="_blank">http://www.icann.org/en/committees/security/sac018.pdf</a>. En 2010, plusieurs serveurs DNS racine ne sont toujours pas accessibles en utilisant IPv6. Tous les serveurs de noms racine sont synchronisés et fournissent les mêmes réponses. En interrogeant l'un des serveurs de noms racine, un client DNS peut obtenir le serveur de noms qui est responsable de n'importe quel nom de domaine de premier niveau. À partir de ce serveur de noms, il est possible de résoudre n'importe quel nom de domaine.</p>
                    <p>Pour être capable de contacter les serveurs de noms racine, chaque client DNS doit connaître leurs adresses IP. Cela implique que les clients DNS doivent maintenir une liste à jour des adresses IP des serveurs de noms racine. La liste actuelle des adresses IP des serveurs de noms racine est maintenant <a href="http://www.internic.net/zones/named.root" target="_blank">http://www.internic.net/zones/named.root</a>. Ces adresses IP sont stables et les serveurs de noms racine changent rarement leurs adresses IP. Les résolveurs DNS doivent cependant maintenir une copie à jour de ce fichier. Sans cette liste, il est impossible de contacter les serveurs de noms racine. Forcer tous les hôtes Internet à maintenir la version la plus récente de cette liste serait difficile d'un point de vue opérationnel. Pour résoudre ce problème, les concepteurs du DNS ont introduit un type spécial de serveur DNS : les résolveurs DNS. Un résolveur est un serveur qui fournit le service de résolution de noms pour un ensemble de clients. Un réseau contient généralment quelques résolveurs. Chaque hôte de ces réseaux est configuré pour envoyer toutes ses requêtes DNS via l'un de ses résolveurs locaux. Ces requêtes sont appelées des requêtes récursives car le résolveur doit faire une récursion à travers la hiérarchie des serveurs de noms pour obtenir la réponse.</p>
                    <p>Les résolveurs DNS présentent plusieurs avantages par rapport à laisser chaque hôte Internet interroger directement les serveurs de noms. Tout d'abord, les hôtes Internet classiques n'ont pas besoin de maintenir une liste à jour des adresses IP des serveurs racine. Deuxièmement, les hôtes Internet classiques n'ont pas besoin d'envoyer des requêtes à des serveurs de noms partout sur Internet. De plus, étant donné qu'un résolveur DNS sert un grand nombre d'hôtes, il peut mettre en cache les réponses reçues. Cela permet au résolveur de renvoyer rapidement les réponses pour les requêtes DNS populaires et réduit la charge sur tous les serveurs DNS [JSBM2002].</p>
                    <p>Le dernier composant du système de noms de domaine est le protocole DNS. Le protocole DNS s'exécute au-dessus des services de datagramme et de flux de données. En pratique, le service de datagramme est utilisé lorsque des requêtes et des réponses courtes sont échangées, et le service de flux de données est utilisé lorsque des réponses plus longues sont attendues. Dans cette section, nous ne discuterons que de l'utilsation du protocole DNS au-dessus du service de datagramme. C'est l'utilisation la plus fréquente du DNS.</p>
                    <p>Les messages DNS sont composés de cinq parties appelées sections dans la RFC 1035. Les trois premières sections sont obligatoires et les deux dernières sont optionnelles. La première section d'un message DNS est son en-tête. Elle contient des informations sur le type de message et le contenu des autres sections. La deuxième section contient la question envoyée au serveur de noms ou au résolveur. La troisième section contient la réponse à la question. Lorsqu'un client envoie une requête DNS, la section Réponse est vide. La quatrième section, nommée Autorité, contient des informations sur les serveurs pouvant fournir une réponse autoritative si nécessaire. La dernière section contient des informations supplémentaires fournies par le résolveur ou le serveur mais qui n'ont pas été demandées dans la question.</p>
                    <p>L'en-tête des messages DNS est composé de 12 octets et sa structure est illustrée dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/entete_DNS.PNG" alt="">
                        <figcaption>Figure 3.9 : En-tête DNS</figcaption>
                    </figure>
                    <p>L'<span class="html">identifiant (ID)</span> est une valeur aléatoire de 16 bits choisie par le client. Lorsqu'un client envoie une requête à un serveur DNS, il se souvient de la requête et de son identifiant. Lorsqu'un serveur renvoie une réponse, il renvoie dans le champ <span class="em">ID</span> l'identifiant choisi par le client. Grâce à cet identifiant, le client peut faire correspondre la réponse reçue à la question qu'il a envoyée.</p>
                    <p>Le drapeau <span class="html">QR</span> est mis à <span class="em">0</span> dans les requêtes DNS et <span class="em">1</span> dans les réponses DNS. L'<span class="html">Opcode</span> est utilisé pour spécifier le type de requête. Par exemple, une <span class="em">requête standard</span> est lorsque le client envoie un <span class="em">nom</span> et que le serveur renvoie les <span class="em">données</span> correspondantes, et une demande de mise à jour est lorsque le client envoie un <span class="em">nom</span> et de nouvelles <span class="em">données</span> et que le serveur met ensuite à jour sa base de données.</p>
                    <p>Le bit <span class="html">AA</span> est mis lorsque le serveur qui a envoyé la réponse a <span class="em">l'autorité</span> pour le nom de domaine trouvé  dans la section de question. Dans les déploiempents DNS originaux, deux types de serveurs étaient considérés : les serveurs <span class="html">autoritaires</span> et les serveurs <span class="html">non-autoritaires</span>. Les serveurs <span class="em">autoritaires</span> sont gérés par les administrateurs système responsables d'un domaine donné. Ils stockent toujours les infomations les plus récentes sur un domaine. Les serveurs <span class="em">non-autoritaires</span> sont des serveurs ou des résolveurs qui stockent des informations DNS sur des domaines externes sans être gérés par les propriétaires d'un domaine. Ils peuvent donc founir des réponses qui sont obsolètes. Du point de vue de la sécurité, le bit <span class="em">autoritaire</span> n'est pas une indication absolue de la validité d'une réponse. La sécurisation du système de noms de domaine est un problème complexe qui n'a été résolu de manière satisfaisante que récemment par l'utilisation de signatures cryptographiqes dans les extensions DNSSEC à DNS décrites dans la RFC 4933. Cependant, ces extensions sont hors du champ d'application de ce chapitre.</p>
                    <p>Le bit <span class="html">RD (recursion desired)</span> est positionné par un client lorsqu'il envoie une requête à un résolveur. Une telle requête est dite <span class="html">récursive</span> car le résolveur effectuera une recherche récursive dans la hiérarchie DNS pour récupérer la réponse au nom du client. Dans le passé, tous les résolveurs étaient configurés pour effectuer des requêtes récursives au nom de n'importe quel hôte Internet. Cependant, cela expose les résolveurs à plusieurs risques de sécurité. Le plus simple est que le résolveur pourrait être surchargé en ayant trop de requêtes récursives à traiter. Au moment de la rédaction de ce texte, la plupart des résolveurs n'autorisent que les requêtes récursives des clients appartenant à leur entreprise ou réseau, et rejettent toutes les autres requêtes récursives. Certains résolveurs DNS permettent à n'importe quel hôte d'envoyer des requêtes. <span class="em">OpenDNS</span> et <span class="em">GoogleDNS</span> sont des exemples de résolveurs ouverts. Le bit <span class="html">RA</span> indique si le serveur prend en charge la récursivité. Le code de réponse <span class="html">RCODE</span> est utilisé pour distinguer différents types d'erreurs. Voir RFC 1035 plus plus de détails. Les quatres derniers champs indiquent la taille des sections <span class="html">Question</span>, <span class="html">Réponse</span>, <span class="html">AUtoritaire</span> et <span class="html">Aditionnelle</span> du message DNS.</p>
                    <p>Les quatre dernières sections du message DNS contiennent des <span class="html">enregistrements de ressources (RR pour Ressouce Records)</span>. Tous les RR ont le même format de niveau supérieur dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/enregistrements_ressources_DNS.PNG" alt="">
                        <figcaption>Figure 3.10 : Enregistrements de ressources DNS</figcaption>
                    </figure>
                    <p>Dans un enregistrement de ressources DNS (RR), le champ "<span class="html">Name</span>" indique le nom du noeud auquel cet enregistrement de ressources se rapporte. Les deux octets du champ "<span class="html">Type</span>" indiquent le type d'enregistrement de ressources. Le champ "<span class="html">Class</span>" était utilisé pour prendre en charge l'utilisation du DNS dans d'autres environnements que l'Internet.</p>
                    <p>Le champ "<span class="html">TTL (Time to Live)</span>" indique la durée de vie de l'<span class="em">enregistrement de ressources</span> en secondes. Ce champ est défini par le serveur qui renvoie une réponse et indique pendant combien de temps un client ou un résolveur peut stocker l'<span class="em">enregistrement de ressources</span> dans son cache. Un <span class="em">TTL</span> long indique un <span class="em">RR</span> stable. Cetaines entreprises utilisent des valeurs <span class="em">TTL</span> courtes pour les hôtes mobiles et également pour les serveurs populaires. Par exemple, une entreprise d'hébergement web qui veut répartir la charge sur un pool de vent serveurs peut configurer ses serveurs de noms pour renvoyer des réponses différentes à différents clients. Si chaque réponse a un petit <span class="em">TTL</span>, les clients seront obligés d'envoyer régulièrement des requêtes DNS. Le serveur de noms répondra à ces requêtes en fournissant l'adresse du serveur le moins chargé.</p>
                    <p>Le champ "<span class="html">RDLength</span>" est la longueur du champ "<span class="html">RData</span>" qui contient les informations du type spécifié dans le champ "<span class="html">Type</span>".</p>
                    <p>Plusieurs types de RR DNS sont utilisés en pratique. Le type <span class="html">A</span> est utilisé pour coder l'adresse IPv4 correspondant au nom spécifié. Le type <span class="html">AAAA</span> est utilisé pour coder l'adresse IPv6 correspondant au nom spécifié. Un enregistrement <span class="html">NS</span> contient le nom de serveur DNS qui est responsable d'un domaine donné. Par exemple, une requête pour l'enregistrement <span class="em">A</span> associé au nom <a href="http://www.ietf.org/" target="_blank">http://www.ietf.org/</a> retourne la réponse suivante.</p>
                    <p>Cette réponse contient plusieurs informations. Tout d'abord, le nom <a href="http://www.ietf.org/" target="_blank">http://www.ietf.org/</a> est associé à l'adresse IP <span class="em">64.170.98.32</span>. Deuxièmement, le domaine <span class="em">ietf.org</span> est géré par six szeveurs DNS différents. Trois de ces serveurs DNS sont accessibles via IPv4 et IPv6. Deux d'entre eux ne sont pas accesibles via IPv6 et <span class="em">ns0.ietf.org</span> n'est accessible que via IPv6. Une requête pour l'enregistrement <span class="em">AAAA</span> associé à <a href="http://www.ietf.org/" target="_target">http://www.ietf.org/</a> renvoie <span class="Em">2001:1890:1112:1::20</span> ainsi que les mêmes sections d'autorité et d'informations supplémentaires.</p>
                    <p>Les <span class="html">CNAME</span> (ou noms canoniques) sont utilisés pour définir des alias. Par exemple, <span class="em">www.example.com/</span> pourrait être un <span class="em">CNAME</span> pour <span class="em">pc12.example.com</span> qui est le nom réel du serveur sur lequel le serveur web pour <span class="em">www.example.com</span> s'exécute.</p>
                    <hr>
                    <p>Note : DNS inversé et in-addr.arpa :</p>
                    <figure>
                        <img src="../images/requete_enregistrement_A_ietf_org.PNG" alt="">
                        <figcaption>Figure 3.11 : Requête pour l'enregistrement A de www.ietf.org</figcaption>
                    </figure>
                    <p>Le DNS est principalement utilisé pour trouver l'adresse IP qui correspond à un nom donné. Cependant, il est parfois utile d'obtenir le nom qui correspond à une adresse IP. Cela est fait en utilisant le <span class="html">RR PTR (pointeur)</span>. La partie <span class="html">RData</span> d'un <span class="em">RR PTR</span> contient le nom tandis que la partie <span class="em">Name</span> du <span class="em">RR</span> contient l'adresse IP encodée dans le domaine <span class="em">in-addr.arpa</span>. Les adresses IPv4 sont encodées dans <span class="em">in-addr.arpa</span> en inversant les quatre chiffres qui composent la représentation décimale pointée de l'adresse. Par exemple, considérons l'adresse IPv4 <span class="em">192.0.2.11</span>. Le nom d'hôte associé à cette adresse peut être trouvé en demandant le <span class="em">RR PTR</span> qui correspond à <span class="em">11.2.0.192.in-addr.arpa</span>. Une solution similaire est utilisée pour prendre en charge les adresses IPv6, voir RFC 3596.</p>
                    <hr>
                    <p>Un point important à noter concernant le système de noms de domaine est son extensibilité.</p>
                    <h4>3.4.2 Courrier électronique :</h4>
                    <p>Le courrier életronique, ou e-mail, est une application très populaire dans les réseaux infomatiques tels que l'Internet. L'e-mail est apparu au début des années 197° et permet aux utilisateurs d"échanger des messages basés sur du texte. Initialement, il était principalement utilisé pour échanger des messahes courts, mais au fil des ans son utilisation s'est développée. Il est maintenant utilisé non seulement pour échanger des petits messages, mais aussi des messages longs qui peuvent être composés de plusieurs parties comme nous le verrons plus tard.</p>
                    <p>Avant d'examiner les détails de l'e-mail Internet, considérons un scénario simple illustré dans la figure ci-dessous, où Alice envoie un e-mail à Bob. Alice prépare son e-mail en utilisant un <span class="html">client e-mail</span> et l'envoie à son <span class="html">serveur e-mail</span>. Le serveur e-mail d'Alice extrait l'adresse de Bob de l'e-mail et délivre le message au serveur de Bob. Bon récupère le message d'Alice sur son serveur et le lit en utilisant son client e-mail préféré ou via son interface webmail.</p>
                    <figure>
                        <img src="../images/architecture_simplifiee_email_internet.PNG" alt="">
                        <figcaption>Figure 3.12 : Architecture simplifiée de l'e-mail Internet</figcaption>
                    </figure>
                    <p>Le système de messagerie électronique que nous considérons dans ce livre est composé de quatre éléments :</p>
                    <ul>
                        <li>
                            <p>un format de message, qui définit comment les messages électroniques valides sont encodés</p>
                        </li>
                        <li>
                            <p>des protocoles, qui permettent aux hôtes et serveurs d'échanger des messages électroniques</p>
                        </li>
                        <li>
                            <p>des logiciels clients, qui permettent aux utilisateurs de créer et lire facilement des messages électroniques</p>
                        </li>
                        <li>
                            <p>des logiciels, qui permettent aux serveurs déchanger efficacement des messages électroniques</p>
                        </li>
                    </ul>
                    <p>Nous allons d'abord discuter du format des messages électriniques, suivi des protocoles utilisés sur l'Internet actuel pour échanger et récupérer les e-mails. D'autres systèmes de messagerie électronique ont été développés dans le passé [Bush1993] [Genilloud1990] [GC2000], mais aujourd'hui la plupart des solutions de messagerie électronique ont migré vers l'e-mail sur Internet. Des informations sur les logiciels utilisés pour composer et livrer des e-mails peuvent être trouvées sur Wikipédia, entre autres, pour les clients de messagerie et les serveurs de messagerie. Des informations plus détaillées sur l'ensemble de l'architecture de messagerie Internet peuvent être toruvées dans la RFC 5598.</p>
                    <p>Les messages éctroniques, comme le courrier postal, sont composés de deux parties :</p>
                    <ul>
                        <li>
                            <p>l'<span class="html">en-teête</span> qui joue le même rôle que l'en-tête d'une lettre dans le courrier régulier. Il contient des métadonnées sur le message.</p>
                        </li>
                        <li>
                            <p>le <span class="html">corps</span> qui contient le message lui-même.</p>
                        </li>
                    </ul>
                    <p>Les messages éclectroniques sont entièrement composés de lignes de caractères ASCII. Chaque ligne peut contenir jusqu'à 998 caractères et est terminée par les caractères de contrôle <span class="em">CR</span> et <span class="em">LF</span> RFC 5322. Les lignes qui composent l'en-tête apparaissent avant le corps du message. Une ligne vide, contenant uniquement les caractères <span class="em">CR</span> et <span class="em">LF</span>, marque la fin de l'en-tête. Cela est illustré dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/structure_messages_email.PNG" alt="">
                        <figcaption>Figure 3.13 : La structure des messages électroniques</figcaption>
                    </figure>
                    <p>L'en-tête d'un e-mail contient plusieurs lignes qui commencent toutes par mot-clé suivi d'un deux-points et d'informations supplémentaires. Le format des messages électroniques et les différents types de lignes d'en-tête sont définis dans la norme RFC 5322. Deux de ces lignes d'en-tête sont obligatoires et doivent apparaître dans tous les messages électroniques :</p>
                    <ul>
                        <li>
                            <p><span class="html">L'adresse de l'expéditeur</span> : Cette ligne d'en-tête commence par <span class="html">From :</span>. Elle contient le nom (optionnel) de lexpéditeur suivi de son adresse électronique entre chevrons. Les adresses adresses électroniques sont toujours composées d'un nom d'utilisateur suivi du signe <span class="html">@</span> et d'un nom de domaine.</p>
                        </li>
                        <li>
                            <p><span class="html">La date</span> : Cette ligne d'en-tête commence par <span class="html">Date :</span>. La norme RFC 5322 définit précisément le format utilisé pour encoder une date.</p>
                        </li>
                    </ul>
                    <p>D'autres lignes d'en-tête apparaissent dans la plupart des messages électroniques. La ligne d'en-tête "<span class="html">Subject :</span>" permet à l'expéditeur d'indiquer le sujet traité dans l'e-mail. Trois types de lignes d'en-tête peuvent être utilisés pour spécifier les destinataires d'un message :</p>
                    <ul>
                        <li>
                            <p>La ligne d'en-tête "<span class="html">To :</span>" contient les adresses électroniques des desyonataires principaix du message. Il peut être surprenant que la ligne d'en-tête "<span class="em">To :</span>" ne soit pas obligatoire dans un message électronique. Bien que la plupart des messages électroniques contiennent cette ligne d'en-tête, un e-mail qui ne contient pas de ligne d'en-tête "<span class="em">To :</span>" et qui utilise plutôt la ligne d'en-tête "<span class="html">bcc :</span>" pour spécifier le destinataire est également valide. Plusieurs adresses peuvent être séparées par des virgules.</p>
                        </li>
                        <li>
                            <p>La ligne d'en-tête "<span class="html">cc :</span>" est utilisée par l'expéditeur pour fournir une liste d'adresses électroniques qui doivent recevoir une copie carbone du message. Plusieurs adresses peuvent être listées dans cette ligne d'en-tête, séparées par des virgules. Tous les destinataires du message électronique reçoivent les lignes d'en-tête "<span class="em">To :</span>" et "<span class="em">cc :</span>".</p>
                        </li>
                        <li>
                            <p>La ligne d'en-tête "<span class="html">bcc :</span>" est utilisée par l'expéditeur pour fournit une liste d'adresses e-mail séparées par des virgules qui doivent recevoir une copie carbone invisible du message. La ligne d'en-tête "<span class="em">bcc :</span>" n'est pas livrée aux destinataires du message électronique.</p>
                        </li>
                    </ul>
                    <p>Voici ci-dessous un simple message électronique contenant les en-têtes <span class="html">From:</span>, <span class="html">To:</span>, <span class="html">Subject:</span> et <span class="html">Date:</span>, ainsi que lignes de corps de message.</p>
<pre><code lang="en">From: Bob Smith <Bob@machine.example>
To: Alice Doe <alice@example.net>, Alice Smith <Alice@machine.example> Subject: Hello
Date: Mon, 8 Mar 2010 19:55:06 -0600

This is the "Hello world" of email messages.
This is the second line of the body</code></pre>
                    <p>Notez la ligne vide après l'en-tête "<span class="html">Date :</span>"; cette ligne vide ne contient que les caractères CR et LF et marque la limite entre l'en-tête et le corps du message.</p>
                    <p>Plusieurs autres en-têtes facultatifs sont définis dans la RFC 5322 et ailleurs. La liste de toutes les lignes d'en-tête  de courrier électronique standard peut être trouvée à l'adresse suivante :<a href="http://www.iana.org/assignments/message-headers/message-header-index.html" target="_blank">http://www.iana.org/assignments/message-headers/message-header-index.html</a>. En outre, de nombreux clients de messagerie électronique et serveurs définissent leurs propres en-têtes commençant par <span class="html">X-</span>. Plusieurs des en-têtes facultatifs définis dans la RFC 5322 méritent d'être discutés ici :</p>
                    <ul>
                        <li>
                            <p>La ligne d'en-tête "<span class="html">Message-Id</span>" est utilisée pour associer un identifiant "unique" à chaque e-mail. Les identifiants d'e-mail sont généralement structurés comme <span class="em">string@domain</span> où <span class="em">string</span> est une chaîne de caractères unique ou un numéro de séquence choisi par l'expéditeur de l'e-mail et <span class="em">domain</span> est le nom de domaine de l'expéditeur. Étant donné que les noms de domaines sont uniques, un hôte peut générer des identifiants de message globalement uniques en concaténant un identifiant loclament unique avec son nom de domaine.</p>
                        </li>
                        <li>
                            <p>La ligne d'en-tête "<span class="html">In-reply-to :</span>" est utilisée lorsqu'un message a été créé en réponse à un message précédent. Dans ce cas, la fin de la ligne "<span class="em">In-reply-to :</span>" contient lidentifiant du message original.</p>
                        </li>
                        <li>
                            <p>La ligne d'en-tête "<span class="html">Received :</span>" est utilisée lorsqu'un message électronique est traité par plusieurs serveurs avant d'atteindre sa destination. Chaque serveur de messagerie intermédiaire ajoute une ligne d'en-tête "<span class="em">Received :</span>" . Ces lignes d'en-tête sont utiles pour déboguer les problèmes de distribution des messages électronisques.</p>
                        </li>
                    </ul>
                    <p>La figure ci-dessous montre les lignes d'en-tête d'un message électronique. Le message a été envoyé depuis un hôte nommé wira.firstpr.com.au et a été reçu par smtp3.sgsi.ucl.ac.be. Les lignes "<span class="em">Received :</span>" ont été enveloppées pour faciliter la lecture.</p>
                    <figure>
                        <img src="../images/received_message-id_email_message.PNG" alt="">
                    </figure>
                    <p>Au départ, l'e-mail était utilisé pour échanger des petits messages de texte ASCII entre les informaticiens. Cependant, avec la croissance d'Internet, le fait de ne supporter que le texte ASCII est devenu une limitation importante pour deux raisons. Tout d'abord, les locuteurs non-anglophones voulaient écrire des e-mails dans leur langue maternelle qui n"cessitait souvent plus de caractères que ceux de la table de caractères ASCII. Deuxièmement, de nombreux utilisateurs voulaient envoyer d'autres types de contenus que simplement du texte ASCII par e-mail, tels que des fichiers binaires, des images ou du son.</p>
                    <p>Pour résoudre ce problème, l'IETF a développé les <span class="html">Multipurpose Internet Mail Extensions (MIME)</span>. Ces extensions ont été soigneusement conçues pour permettre aux e-mails d'Internet de transporter des caractères non-ASCII et des fichiers binaires sans perturber les serveurs de messagerie électronique qui étaient déployés à cette époque. Cette exigence de compatibilité ascendante a obligé les concepteurs MIME à développer des extensions au format de message e-mail existant RFC 822 au lieu de définir un nouveau format complètement différent qui aurait été mieux adapté pour prendre en charge les nouveaux types d'e-mails.</p>
                    <p>RFC 2045 définit trois nouveaux types de lignes d'en-tête pour prendre en charge MIME :</p>
                    <ul>
                        <li>
                            <p>L'en-tête <span class="html">MIME-Version</span> indique la version de la spécification MIME qui a été utilisée pour encoder le message électronique. La version actuelle de MIME est 1.0. D'autres versions de MIME peuvent être définies à l'avenir. Grâce à cette ligne d'en-tête, le logiciel qui traite les messages électroniques pourra s'adapter à la version MIME utilisée  pour encoder le message. Les messages qui ne contiennent pas cette en-tête doivent être formatés selon la spécification RFC 822 d'origine.</p>
                        </li>
                        <li>
                            <p>La ligne d'en-tête <span class="html">Content-Type</span> indique le type de données qui est transporté à l'intérieur du message (voir ci-dessous).</p>
                        </li>
                        <li>
                            <p>La ligne d'en-tête <span class="html">Content-Transfer-Encoding</span> est utilisée pour spécifier la manière dont le message a été encodé. Lorsque MIME a été conçu, certains serveurs de messagerie ne pouvaient traiter que des messages contenant des caractères codés en utilisant l'ensemble de caractères ASCII à 7 bits. MIME permet l'utilisation d'autres encodages de caractères.</p>
                        </li>
                    </ul>
                    <p>À l'intérieur de l'en-tête de l'e-mail, la ligne d'en-tête <span class="em">Content-Type</span> indique comment le message électronique MIME est structuré. RFC 2046 définit l'utilisation de cette ligne d'en-tête. Les deux structures les plus courantes pour les messages MIME sont :</p>
                    <ul>
                        <li>
                            <p>La ligne d'en-tête <span class="html">Content-Type: multipart/mixed</span> indique que le message MIME contient plusieurs parties indépendantes. Par exemple, un tel message peut contenir une partie en texte brut et un fichier binaire.</p>
                        </li>
                        <li>
                            <p>La ligne d'en-tête <span class="html">Content-Type: multipart/alternative</span> peut contenir à la fois une version en texte brut et une version HTML du même texte.</p>
                        </li>
                    </ul>
                    <p>Pour prendre en charge ces deux types de messages MIME, le destinataire d'un message doit être capable d'extraire les différentes parties du message. Dans RFC 822, une ligne vide était utilisée pour séparer les lignes d'en-tête du corps. Utiliser une ligne vide pour séparer les différentes parties d'un corps de courrier électronique serait difficile car le corps des messages électroniques contient souvent une ou plusieurs lignes vides. Une autre option possible serait de définir une ligne spéciale, par exemple <span class="em">*-LAST_LINE-*</span>, pour marque la limite entre deux parties d'un message MIME. Malheureusement, cela n'est pas possible car certains e-mails peuvent contenir cette chaîne dans leur corps (par exemple, des e-mails envoyés aux étudiants pour expliquer le fomat des messages MIME). Pour résoudre ce problème, la ligne d'en-tête <span class="em">Content-Type</span> contient un deuxième paramètre qui spécifie la chaîne utilisée par l'expéditeur du message MIME pour délimiter les différentes parties. En pratique, cette chaîne est souvent choisie au hasard par le client de messagerie.</p>
                    <p>Le message électronique ci-dessous, copié à partir de RFC 2046, montre un message MIME contenant deux parties qui sont toutes deux en texte brut et encodées en utilisant l'ensemble de caractères ASCII. La chaîne simple <span class="html">boundary</span> est définie dans l'en-tête <span class="em">Content-Type</span> comme marqueur de la limite entre deux parties successives. Un autre exemple de messages MIME peut être trouvé dans RFC 2046.</p>
                    <figure>
                        <img src="../images/content-type_email_message.PNG" alt="">
                    </figure>
                    <p>L'en-tête <span class="em">Content-Type:</span> peut également être utilisé à l'intérieur d'une partie MIME. Dans ce cas, il indique le type de données placé dans cette partie. Chaque type de données est spécifiée peut être trouvée dans le RFC 2046. Voici quelques-unes des lignes d'en-tête <span class="em">Content-Type</span> les plus populaires :</p>
                    <ul>
                        <li>
                            <p><span class="html">text</span> : La partie du message contient des informations au format texte. Il existe plusieurs sous-types : <span class="html">text/plain</span> pour le texte ASCII régulier, <span class="html">text/html</span> défini dans le RFC 2854 pour les documents au format HTML ou le format <span class="html">text/enriched</span> défini dans le RFC 1896. La ligne d'en-tête <span class="em">Content-Type</span> peut contenir un deuxième paramètre qui spécifie le jeu de caractères utilisé pour coder le texte. <span class="html">charset=us-ascii</span> est le tableau de caractères ASCII standard. D'autres jeux de caractères fréquents incluent <span class="html">charset=UTF8</span> ou <span class="html">charset=iso-8859-1</span>. La liste des jeux de caractères standard est maintenue par l'IANA.</p>
                        </li>
                        <li>
                            <p><span class="html">image</span> : La partie du message contient une représentation binaire d'une image. Le sous-type indique le format de l'image comme <span class="html">gif</span>, <span class="html">jpg</span> ou <span class="html">png</span>.</p>
                        </li>
                        <li>
                            <p><span class="html">audio</span> : La partie du message contient un extrait audio. Le sous-type indique le format de l'extrait audio comme <span class="html">wav</span> ou <span class="html">mp3</span>.</p>
                        </li>
                        <li>
                            <p><span class="html">video</span> : La partie du message contient un extrait vidéo. Le sous-type indique le format de l'extrait vidéo comme <span class="html">avi</span> ou <span class="html">mp4</span>.</p>
                        </li>
                        <li>
                            <p><span class="html">application</span> : La partie du message contient des informations binaires produites par l'application particulière listée comme sous-type. Les clients de messagerie utilsent le sous-type pour lancer l'application qui est capable de décoder les informations binaires reçues.</p>
                        </li>
                    </ul>
                    <hr>
                    <p>Note : De ASCII à Unicode :</p>
                    <p>Les premiers ordinateurs utilisaient différentes techniques pour représenter les caractères en mémoire et sur disque. Pendant les années 1960, les ordinateurs ont commencé à échanger des informations via des bandes magnétiques ou des lignes téléphoniques. Malheureusement, chaque fournisseur avait son propre jeu de caractères propriétaire, et l'échange de données entre ordinateurs de différents fournisseurs était souvent difficile. La table de caractères ASCII à 7 bits du RFC 20 a été adoptée par plusieurs fournisseurs et par de nombreux protocoles Internet. Cependant, ASCII est devenu un problème avec l'internationalisation de l'Internet et le désir de plus en plus d'utilisateurs d'utiliser des jeux de caractères qui prennent en charge leur propre langue écrite. Une première tentative de résolution de ce problème a été la définition des jeux de caractères ISO-8859 par l'ISO. Cette famille de normes spécifiait différents de caractères qui permettaient la représentation de nombreuses langues écrites européennes en utilisant des caractères à 8 bits. Malheureusement, un jeu de caractères à 8 bits n'est pas suffisant pour prendre en charge certaines langues largement utilisées, telles que celles utilisées dans les pays asiatiques. Heureusement, à la fin des années 1980, plusieurs informaticiens ont proposé de développer une norme qui prend en charge toutes les langues écrites utilisées sur Terre aujourd'hui. La norme Unicode [Unicode] a maintenant été adoptée par la plupart des fournisseurs d'ordinateurs et de logiciels. Par exemple, Java utilise Unicode nativement pour manipuler des caractères, Python peut gérer à la fois des caractères ASCII et Unicode. Les applications Internet se déplacent lentement vers une prise en charge complète des ensembles de caractères Unicode, mais passer d'ASCII à Unicode est un changement important qui peut avoir un impact énorme sur les implémentations actuellement déployées. Voir, par exemple, le travail pour internationaliser complètement le courrier électronique RFC 4952 et les noms de domaine RFC 5890.</p>
                    <hr>
                    <p>La dernière ligne d'en-tête MIME est <span class="html">Content-Transfer-Encoding:</span>. Cette ligne d'en-tête est utilisée après la ligne d'en-tête <span class="em">Content-Type:</span>, dans une partie de message, et spécifie comment la partie de message a été encodée. L'encodage par défaut consiste à utiliser l'ASCII à 7 bits. Les encodages les plus fréquents  sont <span class="html">quoted-printable</span> et <span class="html">Base64</span>. Les deux prennent en charge l'encodage d'une séquence d'octets en un ensemble de lignes ASCII qui peuvent être transmises en toute sécurité par les serveurs de messagerie électronique. Le <span class="em">quoted-printable</span> est défini dans le RFC 2045. Nous décrivons brièvement la <span class="em">Base64</span> qui est définie dans le RFC 2045 et le RFC 4648.</p>
                    <p>La <span class="em">Base64</span> divise la séquence d'octets à encoder en groupes de trois octets (le dernier groupe pouvant être partiellement rempli). Chaque groupe de trois octets est ensuite divisé en quatre champs de six bits et chaque champ de six bits est encodé en tant que caractères à partir du tableau ci-dessous.</p>
                    <table class="tableBalises" role="presentation">
                        <thead>
                            <tr>
                                <th>Valeur</th>
                                <th>Encodage</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>0</td>
                                <td>A</td>
                            </tr>
                            <tr>
                                <td>1</td>
                                <td>B</td>
                            </tr>
                            <tr>
                                <td>2</td>
                                <td>C</td>
                            </tr>
                            <tr>
                                <td>3</td>
                                <td>D</td>
                            </tr>
                            <tr>
                                <td>4</td>
                                <td>E</td>
                            </tr>
                            <tr>
                                <td>5</td>
                                <td>F</td>
                            </tr>
                            <tr>
                                <td>6</td>
                                <td>G</td>
                            </tr>
                            <tr>
                                <td>7</td>
                                <td>H</td>
                            </tr>
                            <tr>
                                <td>8</td>
                                <td>I</td>
                            </tr>
                            <tr>
                                <td>9</td>
                                <td>J</td>
                            </tr>
                            <tr>
                                <td>10</td>
                                <td>K</td>
                            </tr>
                            <tr>
                                <td>11</td>
                                <td>L</td>
                            </tr>
                            <tr>
                                <td>12</td>
                                <td>M</td>
                            </tr>
                            <tr>
                                <td>13</td>
                                <td>N</td>
                            </tr>
                            <tr>
                                <td>14</td>
                                <td>O</td>
                            </tr>
                            <tr>
                                <td>15</td>
                                <td>P</td>
                            </tr>
                            <tr>
                                <td>16</td>
                                <td>Q</td>
                            </tr>
                            <tr>
                                <td>17</td>
                                <td>R</td>
                            </tr>
                            <tr>
                                <td>18</td>
                                <td>S</td>
                            </tr>
                            <tr>
                                <td>19</td>
                                <td>T</td>
                            </tr>
                            <tr>
                                <td>20</td>
                                <td>U</td>
                            </tr>
                            <tr>
                                <td>21</td>
                                <td>V</td>
                            </tr>
                            <tr>
                                <td>22</td>
                                <td>W</td>
                            </tr>
                            <tr>
                                <td>23</td>
                                <td>X</td>
                            </tr>
                            <tr>
                                <td>24</td>
                                <td>Y</td>
                            </tr>
                            <tr>
                                <td>25</td>
                                <td>Z</td>
                            </tr>
                            <tr>
                                <td>26</td>
                                <td>a</td>
                            </tr>
                            <tr>
                                <td>27</td>
                                <td>b</td>
                            </tr>
                            <tr>
                                <td>28</td>
                                <td>c</td>
                            </tr>
                            <tr>
                                <td>29</td>
                                <td>d</td>
                            </tr>
                            <tr>
                                <td>30</td>
                                <td>e</td>
                            </tr>
                            <tr>
                                <td>31</td>
                                <td>f</td>
                            </tr>
                            <tr>
                                <td>32</td>
                                <td>g</td>
                            </tr>
                            <tr>
                                <td>33</td>
                                <td>h</td>
                            </tr>
                            <tr>
                                <td>34</td>
                                <td>i</td>
                            </tr>
                            <tr>
                                <td>35</td>
                                <td>j</td>
                            </tr>
                            <tr>
                                <td>36</td>
                                <td>k</td>
                            </tr>
                            <tr>
                                <td>37</td>
                                <td>l</td>
                            </tr>
                            <tr>
                                <td>38</td>
                                <td>m</td>
                            </tr>
                            <tr>
                                <td>39</td>
                                <td>n</td>
                            </tr>
                            <tr>
                                <td>40</td>
                                <td>o</td>
                            </tr>
                            <tr>
                                <td>41</td>
                                <td>p</td>
                            </tr>
                            <tr>
                                <td>42</td>
                                <td>q</td>
                            </tr>
                            <tr>
                                <td>43</td>
                                <td>r</td>
                            </tr>
                            <tr>
                                <td>44</td>
                                <td>s</td>
                            </tr>
                            <tr>
                                <td>45</td>
                                <td>t</td>
                            </tr>
                            <tr>
                                <td>46</td>
                                <td>u</td>
                            </tr>
                            <tr>
                                <td>47</td>
                                <td>v</td>
                            </tr>
                            <tr>
                                <td>48</td>
                                <td>w</td>
                            </tr>
                            <tr>
                                <td>49</td>
                                <td>x</td>
                            </tr>
                            <tr>
                                <td>50</td>
                                <td>y</td>
                            </tr>
                            <tr>
                                <td>51</td>
                                <td>z</td>
                            </tr>
                            <tr>
                                <td>52</td>
                                <td>0</td>
                            </tr>
                            <tr>
                                <td>53</td>
                                <td>1</td>
                            </tr>
                            <tr>
                                <td>54</td>
                                <td>2</td>
                            </tr>
                            <tr>
                                <td>55</td>
                                <td>3</td>
                            </tr>
                            <tr>
                                <td>56</td>
                                <td>4</td>
                            </tr>
                            <tr>
                                <td>57</td>
                                <td>5</td>
                            </tr>
                            <tr>
                                <td>58</td>
                                <td>6</td>
                            </tr>
                            <tr>
                                <td>59</td>
                                <td>7</td>
                            </tr>
                            <tr>
                                <td>60</td>
                                <td>8</td>
                            </tr>
                            <tr>
                                <td>61</td>
                                <td>9</td>
                            </tr>
                            <tr>
                                <td>62</td>
                                <td>+</td>
                            </tr>
                            <tr>
                                <td>63</td>
                                <td>/</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>L'exemple ci-dessous, de RFC 4648, illustre l'encodage <span class="em">Base64</span>.</p>
                    <table class="tableBalises" role="presentation">
                        <tbody>
                            <tr>
                                <td>données d'entrée</td>
                                <td>0x14fb9c03d97e</td>
                            </tr>
                            <tr>
                                <td>8 bits</td>
                                <td>00010100 11111011 10011100 00000011 11011001 01111110</td>
                            </tr>
                            <tr>
                                <td>6 bits</td>
                                <td>000101 001111 101110 011100 000000 111101 100101 111110</td>
                            </tr>
                            <tr>
                                <td>Décimal</td>
                                <td>5 15 46 28 0 61 37 62</td>
                            </tr>
                            <tr>
                                <td>Encodage</td>
                                <td>F P u c A 9 l +</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>Le dernier point à discuter concernant la <span class="em">Base64</span> est ce qui se passe lorsque la longueur de la séquence d'octets à encoder n'est pas un multiple de trois. Dans ce cas, le dernier groupe d'octets peut contenir un ou deux octsts au lieu de trois. La <span class="em">Base64</span> réserve le caractère <span class="html">=</span>comme caractère de remplissage. Ce caractère est utilisé deux fois lorsque le dernier groupe contient deux octets et une fois lorsqu'il contient un octet, comme illustré dans les deux exemples ci-dessous.</p>
                    <table class="tableBalises" role="presentation">
                        <tbody>
                            <tr>
                                <td>Données d'entrée</td>
                                <td>0x14</td>
                            </tr>
                            <tr>
                                <td>8 bits</td>
                                <td>00010100</td>
                            </tr>
                            <tr>
                                <td>6 bits</td>
                                <td>000101 000000</td>
                            </tr>
                            <tr>
                                <td>Décimal</td>
                                <td>5 0</td>
                            </tr>
                            <tr>
                                <td>Encodage</td>
                                <td>F A = =</td>
                            </tr>
                        </tbody>
                    </table>
                    <table class="tableBalises" role="presentation">
                        <tbody>
                            <tr>
                                <td>Données d'entrée</td>
                                <td>0x14b9</td>
                            </tr>
                            <tr>
                                <td>8 bits</td>
                                <td>00010100 11111011</td>
                            </tr>
                            <tr>
                                <td>6 bits</td>
                                <td>000101 001111 101100</td>
                            </tr>
                            <tr>
                                <td>Décimal</td>
                                <td>5 15 44</td>
                            </tr>
                            <tr>
                                <td>Encodage</td>
                                <td>F P s =</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>Maintenant que nous avons expliqué le format des messages électroniques, nous pouvons discuter de la manière dont ces messages peuvent être échangés via Internet. La figure ci-dessous illustre les protocoles utilisés lorsque Alice envoie un message électronique à Bob. Alice prépare son e-mail avec un client de messageire électronique ou sur une interface swebmail. Pour envoyer son e-mail à Bob, le client d'Alice utilisera le <span class="html">Simple Mail Transfer Protocol (SMTP)</span> pour livrer son message à son serveur SMTP. Le client de messagerie électronique d'Alice est configuré avec le nom du serveur SMTP par défaut pour son domaine. Il y a généralemnt au moins un serveur SMTP par domaine. Pour livrer le message, le serveur SMTP d'Alice doit trouver le serveur SMTP qui contient la boîte aux lettres de Bob. Cela peut être fait en utilisant les enregistrements <span class="html">Mail eXchange (MX)</span> du DNS. Un ensemble d'enregistrements MX peut être associé à chaque domaine. Chaque enregistrement MX contient une préférence numérique et le nom de domaine pleinement qualifié d'un serveur SMTP qui est capable de livrer des messages électroniques destinés à toutes les adresses électroniques valides de ce domaine. Le DNS peut renvoyer plusieurs enregistrements MX pour un domaine donné. Dans ce cas, le serveur avec la plus faible préférence est utilisé en premier. Si ce serveur n'est pas joignable, le deuxième serveur le plus préféré est utilisé, etc. Le serveur SMTP de Bob stockera le message envoyé par Alice jusqu'à ce que Bob le récupère en utilisant une interface webmail ou des protocoles tels que le <span class="html">Post Office Protocol (POP)</span> ou le <span class="html">Intenet Message Access Protocol (IMAP)</span>.</p>
                    <figure>
                        <img src="../images/protocoles_livraison_email.png" alt="">
                        <figcaption>Figure 3.14 : protocoles de livraison d'e-mail</figcaption>
                    </figure>
                    <h4>Le SMTP (Simple Mail Transfer Protocol) :</h4>
                    <p><span class="html">Simple Mail Transfer Protocol (SMTP)</span> défini dans la RFC 5321 est un protocole client-serveur. La spécification SMTP distingue cinq types de processus impliqués dans la livraison des messages électroniques. Les messages électroniques sont composés sur un <span class="html">agent utilisateur de messagerie (MUA pour Mail User Agent)</span>. Le MUA est généralement soit un client de messagerie électronique, soit une interface webmail. Le MUA envoie le message électronique à un <span class="html">agent de soumission de messagerie (MSA pour Mail Submission Agent)</span>. Le MSA traite le courrier électronique reçu et le transmet à l'<span class="html">agent de transmission de messagerie (MTA pour Mail transmission Agent)</span>. Le MTA est responsable de la transmission du courrier électronique, directement ou via des MTA intermédiaires, vers le MTA du domaine de destination. Ce MTA de destination transmet ensuite le message à l'<span class="html">agent de distribution de messagerie (MDA pour Mail Delivery Agent)</span> où il sera accessible par le MUA du destinataire. SMTP est utilisé pour les interactions entre MUA et MSA, MSA-MTA et MTA-MTA. Au cours des dernières années, de nombreux fournisseurs de servoces Internet, réseaux universitaires et d'entreprise ont déployé des extensions SMTP (RFC 4954) sur leurs MSA. Ces extensions obligent les MUA à s'authentifier avant que le MSA n'accepte un message électronique en provenance du MUA.</p>
                    <p>SMTP est un protocole basé sur le texte, comme de nombreux autres protocoles de couche d'application sur Internet. Il repose sur le service de flux de bytes. Les serveurs écoutent sur le port 25. Les clients envoient des commandes qui sont chacune composées d'une ligne de texte ASCII terminée par CR+LF. Les serveurs répondent en envoyant des lignes ASCII qui contiennent un code numérique à trois chiffres d'erreur/réussite et des commentaires facultatifs.</p>
                    <p>Le protocole SMTP, comme la plupart des protocoles basés sur le texte, est spécifié en <span class="em">BNF</span>. La BNF complète est définie dans la RFC 5321. Les principales commandes SMTP sont définies par les règles BNF illystrées dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/specification_BNF_commandes_SMTP.png" alt="">
                        <figcaption>Figure 3.15 : spécification BNF des commandes SMTP</figcaption>
                    </figure>
                    <p>Dans cette BNF, <span class="html">atext</span> correspond aux caractères ASCII imprimables. Cette règle BNF est définie dans le RFC 5322. Les cinq commandes principales sont <span class="html">EHLO</span>, <span class="html">MAIL FROM:</span>, <span class="html">RCPT TO:</span>, <span class="html">DATA</span> et <span class="html">QUIT</span>. Les premières versions de SMTP utilisaient <span class="html">HELO</span> comme première commande envoyée par un client à un serveur SMTP. Lorsque SMTP a été étendu pour prendre en charge des fonctionnalités plus récentes telles que les caractères 8 bits, il était nécessaire de permettre à un serveur de reconnapitre s'il interagissait avec un clinet qui prenait en charge les extensions ou non. <span class="html">EHLO</span> est devenu obligatoire avec la publication du RFC 2821. <span class="html">Postmaster</span> est l'alias de l'administrateur système qui est responsable d'un domaine ou d'un serveur SMTP donné. Tous les domaines doivent avoir un alias <span class="html">Postmaster</span>.</p>
                    <p>Les réponses SMTP sont définies par la BNF illustrée dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/specification_BNF_reponses_SMTP.png" alt="">
                        <figcaption>Figure 3.16 : spécification BNF des réponses SMTP</figcaption>
                    </figure>
                    <p>Les serveurs SMTP utilisent des codes de réponse structurés contenant trois chiffres et un commentaire facultatif. Le premier chiffre du code de réponse indique si la commande a réussi ou non. Un code de réponse de <span class="html">2xy</span> indique que la commande a été acceptée. Un code de réponse de <span class="html">3xy</span> indique que la commande a été acceptée, mais des informations supplémentaires provenant du client sont attendues. Un code de réponse de <span class="html">4xy</span> indique une réponse négative temporaire. Cela signifie que, pour une raison quelconque, indiquée soit par les autres chiffres ou le commentaire, la commande ne pdut pas être traitée immédiatement, mais qu'il y a de l'espoir que le problème ne soit que temporaire. Cela dit essentiellement au client de réessayer la même commande plus tard. En revanche, un code de réponse de <span class="html">5xy</span> indique une erreur ou une défaillance permanente. Dans ce cas, il est inutile pour le client de réessayer plus tard. D'autres protocoles de couche application tels que FTP RFC 959 ou HTTP RFC 2616 utilisent une structure similaire pour leurs codes de réponse. Des détails supplémentaires sur les autres codes de réponse peuvent être trouvés dans RFC 5321.</p>
                    <p>Des exemples de code de réponse SMTP incluent les suivants :</p>
                    <figure>
                        <img src="../images/exemples_codes_reponse_SMTP.PNG" alt="">
                    </figure>
                    <p>Les quatre premiers codes de réponse correspondent à des erreyrs dans les commandes envoyées par le client. Le quatrième code de réponse serait envoyé par le serveur lorsque le client envoie des commandes dans un ordre incorrect (par exemple, le client essaie d'envoyer un e-mail avant de fournir l'adresse de destination du message). Le code de réponse <span class="html">220</span> est utilisé par le serveur comme premier message lorsqu'il accepte d'interagir avec le client. Le code de réponse <span class="html">221</span> est envoyé par le serveur avant de fermer la connexion de transport sous-jacente. Le code de réponse <span class="html">421</span> est renvoyé lorsqu'il y a un problème (par exemple, le manque de ressources mémoire/disque) qui empêche le serveur d'accepter la connexion de transport. Le code de réponse <span class="html">250</span> est la réponse positive standard qui indique le succès de la commande précédente. Les codes de réponses <span class="html">450</span> et<span class="html">452</span> indiquent que la boîte aux lettres de destination est temporairement indisponible, pour diverses raisons, tandis que le code de réponse <span class="html">550</span> indique que la boîte aux lettres n'existe pas ou ne peut pas être utilisée pour des raisons de politique. Le code de réponse <span class="html">354</span> indique que le client peut commencer à transmettre son message électronique.</p>
                    <p>Ke transfert d'un message électronique s'effectue en trois phases. Au cours de la première phase, le client ouvre une connexion de transport avec le serveur. Une fois la connexion établie, le client et le serveyr échangent des messages de salutattion (commande <span class="html">EHLO</span>). La plupart des serveurs insistent pour recevoir des messages de salutation valides et certains d'entre eux abandonnent la connexion de transport sous-jacente s'ils ne reçoivent pas de message de salutattion valide. Une fois les salutations échangées, la phase de transfert d'e-mail peut commencer. Au cours de cette phase, le client transfère un ou plusieurs messages électroniques en indiquant l'adresse électronique de l'expéditeur (commande <span class="html">MAIL FROM:</span>), l'adresse électronique du destinataire (commande <span class="html">RCPT TO:</span>) suivies des en-têtes et du corps du message électronique (commande <span class="html">DATA</span>). Une fois que le client a fini d'envoyer tous ses messages électroniques en attente au serveur SMTP, il met fin à l'association SMTP (commande <span class="html">QUIT</span>).</p>
                    <p>Un transfert réussi d'un message électronique est illustré ci-dessous :</p>
                    <figure>
                        <img src="../images/transfert_reussi_message_email.PNG" alt="">
                    </figure>
                    <p>Dans l'exemple ci-dessus, le MTA en cours d'exécution sur <span class="em">mta.example.org</span> ouvre une connexion TCP vers le serveur SMTP sur l'hôte <span class="em">smtp.example.com</span>. Les lignes préfixées par <span class="em">S:</span> (resp. <span class="em">C:</span>) sont les réponses envoyées par serveur (resp. les commandes envoyées par le client). Le serveur envoie ses salutations dès que la connexion TCP a été établie. Le client envoie ensuite la commande <span class="em">EHLO</span> avec son nom de domaine complet. Le serveur répond avec le code de réponse <span class="html">250</span> et envoie ses salutations. L'association SMTP peut maintenant être utilisée pour échanger un e-mail.</p>
                    <p>Pour envoyer un e-mail, le client doit d'abord fournir l'adresse du destinataire avec <span class="html">RCPT TO:</span>. Ensuite, il utilise <span class="html">MAIL FROM:</span> avec l'adresse de l'expéditeur. Le destinataire et l'expéditeur sont tous deux acceptés par le serveur. Le client peut maintenant émettre la commande <span class="html">DATA</span> pour commencer le transfert du message électronique. Après avoir reçu le code de réponse <span class="html">354</span>, le client envoie les en-têtes et le corps de son message électronique. Le client indique la fin du message en envoyant un eligne contenant uniquement le caractère <span class="html">. (point ou dot en anglais)</span>. Cela implique qu'un message électronique valide ne peut pas contenir une ligne avec un seul point suivi de CR et LF. Si un utilisateur tape une telle ligne dans un e-mail, son client de messagerie ajoute automatiquement un caractère d'espace avant ou après le point lors de l'envoi du message via SMTP. Le serveur confirme que le message électronique a été mis en file d'attente pour la livraison ou la transmission avec un code de réponse de <span class="html">250</span>. Le client émet la commande <span class="html">QUIT</span> pour fermer la session et le serveur confirme avec le code de réponse <span class="html">221</span>, avant de fermer la connexion TCP.</p>
                    <hr>
                    <p>Note : Relais SMTP ouverts et spam :</p>
                    <p>Depuis sa création en 1971, l'e-mail est un outil très utile utilisé par de nombreux utilisateurs pour échanger beaucoup d'informations. Au début, tous les serveurs SMTP étaient ouverts et tout le monde pouvait les utiliser pour faire transiter des e-mails vers leur destination finale. Malheureusement, au fil des ans, certains utilisateurs sans scrupules ont trouvé des moyens d'utiliser l'e-mail à des fins de marketing ou pour envoyer des logiciels malveillants. Le premier abus documenté de l'e-mail à des fins de marketing s'est produit en 1978 lorsqu'un marketeur travaillant pour un vendeur d'ordinateurs a envoyé un e-mail de marketing à de nombreux utilisateurs d'ARPANET. À cette époque, l'ARPANET ne pouvait être utilisé qu'à des fins de recherche et c'était un abus de la politique d'utilisation acceptable. Malheureusement, étant donné le coût extrêmement faible d'envoi d'e-mails, le problème des e-mails non sollicités n'a pas cessé. Les e-mails non sollicités sont maintenant appelés spam et une étude menée par ENISA en 2009 révèle que 95% des e-mails étaient du spam et ce chiffre semble continuer à croître. Cela impose une charge à l'infrastructure de messagerie des fournisseurs d'accès Internet et des grandes entreprises qui ont besoin de traiter de nombreux messages inutiles.</p>
                    <p>Étant donné la quantité de messages de spam, les serveurs SMTP ne sont plus ouverts conformément à la RFC 5068. Plusieurs extensions à SMTP ont été developpées ces dernières années pour faire face à ce problème. Par exemple, le schéma d'authentification SMTP défini dans la RFC 4954 peut être utilisé par un serveur SMTP pour authentifier un client. Plusieurs techniques ont également été proposées pour permettre aux serveurs SMTP d'authentifier les messages envoyés par leurs utilisateurs (RFC 4870, RFC 4871).</p>
                    <hr>
                    <h4>Le POP (Post Office Protocol) :</h4>
                    <p>Lorsque les premières versions de SMTP ont été conçues, Internet était composé de mini-ordinateurs qui étaient utilisés par un département universitaire ou un laboratoire de recherche entier. Ces mini-ordinateurs étaient utilisés par de nombreux utilisateurs en même temps. Le courrier électronique était principalement utilisé pour envoyer des messages d'un utilisateur sur un hôte donné à un autre utilisateur sur un hôte distant. À l'époque, SMTP était le seul protocole impliqué dans la livraison des courriers électroniques, car tous les hôtes connectés au réseau exécutaient un serveur SMTP. Sur ces hôtes, un courrier électronique destiné à des utilisateurs locaux était livré en plaçant le courrier électronique dans un répertoire ou un fihcier spécial appartenant à l'utilisateur. Cependant, l'introduction des ordinateurs personnels dans les années 1980 a changé cet environnement. Au début, les utilisateurs de ces ordinateurs personnels utilisaient des applications telles que Telnet pour ouvrir une session à distance sur le mini-ordinateur local pour lire leur courrier électronique. Cette méthode n'était pas conviviale. Une meilleure solution est apparue avec le développement d'applications de messagerie conviviales sur les ordinateurs personnels. Plusieurs protocoles ont été conçus pour permettre à ces applications clientes de récupérer les messages électroniques destinés à un utilisateur à partir de son serveur. Deux de ces protocoles sont devenus populaires et sont toujours utilisés aujourd'hui. Le <span class="html">Post Office Protocol (POP)</span>, défini dans la RFC 1939, est le plus simple. Il permet à un client de télécharger tous les messages destinés à un utilisateur donné depuis son serveur de messageire électronique. Nous décrivons brièvement POP dans cette section. Le deuxième protocole est le <span class="html">protocole d'accès aux message Internet (IMAP pour iNternet Message Access Protocol)</span>, defini dans la RFC 3501. IMAP est plus puissant, mais aussi plus complexe que POP. IMAP a été conçu pour permettre aux applications clientes d'accéder efficacement et en temps réel aux messages stockés dans divers dossiers sur les serveurs. IMAP suppose que tous les messages d'un utilisateur donné sont stockés sur un serveur et fournit les fonctions nécessaires pour rechercher, télécharger, supprimer ou filtrer des messages.</p>
                    <p>POP est un autre exemple de protocole simple basé sur des lignes. POP fonctionne au-dessus du service de flux de bytes. Un serveur POP écoute généralment sur le port <span class="html">110</span>. Une session POP est comosée de trois parties : une phase d'<span class="html">autorisation</span> au cours de laquelle le serveur vérifie les informations d'identification du client, une phase de <span class="html">transaction</span> au cours de laquelle le client télécharge des messages et une phase de <span class="html">mise à jour</span> qui conclut la session. Le client envoie des commandes et les réponses du serveur sont préfixées par <span class="html">+OK</span> pour indiquer une commande réussie ou par <span class="html">-ERR</span> pour indiquer des erreurs.</p>
                    <p>Lorsqu'un client ouvre une connexion de transport avec le serveur POP, ce dernier envoie une bannière sous forme d'une ligne ASCII commençant par <span class="html">+OK</span>. La session POP est alors en phase d'autorisation. Pendant cette phase, le client peut envoyer son npm d'utilisateur (resp. mot de passe) avec la commande <span class="html">USER</span> (resp. <span class="html">PASS</span>). Le  serveur répond par <span class="html">+OK</span> si le nom d'utilisateur (resp. mot de passe) est valide et par <span class="html">-ERR</span>sinon.</p>
                    <p>Une fois que le nom d'utilisateur et le mot de passe ont été validés, la session POP entre dans la phase de transaction. Dans cette phase, le client peut envoyer plusieurs commandes. La commande <span class="html">STAT</span> est utilisée pour récupérer l'état du serveur. À la réception de cette commande, le serveur répond avec une ligne qui conteint <span class="html">+OK</span> suivi du nombre de messahes dans la boîte aux lettres et de la taille totale de la boîte aux lettres en octets. La commande <span class="html">RETR</span>, suivi d'un espace et d'un entier, est utilisée pour récupérer le nième message de la boîte aux lettres. La commande <span class="html">DELE</span> est utilisée pour marquer le nième message de la boîte aux lettres comme étant supprimé.</p>
                    <p>Une fois que le client a récupéré et éventuellement supprimé les e-mails contenus dans la boîte aux lettres, il doit envoyer la commande <span class="html">QUIT</span>. Cette commande met fin à la session POP et permet au serveur de supprimer tous les messages qui ont été marqués pour suppression en utilisant la commande <span class="html">DELE</span>.</p>
                    <p>La figure ci-dessous montre une session POP simple. Toutes les lignes préfixées par <span class="em">C:</span> (resp. <span class="em">S:</span>) sont envoyées par le client (resp. le serveur).</p>
                    <figure>
                        <img src="../images/session_simple_POP.PNG" alt="">
                    </figure>
                    <p>Dans cet exemple, un client POP contacte un serveur POP au nom de l'utilisateur nommé Alice. Notez que dans cet exemple, le mot de passe d'Alice est envoyé en clair par le client. Cela implique que si quelqu'un est capable de capturer les paquets envoyés par Alice, il connaîtra le mot de passe d'Alice. La RFC 1939 définit le schéma d'authentification <span class="html">APOP</span> qui n'est pas vulnérable à de telles attaques. Ensuite, le client d'Alice envoie la commande <span class="html">STAT</span> pour connaître le nombre de messages qui sont stockés dans sa boîte aux lettres. Elle récupère ensuite et supprime le premier message de la boîte aux lettres.</p>
                    <h4>3.4.3. Le HTTP (HyperText Transfer Protocol) :</h4>
                    <p>Au début de l'Internet, il était principalement utilisé pour l'accès à distance aux terminaux avec <span class="html">Telnet</span>, l'e-mail et le tranfert de fichiers.</p>
                    <p>Le protocole de transfert de fichiers par défaut, <span class="html">FTP</span>, défini dans la RFC 959, était largement utilisé et les clients et serveurs <span class="html">FTP</span> sont enocre inclus dans la plupart des systèmes d'exploitation.</p>
                    <p>De nombreux clients FTP offrent une interface utilisateur similaire à une shell Unix et permettent au client de naviguer dans le système de fichiers sur le serveur et d'envoyer et de récupérer des fichiers. Les serveurs FTP peuvent être configurés en deux modes :</p>
                    <ul>
                        <li>
                            <p>authentifié : Dans ce mode, le serveur FTP n'accepte que les utilisateurs avec un nom d'utilisateur et un mot de passe valides. Une fois authentifiés, ils peuvent accéder aux fichiers et répertoires en fonction de leurs permissions.</p>
                        </li>
                        <li>
                            <p>anonyme : Dans ce mode, les clients fournissent l'identifiant utilisateur anonyme et leur adresse e-mail comme mot de passe. Ces clients ont accès à une zone spéciale du système de fichiers qui ne contient que des fichiers publics.</p>
                        </li> 
                    </ul>be central
                    <p>FTP était très populaire dans les années 1990 et au début des années 2000, mais aujourd'hui, il a été largement remplacé par des protocoles plus récents. L'accès authentifié aux fichiers se fait principalement en utilisant le <span class="html">protocole Secure Shell (SSH)</span> défini dans la RFC 4251 et pris en charge par des clients tels que <span class="html">SCP</span> ou <span class="html">SFTP</span>. De nos jours, l'accès anonyme est principalement fourni par des protocoles web.</p>
                    <p>Dans les années 1980, les physiciens de haute énergie travaillant au CERN devaient échanger efficacement des documents sur leurs expériences en cours et prévues. Tim Berners-Lee a évalué plusieurs techniques de partage de documents qui étaient disponibles à cette époque [B1989]. Comme aucune des solutions existantes ne répondait aux besoins du CERN, ils ont choisi de développer un tout nouveau système de partage de documents. Ce système s'appelait initialement le <span class="html">mesh</span>, mais a rapidement été renommé le <span class="html">World Wide Web</span>. Le point de départ du <span class="em">World Wide Web</span> sont les documents hypertexte. Un document hypertexte est un document qui contient des références (hyperliens) vers d'autres documents auxquels le lecteur peut accéder immédiatement. L'hypertexte n'a pas été inventé pour le <span class="em">World Wide Web</span>. L'idée de documents hypertexte a été proposée en 1945 [Bush1945] et les premières expériences ont été menées dans les années 1960 [Nelson1965] [Myers1998]. Par rapport aux documents hypertexte utilisés à la fin des années 1980, la principale innovation introduite par le <span class="em">World Wide Web</span> a été de permettre aux hyperliens de référencer des documents stockés sur des machines distantes.</p>
                    <figure>
                        <img src="../images/clients_serveurs_www.png" alt="">
                        <figcaption>Figure 3.17 : clients et serveurs du World Wide Web</figcaption>
                    </figure>
                    <p>Un système de partage de documents tel que le <span class="em">World Wide Web</span> est composé de trois parties importantes :</p>
                    <ol>
                        <li>
                            <p>Un schéma d'adressage standardisé qui permet l'identification sans ambiguïté des documents.</p>
                        </li>
                        <li>
                            <p>Un format de document standardisé : le <span class="html">langage de balisage hypertexte (HyperText Markup Language)</span>.</p>
                        </li>
                        <li>
                            <p>Un protocole standardisé qui facilite la récupération efficace des documents stockés sur un serveur.</p>
                        </li>
                    </ol>
                    <p>Les standards ouverts ont joué et contineuent de jouer un rôle clé dans le succès du <span class="em">World Wide Web</span> tels que nous le connaissons aujourd'hui. Sans les standards ouverts, le World Wide Web n'aurait jamais atteint sa taille actuelle. En plus des standards ouverts, un autre facteur important pour le succès du web était la disponibilité d'implémentations ouvertes et efficaces de ces standards. Lorsque le CERN a commencé à travailler sur le web, leur objectif était de construire un système fonctionnel qui pourrait être utilisé par les physiciens. Ils ont développé des implémentations open-source des premiers serveurs web et des clients web. Ces implémentations open-source étaient puissantes et pouvaient être utilisées telles quelles par des institutions souhaitant partager des informations sur le web. Elles ont également été étendues par d'autres développeurs qui ont contribué à de nouvelles fonctionnalités. Par exemple, NCSA a ajouté la prise en charge des images dans leur navigateur Mosaic qui a finalement été utilisé pour créer Netscape Communications.</p>
                    <p>Les premiers composants du World Wide Web sont les <span class="html">URI (Uniform Resource Identifiers)</span>, définis dans la RFC 3986. Un URI est une chaîne de caractères qui identifie de manière non ambiguë une ressource sur le World Wide Web. Voici un sous-ensemble de la BNF pour les URI.</p>
                    <figure>
                        <img src="../images/BNF_URI.PNG" alt="">
                    </figure>
                    <p>Le premier composant d'un URI est son <span class="html">schéma</span>. Un <span class="em">schéma</span> peut être considéré comme un sélecteur, indiquant la signification des champs qui le suivent. En pratique, le schéma identifie souvent le protocole de couche application qui doit être utilisé par le client pour récupérer le document, mais ce n'est pas toujours le cas. Certains schémas n'impliquent aucun protocole et certains ne permettent pas de récupérer un document. Un exemple d'URO non récupérable est <span class="em">urn:isbn-380-81593-1</span>, qui est un identificateur unique pour un livre, via le schéma <span class="em">urn</span> (voir RFC 3187).Bien sût, n'importe quelle URI peut être rendue récupérable via un serveur dédié ou un nouveau protocole, mais celui-ci n'a pas de protocole explicite. La même chose s'applique à l'étiquette du schéma (voir RFC 4151), souvent utilisée dans la syndication Web (voir RFC 4287 sur le format de syndication Atom). Même lorsque le schéma est récupérable (par exemple avec http), il est souvent utilisé uniquement comme identificateur, pas comme moyen d'obtenir une ressource. Voir <a href="http://norman.walsh.name/2006/07/25/namesAndAddresses" target="_blank">http://norman.walsh.name/2006/07/25/namesAndAddresses</a> pour une bonne explication. Le schéma le plus fréquent est <span class="html">http</span>, qui sera décrit plus tard. <span class="em">Un schéma d'URI peut être féfini pour presque n'importe quel protocole de couche application [#furiist]_.</span>Les caractères '<span class="em">:</span>' et '<span class="em">//</span>' suivent le schéma de n'importe quel URI.</p>
                    <p>La deuxième partie de l'URI est l'<span class="html">autorité</span>. Avec une URI récupérable, cela comprend le nom DNS ou l'adresse IP du serveur où le document peut être récupéré en utilisant le protocole spécifié via le schéma. Ce nom peut être précédé de certaines informations sur l'utilisateur (par exemple, un nom d'utilisateur) qui demande les informations. Les définitions antérieures de l'URI permettaient la spécification d'un nom d'utilisateur et d'un mot de passe avant le caractère @ (RFC 1738), mais cela est maintenant déconseillé car placer le mot de passe à l'intérieur d'une URI est insécurisé. Le nom d'hôte peut être suivi du caractère deux-points et d'un numéro de port. Un numéro de port par défaut est défini pour certains protocoles et le numéro de port doit être inclus dans l'URI que si un numéro de port non par défaut est utilisé (pour d'autres protocoles, des techniques comme des enregistrements de service DNS sont utilisées).</p>
                    <p>La troisième partie de l'URI est le chemin d'accès au document. Ce chemin est structuré comme des noms de fichiers sur un hôte Unix (mais cela n'implique pas que les fichiers sont effectivement stockés de cette manière sur le serveur). Si le chemin d'accès n'est pas spécifié, le serveur renverra un document par défaut. Les deux dernières parties facultatives de l'URI sont utilisées pour forunir une requête et indiquer une partie spécifique (par exemple, une section dans un article) du document demandé. Des exemples d'UURI sont présentés ci-dessous.</p>
                    <figure>
                        <img src="../images/exemples_uris.PNG" alt="">
                    </figure>
                    <p>Le premier URI correspond à un document nommé <span class="em">rfc3986.html</span> stocké sur le serveur nommé <span class="em">tools.ietf.org</span> et peut être accédé en utilisant le protocole <span class="em">HTTP</span> sur son port par défaut. Le deuxième URI correspond à un message électronique, avec pour objet "<span class="em">current-issue</span>", qui sera envoyé à l'utilisateur <span class="em">infobot</span> dans le domaine <span class="em">example.com</span>. Le schéma URI <span class="em">mailto:</span> est défini dans la RFC 6068. Le troisième URI fait référence à la partie <span class="em">BasHTTPServer.BaseHTTPRequestHandler</span> du document <span class="em">basehttpserver.html</span> qui est stocké dans le répertoire de la bibliothèque sur le serveur <span class="em">docs.python.org</span>. Ce document peut être récupéré en utilisant le protocole <span class="em">HTTP</span>. La requête <span class="em">highlight=http</span> est associée à cet URI. Le quatrième exemple est un serveur qui utilise le protocole <span class="em">Telnet</span>, utilise l'adresse IPv6 <span class="em">2001:6&8:3080:3::2</span> et est accessible sur le port <span class="em">80</span>. Le dernier URI est quelque peu spécial. La plupart des utilisateurs supposeront aqu'il correspond à un document stocké sur le serveur <span class="em">com.example.com</span>. Cependant, pour interpréter cet URI, il est important de se rappeler que le caractère @ est utilisé pour le nom d'utilisateur du nom d'hôte dans la partie d'autorisation d'un URI. Cela implique que l'URI pointe vers un document nommé <span class="Em">top_story.htm</span> sur l'hôte ayant l'adresse IPv4 <span class="em">10.0.0.1</span>. Le document sera récupéré en utilisant le protocole <span class="em">FTP</span> avec le nom d'utilisateur défini sur <span class="em">cnn.example.com&story=breaking_news</span>.</p>
                    <p>La deuxième composante du <span class="em">World Wide Web</span> est le <span class="em">langage de balisage hypertexte (HTML pour HyperText Markup Language)</span>. HTML définit le format des documents échangés sur le Web. La première version d'HTML a été dérivée du <span class="html">langage de balisage généralisé standardisé (SGML pour Standard Generalized Markup Language)</span> en 1986 par ISO. SGML a été conçu pour permettre aux documents de grands projets dans des industries telles que le gouvernement, le droit ou l'aéropspatiale d'être partagés de manière efficace et lisible par les machines. Ces industries nécessitent que les documents restent lisibles et modifiables pendant des dizaines d'années et ont insisté sur un format standardisé pris en charge par plusieurs fournisseurs. Aujourd'hui, SGML n'est plus largement utilisé au-delà d'applications spécifiques, mais ses descendants, y compris <span class="em">HTML</span> et <span class="em">XML</span>, sont maintenant répandus.</p>
                    <p>Un langage de balisage est une manière structurée d'ajouter des annotations sur la mise en forme du document dans le document lui-même. Des exemples de langages de balisage incluent <span class="em">troff</span>, qui est utilisé pour écrire les pages de manuel Unix ou <span class="html">Latex</span>. HTML utilise des marqueurs pour annoter du texte et un document est composé d'éléments HTML. Chaque élément est généralment de trois élements : une balise de début qui inclut éventuellement des attributs spécifiques, du texte (souvent comprenant d'autres éléments) et une balise de fin. Une balise HTML est un mot-clé inclus entre des chevrons. La forme générique d'un élément HTML :</p>
<pre><code>&lt;tag&gt;Un texte à afficher&lt;/tag&gt;</code></pre>
                    <p>Des éléments HTML plus complexes peuvent également inclure des attributs facultatifs dans la balise de début :</p>
<pre><code>&lt;tag attribut1="valeur1" attribut2="valeur2"&gt;un texte à afficher&lt;/tag&gt;</code></pre>
                    <p>Le document HTML ci-dessous est composé de deux parties : un en-tête, délimité par les marqueurs <span class="html">&lt;head&gt;</span> et <span class="html">&lt;/head&gt;</span>, et un corps (entre les marqueurs <span class="html">&lt;body&gt;</span> et <span class="html">&lt;/body&gt;</span>). Dans l'exemple-cessous, l'en-tête ne contient qu'un titre, mais d'autres types d'informations peuvent être inclus dans l'en-tête. Le corps contient une image, du texte et une liste avec trois liens hypertexte. L'image est incluse dans la page Web en indiquant son URI entre crochets à l'intérieur de la balise <span class="html">&lt;img src="..."&gt;</span>. L'image peut bien sûr résider sur n'importe quel serveur et le client la téléchargera automatiquement lors de l'affichage de la page Web. La balise <span class="html">&lt;h1&gt;...&lt;/h1&gt;</span> est utilisée pour spécifier le premier niveau de titres. La balise <span class="html">&lt;ul&gt;</span> indique une liste non numérotée tandis que la balise <span class="html">&lt;li&gt;</span> indique un élément de liste. La balise <span class="html">&lt;a href="URI"&gt;texte&lt;/a&gt;</span> indique un lien hypertexte. Le texte sera souligné dans la page Web rendue et le client récupérera l'URI spécifié lorsque l'utilisateur cliquera sur le lien.</p>
                    <figure>
                        <img src="../images/page_HTML_simple.jpg" alt="">
                        <figcaption>Figure 3.18 : Une page HTML simple</figcaption>
                    </figure>
                    <p>Des informations supplémentaires sur les diverses extensions à HTML peuvent être trouvées dans les spécifications officielles maintenues par le W3C.</p>
                    <p>La troisième composante du <span class="em">World Wide Web</span> est le <span class="em">protocole de transport hypertexte (HTTP pour HyperText Transport Protocol)</span>. HTTP est un procole basé sur le texte, dans lequel le cient envoie une requête et le serveur renvoie une réponse. HTTP s'exécute au-dessus du service de flux de bits et les serveurs HTTP  écoutent par défaut sur le port <span class="em">80</span>. La conception de HTTP est largement inspirée des protocoles de messagerie Internet. Chaque requête HTTP contient trois parties :</p>
                    <ul>
                        <li>
                            <p>Une <span class="html">méthode</span>, qui indique le type de requête, un URI et la version du protocole HTTP utilisé par le client.</p>
                        </li>
                        <li>
                            <p>Un <span class="em">en-tête</span>, qui est utilisé par le client pour spécifier des paramètres facultatifs pour la requête. Une ligne vide est utilisée pour marquer la fin de l'en-tête.</p>
                        </li>
                        <li>
                            <p>Un document MIME facultatif attaché à la requête.</p>
                        </li>
                    </ul>
                    <p>La réponse envoyée par le serveur contient également trois parties :</p>
                    <ul>
                        <li>
                            <p>Une <span class="html">ligne d'état</span>, qui indique si la requête a été réussie ou non.</p>
                        </li>
                        <li>
                            <p>Un <span class="html">en-tête</span>, qui contient des informations supplémentaires sur la réponse. L'en-tête de réponse se termine par une ligne vide.</p>
                        </li>
                        <li>
                            <p>Un document MIME.</p>
                        </li>
                    </ul>
                    <figure>
                        <img src="../images/requetes_reponses_HTTP.png" alt="">
                        <figcaption>Figure 3.19 : requêtes et réponses HTTP</figcaption>
                    </figure>
                    <p>Plusieurs types de méthodes peuvent être utilisés dans les requêtes HTTP. Les trois plus importants sont :</p>
                    <ul>
                        <li>
                            <p>La méthode <span class="html">GET</span> est la plus populaire. Elle est utilisée pour récupérer un document à partir d'un serveur. La méthode <span class="em">GET</span> est encodée en <span class="html">GET</span> suivie du chemin de l'URI du document demandé et de la version d'HTTP utilisée par le client. Par exemple, pour récupérer l'URI <span class="em">https://www.w3.org/MarkUp/</span>, un client doit ouvrir une connexion TCP sur le port <span class="em">80</span> avec l'hôte <span class="em">www.w3.org</span> et envoyer une requête HTTP contenant la ligne suivante :</p>
<pre><code>GET /MarkUp/HTTP/1.0</code></pre>
                        </li>
                        <li>
                            <p>La méthode <span class="html">HEAD</span> est une variante de la méthode <span class="em">GET</span> qui permet de récupérer les lignes d'en-tête pour une URI donnée sans récupérer l'intégralité du document. Elle peut être utilisée par un client pour vérifier su-i un document existe, par exemple.</p>
                        </li>
                        <li>
                            <p>La méthode <span class="html">POST</span> peut être utilisée par un client pour envoyer un document à un serveur. Le document envoyé est attaché à la requête HTTP en tant que document MIME.</p>
                        </li>
                    </ul>
                    <p>Les clients et serveurs HTTP peuvent inclure de nombreux en-têtes HTTP différents dans les requêtes et réponses HTTP. Chaque en-tête HTTP est encodé en une seule ligne ASCII terminée par CR et LF. Plusieurs de ces en-têtes sont brièvement décrits ci-dessous. Une discussion détaillée de tous les en-têtes standard peut être trouvée dans la RFC 1945. Les en-têtes MIME peuvent apparaître à la fois dans les requêtes HTTP et les réponses HTTP.</p>
                    <ul>
                        <li>
                            <p>L'en-tête <span class="html">Content-Length:</span> est l'en-tête MIME qui indique la longueur du document MIME en octets.</p>
                        </li>
                        <li>
                            <p>L'en-tête <span class="html">Content-Type:</span> est l'en-tête MIME qui indique le type de document MIME attaché. Les pages HTML utilisent le type <span class="html">text/html</span>.</p>
                        </li>
                        <li>
                            <p>L'en-tête <span class="html">Content-Encoding:</span> indique comment le document MIME a été encodé. Par exemple, cette en-tête sera définie sur <span class="html">x-gzip</span> pour un document compressé à l'aide du logiciel <span class="html">gzip</span>.</p>
                        </li>
                    </ul>
                    <p>Les RFC 1945 et RFC 2616 définissent des en-têtes spécifiques aux réponses HTTP. Ces en-têtes de serveur comprennent :</p>
                    <ul>
                        <li>
                            <p>L'en-tête <span class="html">Server:</span> indique la version du serveur Web qui a généré la réponse HTTP. Certains serveurs fournissent des informations sur leur version de logiciel et les modules optionnels qu'ils utilisent. Pour des raisons de sécurité, certains administrateurs système désactivent ces en-têtes pour éviter de révéler trop d'informations sur leur serveur aux éventuels attaquants.</p>
                        </li>
                        <li>
                            <p>L'en-tête <span class="html">Date:</span> indique la date à laquelle la réponse HTTP a été produite par le serveur.</p>
                        </li>
                        <li>
                            <p>L'en-tête <span class="html">Last-Modified:</span>indique la date et l'heure de la dernière modification du document joint à la réponse HTTP.</p>
                        </li>
                    </ul>
                    <p>De même, les lignes d'en-tête suivantes ne peuvent apparaître que dans les demandes HTTP envoyées par un client :</p>
                    <ul>
                        <li>
                            <p>L'en-tête <span class="html">User-Agent:</span> fournit des informations sur le client qui a généré la demande HTTP. Certains serveurs analysent cette ligne d'en-tête et renvoient des en-têtes différents et parfois des documents différents pour différents agents utilisateur.</p>
                        </li>
                        <li>
                            <p>L'en-tête <span class="html">If_Modified-Since:</span> est suivi d'une date. Il permet aux clients de mettre en cache en mémoire ou sur disque les documents récents ou les fréquemment utilisés. Lorqu'un client doit demander une URI à un serveur, il vérifie d'abord si le documentest déjà dans son cache. S'il est présent, le client envoie une demande HTTP avec l'en-tête <span class="em">If-Modified-Since:</span> indiquant la date du document mis en cache. Le serveur ne renverra le document joint à la réponse HTTP que s'il est plus récent que la version stockée dans le cache du client.</p>
                        </li>
                        <li>
                            <p>L'en-tête <span class="html">Referrer:</span> est suivi d'une URI. Il indique l'URI du document que le client a visité avant d'envoyer cette demande HTTP. Grâce à cet en-tête, le serveur peut connaître l'URI du document contenant le lien hypertexte suivi par le client, le cas échéant. Cette information est très utile pour mesurer l'impact des publicités contenant des liens hypertexte placées sur des sites Web.</p>
                        </li>
                        <li>
                            <p>L'en-tête <span class="html">Host:</span> contient le nom de domaine pleinement qualifié de l'URI demandée.</p>
                        </li>
                    </ul>
                    <hr>
                    <p>Note : L'importance du ligne d'en-tête <span class="em">Host:</span> :</p>
                    <p>La première verssion de HTTP ne comprenait pas la ligne d'en-tête <span class="em">Host:</span>. C'était une grave limitation pour les entreprises d'hébergement Web. Par exemple, considérons une entreprise d'hébergement Web qui souhaite server à la fois <span class="em">web.example.com</span> et <span class="em">www.example.net</span> sur le même serveur physique. Les deux sites Web contiennent un document <span class="em">/index.html</span>. Lorsqu'un client envoie une demande pour <span class="em">http://web.example.com/index.html</span> ou <span class="em">http://www.example.net/index.html</span>, la demande HTTP 1.0 contient la ligne suivante :</p>
<pre><code>GET /index.html HTTP 1.0</code></pre>
                    <p>En analysant cette ligne, un serveur ne peut pas déterminer quel fichier <span class="em">index.html</span> est demandé. Grâce à la ligne d'en-tête <span class="em">Host:</span>, le serveur sait si la demande est pour <span class="em">http://web.example.com/index.html</span> ou <span class="em">http://www.dummy.net/index.html</span>. Sans la ligne d'en-tête <span class="em">Host:</span> a permis aux entreprises d'hébergement Web de développer leur activité en prenant en charge un grand nombre de serveurs Web indépendants sur le même serveur physique.</p>
                    <hr>
                    <p>La ligne d'état de la réponse HTTP commence par la version d'HTTP utilisée par le serveur (généralement <span class="html">HTTP/1.0</span> défini dans la RFC 1945 ou <span class="html">HTTP/1.1</span> défini dans la RFC 2616), suivie d'un code d'état à trois chiffres et d'informations supplémentaires en anglais. Les codes d'état HTTP ont une structure similaire aux codes de réponse utilisés par SMTP.</p>
                    <ul>
                        <li>
                            <p>Tous les codes d'état commençant par le chiffre <span class="em">2</span>indiquent une réponse valide. <span class="html">200 Ok</span> indique que la demande HTTP  a été traitée avec succès par le serveur et que la réponse est valide.</p>
                        </li>
                        <li>
                            <p>Tous les codes d'état commençant par le chiffre <span class="em">3</span> indiquent que le document demandé n'est plus disponible sur le serveur. <span class="html">301 Moved Permanently</span> indique que le document demandé n'est plus disponible sur ce serveur. Une ligne d'en-tête <span class="html">Location:</span> contenant le nouvel URI du document demandé est insérée  dans la réponse HTTP. <span class="html">304 Not-Modified</span> est utilisé en réponse à une demande HTTP contenant la ligne d'en-tête <span class="em">If-Modified-Since:</span>. Cette ligne d'état utilisée par le serveur si le documenr stocké sur le serveur n'est pas plus récent que la date indiquée dans la ligne d'en-tête <span class="em">If-Modified-Since:</span>.</p>
                        </li>
                        <li>
                            <p>Tous les codes de statut commençant par le chiffre <span class="em">4</span> indiquent que le serveur a détecté une erreur dans la requête HTTP envoyée par le client. <span class="html">400 Bad Request</span> indique une erreur de syntaxe dans la requête HTTP. <span class="html">404 Not Found</span> indique que le document demandé n'existe pas sur le serveur.</p>
                        </li>
                        <li>
                            <p>Tous les codes de statut commençant par le chiffre <span class="em">5</span> indiquent une erreur sur le serveur. <span class="html">500 Internal Server Error</span> indique que le serveur n'a pas pu traiter la demande en raison d'une erreur sur le serveur lui-même.</p>
                        </li>
                    </ul>
                    <p>Dans la requête HTTP et la réponse HTTP, le document MIME se réfère à une représentation du document avec les en-têtes MIME indiquant le type de document et sa taille.</p>
                    <p>À titre d'illustration de HTTP/1.0, la transcription ci-dessous montre une requête pour <span class="em">http://www.ietf.org/</span> et la réponse HTTP correspondante. La requête HTTP a été envoyée à l'aide de l'outil en ligne de commande <span class="html">curl</span>. La ligne d'en-tête <span class="html">User-Agent:</span> contient plus d'informations sur ce logiciel client. Il n'y a pas de document MIME attaché à cette requête HTTP, et elle se termine par une ligne vide.</p>
<pre><code>GET / HTTP/1.0
User-Agent: curl/7.19.4 (universal-apple-darwin10.0) libcurl/7.19.4 OpenSSL/0.9.8l zlib/1.2.3
Host: www.ietf.org</code></pre>
                    <p>La réponse HTTP indique la version du logiciel serveur utilisé ainsi que les modules inclus. L'en-tête <span class="html">Last-Modified:</span> indique que le document demandé a été modifié environ une semaine avant la requête. Un document HTML (non affiché) est attaché à la réponse. Notez la ligne vide entre l'en-tête de la réponse HTTP et le document MIME joint. La ligne d'en-tête<span class="html">Server:</span> a été tronquée dans cette sortie.</p>
<pre><code>HTTP/1.1 200 OK
Date: Mon, 15 Mar 2010 13:40:38 GMT
Server: Apache/2.2.4 (Linux/SUSE) mod_ssl/2.2.4 OpenSSL/0.9.8e (truncated) Last-Modified: Tue, 09 Mar 2010 21:26:53 GMT
Content-Length: 17019 Content-Type: text/html

&lt;!DOCTYPE HTML PUBLIC .../HTML&gt;</code></pre>
                    <p>HTTP a été initialement conçu pour partager des documents texte autonomes. Pour cette raison, et pour faciliter l'implémentation des clients et des serveurs, les concepteurs de HTTP ont choisi d'ouvrir une connexion TCP pour chaque requête HTTP. Cela implique qu'un client doit ouvrir une connexion TCP pour chaque URI qu'il veut récupérer à partir d'un serveur, comme illustré sur la figure ci-dessous. Pour une page Web ne contenant que des documents texte, c'était un choix de conception raisonnable car le client reste généralment inactif pendant que l'utilisateur (humain) lit le document récupéré.</p>
                    <figure>
                        <img src="../images/HTTP_1.0_connexion_TCP.jpg" alt="">
                        <figcaption>Figure 3.20 : HTTP 1.0 et la connexion TCP sous-jacente</figcaption>
                    </figure>
                    <p>Cependant, à mesure que le web évoluait pour prendre en charge des documents plus riches contenant des images, l'ouverture d'une connexion pour chaque URO estt devenue un problème de performance [Mogul1995]. En effet, en plus de sa partie HTML, une page web peut inclure des dizaines d'images ou plus. Forcer le client à ouvrir une connexion TCP pour chaque composant d'une page web présente deux inconvénients importants. Tout d'abord, le client et le serveur doivent échanger des paquets pour ouvrir et fermer une connexion TCP, comme nous le verrons plus tard. Cela augmente les surcharges du réseau et le temps total nécessaire pour récupérer complètement tous les composants d'une page web. Deuxièmement, un grand nombre de connexions TCP établies peut être un <span class="em">goulot d'étranglement (bottleneck en anglais)</span> de performance pour les serveurs.</p>
                    <p>Ce problème a été résolu en étendant HTTP pour prendre en charge des connexions TCP persistantes RFC 2616. Une connexion persistante est une connexion TCP sur laquelle un client peut envoyer plusieurs requêtes HTTP. Cela est illustré dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/HTTP_1.1_connexions_persistantes.jpg.png" alt="">
                        <figcaption>Figure 3.21 : HTTP 1.1 connexions persistantes</figcaption>
                    </figure>
                    <p>Pour permettre aux clients et aux serveurs de contrôler l'utilisation de ces connexions TCP persistantes, HTTP 1.1 RFC 2616 définit plusieurs nouveaux en-tête HTTP :</p>
                    <ul>
                        <li>
                            <p>L'en-tête <span class="html">Connection:</span> est utilisé avec l'argument <span class="html">Keep-Alive</span> pour le client pour indiquer qu'il s'attend à ce que la connexion TCP sous-jacente soit persistante. Lorsque cet en-tête  est utilisé avec l'argument <span class="html">Close</span>, il indique que l'entité qui l'a envoyé fermera la connexion TCP sous-jacente à la fin de la réponse HTTP.</p>
                        </li>
                        <li>
                            <p>L'en-tête <span class="html">Keep-Alive:</span> est utilisé par le serveur pour informer le client de la manière dont il accepte d'utiliser la connexion persistante. Un <span class="em">Keep-Alive</span> typique contient deux paramètres : le nombre maximal de requêtes que le serveur accepte de traiter sur la connexion TCP sous-jacente et le délai d'attente (en secondes) après lequel le serveur fermera une connexion inactive.</p>
                        </li>
                    </ul>
                    <p>L'exemple ci-dessous montre le fonctionnement de HTTP/1.1 sur une connexion TCP persistante pour récupérer trois URI stockées sur le même serveur. Une fois que la connexion a été établie, le client envoie sa première requête avec l'en-tête <span class="html">Connection:keep-alive</span> pour demander une connexion persistante.</p>
<pre><code>GET / HTTP/1.1
Host: www.kame.net
User-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_2; en-us)
Connection: keep-alive</code></pre>
                    <p>Le serveur répond avec l'en-tête <span class="html">Connection: Keep-Alive</span> et indique qu'il accepte un maximum de 100 requêtes HTTP sur cette connexion et qu'il fermera la connexion s'il reste inactif pendant 15 secondes.</p>
<pre><code>HTTP/1.1 200 OK
Date: Fri, 19 Mar 2010 09:23:37 GMT
Server: Apache/2.0.63 (FreeBSD) PHP/5.2.12 with Suhosin-Patch Keep-Alive: timeout=15, max=100
Connection: Keep-Alive Content-Length: 3462 Content-Type: text/html
&lt;html&gt;... &lt;/html&gt;</code></pre>
                    <p>Le client envoie une deuxième demande pour la feuille de style de la page web récupérée.</p>
<pre><code>GET /style.css HTTP/1.1 Host: www.kame.net
Referer: http://www.kame.net/
User-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_2; en-us)
Connection: keep-alive</code></pre>
                    <p>Le serveur répond avec la feuille de style demandée et maintient la connexion persistante. Notez que le serveur n'accepte plus que 99 requêtes HTTP sur cette connexion persistante.</p>
<pre class="talle60"><code>HTTP/1.1 200 OK
Date: Fri, 19 Mar 2010 09:23:37 GMT
Server: Apache/2.0.63 (FreeBSD) PHP/5.2.12 with Suhosin-Patch Last-Modified: Mon, 10 Apr 2006 05:06:39 GMT
Content-Length: 2235
Keep-Alive: timeout=15, max=99 Connection: Keep-Alive
Content-Type: text/css
...</code></pre>
                    <p>Ensuite, le client demande automatiquement l'icône du serveur web, qui pourrait être affichée par le navigateur. Ce serveur ne contient pas cette URI et répond donc avec un statut HTTP 404. Cependant, la connexion TCP sous-jacente n'est pas fermée immédiatement.</p>
                    <p>Les icônes favorites sont de petites icônes utilisées pour représenter les serveurs web dans la barre d'outils des navigateurs Internet. Microsoft a ajouté cette fonctionnalité dans leurs navigateurs sans tenir compte des normes W3C. Voir <a href="http://www.w3.org/2005/10/howto-favicon" target="_blank">http://www.w3.org/2005/10/howto-favicon</a> pour une discussion sur la façon de prendre en charge proprement de telles icônes favorites.</p>
<pre><code>GET /favicon.ico HTTP/1.1 Host: www.kame.net
Referer: http://www.kame.net/
User-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_2; en-us) Connection: keep-alive
HTTP/1.1 404 Not Found
Date: Fri, 19 Mar 2010 09:23:40 GMT
Server: Apache/2.0.63 (FreeBSD) PHP/5.2.12 with Suhosin-Patch Content-Length: 318
Keep-Alive: timeout=15, max=98 Connection: Keep-Alive
Content-Type: text/html; charset=iso-8859-1

&lt;!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN"&gt; ...</code></pre>
                    <p>Comme illustré ci-dessus, un client peut envoyer plusieurs requêtes HTTP sur la même connexion persistante. Toutefois, il est important de noter que toutes ces requêtes sont considérées comme indépendantes par le serveur. Chaque requête HTTP doit être autonome. Cela implique que chaque requête doit inclure toutes les lignes d'en-tête nécessaires au serveur pour comprendre la requête. L'indépendance de ces requêtes est l'un des choix de conception importants d'HTTP. En conséquence de ce choix de conception, lorsqu'un serveur traite une requête HTTP, il n'utilise aucun autre information que celle contenue dans la requête elle-même. Cela implique pourquoi le client ajoute sa ligne <span class="em">User-Agent:</span> dans toutes les requêtes HTTP qu'il envoie sur la connexion TCP persistante.</p>
                    <p>Cependant, en pratique, certains serveurs veulent fournir du contenu adapté à chaque utilisateur. Par exemple, certains serveurs peuvent fournir des informations dans plusieurs langues ou d'autres serveurs veulent fournir des publicités ciblées pour différents types d'utilisateurs. Pour cela, les serveurs doivent conserver des informations sur les préférences de chaque utilisateur et utiliser ces informations pour produire un contenu correspondant aux préférences de l'utilisateur. HTTP contient plusieurs mécanismes qui permettent de résoudre ce problème. Nous en discutons de trois d'entre eux ci-dessous.</p>
                    <p>Une première solution consiste à obliger les utilisateurs à s'authentifier. C'est la solution utilisée par FTP pour contrôler les fichiers que chaque utilisateur peut accéder. Initialement, les noms d'utilisateur et les mots de passe pouvaient être inclus dans les URI RFC 1738. Cependant, placer des mots de passe en clair dans les URI RFC 1738. Cependant, placer des mots de passe en clair dans un URI potentiellement visible publiquement est complètement insécurisé et cette utilisation a maintenant été obsolète FRC 3986. HTTP prend en charge plusieurs en-têtes d'extension RFC 2617 qui peuvent être utilisés par un serveur pour demander l'authentification du client en fournissant ses informations d'identification. Cependant, les noms d'utilisateur et les mots de passe n'ont pas été populaires sur les serveurs Web car ils obligent les utilisateurs humains à se souvenir d'un nom d'utilisateur et d'un mot de passe par serveur. Se souvenir d'un mot de passe est acceptable lorsqu'un utilisateur a besoin d'accéder à un contenu protégé, mais les utilisateurs n'accepteront pas le besoin d'un nom d'utilisateur et d'un mot de passe uniquement pour recevoir des publicités ciblées des sites Web qu'ils visitent.</p>
                    <p>Une deuxième solution pour permettre aux serveurs d'ajuster le contenu aux besoins et aux capacités de l'utilisateur est de s'appuyer sur les différents types d'en-têtes HTTP <span class="html">Accept-*</span>. Par exemple, <span class="html">Accept-Language:</span> peut être utilisé par le client pour indiquer ses langues préférées. Malheureusement, en pratique, cet en-tête est généralement défini en fonction de la langue par défaut du navigateur et il n'est pas possible pour un utilisateur d'indiquer la langue qu'il préfère utiliser en sélectionnant des options sur chaque serveur Web visité.</p>
                    <p>La troisième solution, largement adoptée, est les cookies HTTP. Les cookies HTTP ont été initialement développés en tant qu'extension privée par Netscape. Ils font maintenant partie de la norme RFC 6265. En bref, un cookie est une courte chaîne choisie par un serveur pour représenter un client donné. Deux en-têtes HTTP sont utilisés : <span class="html">Cookie:</span> et <span class="html">Set-Cookie:</span>. Lorsqu'un serveur reçoit une demande HTTP d'un nouveau client (c'est-à-dire une demande HTTP qui ne contient pas l'en-tête <span class="em">Cookie:</span>), il génère un cookie pour le client et l'inclut dans l'en-tête <span class="em">Set-Cookie:</span> de la réponse HTTP renvoyée. L'en-tête <span class="em">Set-Cookie:</span> contient plusieurs paramètres supplémentaires, notamment les noms de domaine pour lesquels le cookie est valide. Le client stocke tous les cookies reçus sur le disque et à chaque fois qu'il envoie une demande HTTP, il vérifie s'il connaît déjà un cookie pour ce domaine. Si c'est le cas, il attache l'en-tête <span class="em">Cookie:</span> à la demande HTTP. Cela est illustré dans la figure ci-dessous avec HTTP 1.1, mais les cookies fonctionnent également avec HTTP 1.0.</p>
                    <figure>
                        <img src="../images/cookies_HTTP.png" alt="">
                        <figcaption>Figure 3.22 : cookies HTTP</figcaption>
                    </figure>
                    <hr>
                    <p>Note : Problèmes de confidentialité avec les cookies HTTP :</p>
                    <p>Les cookies HTTP introduits par Netscape sont Netscape sont essentiels pour les sites de commerce électronique. Cependant, ils ont également soulevé de nombreuses discussions concernant leurs utilisations potentiellement malveillantes. Considérons l'entreprise <span class="em">ad.com</span>, qui diffuse de nombreuses publicités sur des sites web. Un site web qui souhaite inclure les publicités d'ad.com à côté de son contenu ajoutera des liens vers ad.com dans ses pages HTML. Si ad.com est utilisé par de nomreux sites web, ad.com pourrait être en mesure de suivre les intérêts de tous les utilisateurs qui visitent les sites web de ses clients et d'utiliser ces informations pour fournir des publicités ciblées. Des défenseurs de la vie privée ont même poursuivi des entreprises de publicité en ligne pour les obliger à se conformer aux réglementations en matière de vie privée. Des technologies plus récentes liées soulèvent également des préoccupations en matière de vie privée.</p>
                    <hr>
                    <h3>3.5 Écrire des applications réseau simples :</h3>
                    <p>Les applications réseau étaient généralement implémentées en utilisant l'<span class="html">API socket</span>. Cette API a été conçue lors de la première implémentation de TCP/IP dans le système d'exploitation UNix BSD [Sechrest] [LFJMT], et a servi de modèle pour de nombreuses API entre les applications et la pile réseau dans un système d'exploitation. Bien que l'API socket soit très populaire, d'autres API ont également été développées. Par exemple, l'API STREAMS a été ajouté à plusieurs variantes d'Unix System V [Rago 1993]. L'API socket est prise en charge par la plupart des langages de programmation et plusieurs manuels lui ont été consacrés. Les utilisateurs du langage C peuvent consulter [DC2009], [Stevens1998], [SFR2004] ou [Kerrisk2010]/ L'implémentation Java de l'API socket est décrite dans [CD2008] et dans le tutoriel Java. Dans cette section, nous utiliserons l'implémentation Python de l'API socket pour illustrer les concepts clés. Des informations supplémentaires sur cette API peuvent être trouvées dans la section socket de la documentation Python.</p>
                    <p>L'API socket est assez bas niveau et ne doit être utilisée que lorsque vous avez besoin d'un contrôle complet de l'accès réseau. Si votre application a simplement besoin, par exemple, de récupérer des données avec HTTP, il existe des API beaucoup plus simples et de plus haut niveau.</p>
                    <p>Une discussion détaillée de l'API socket est en dehors du champ d'application de cette section et les références citées ci-dessus fournissent une discussion détaillée de tous les détails de l'API socket. Comme point de départ, il est intéressant de comparer l'API socket avec les primitives de service que nous avons discutées dans le chapitre précédent. Considérons d'abord le service sans connexion qui se compose des deux primitives suivantes :</p>
                    <ul>
                        <li>
                            <p><span class="html">DATA.request(destination,message)</span> est utilisé pour envoyer un message à une destination spécifiée. Dans cette API socket, cela correspond à la méthode <span class="html">send</span>.</p>
                        </li>
                        <li>
                            <p><span class="html">DATA.indication(message)</span> est émis par le service de transport pour livrer un message à l'application. Dans l'API socket, cela correspond au retour de la méthode <span class="html">recv</span> qui est appelée par l'application.</p>
                        </li>
                    </ul>
                    <p>Les primitives <span class="html">DATA</span> sont échangées via un point d'accès de service. Dans l'API socket, l'équivalent du point d'accès de service est la <span class="html">socket</span>. Une socket est une structure de données qui est maintenue par la pile réseau et est utilisée par l'application à chaque fois qu'elle a besoin d'envoyer ou de recevoir des données via la pile réseau. La mthode <span class="html">socket</span> dans l'API Python prend deux arguments principaux :</p>
                    <ul>
                        <li>
                            <p>Une famille d'adresses qui spécifie le type de famille d'adresses et donc la pile réseau sous-jacente qui sera utilisée avec la socket. Ce paramètre peut être soit <span class="html">socket.AF_IFNET</span> ou <span class="html">socket.AF_INET6</span>. <span class="em">socket.AF_INET</span>, qui correspond à la pile de protocole TCP/IPv4, est la valeur par défaut. <span class="em">socket.AF_INET6</span> correpond à la pile de protocoles TCP/IPv6.</p>
                        </li>
                        <li>
                            <p>Un type qui indique le type de service attendu de la pile réseau. <span class="html">socket.STREAM</span> (la valeur par défaut) correspond au service de connexion fiable à flux d'octets. <span class="html">socket.DGRAM</span> corrspond au service sans connexion.</p>
                        </li>
                    </ul>
                    <p>Un client simple qui envoie une demande à un serveur est souvent écrit comme suit dans les descriptions de l'API socket.</p>
<pre><code># A simple client of the connectionless service
import socket import sys HOSTIP=sys.argv[1]
PORT=int(sys.argv[2]) MSG="Hello, World!"
s = socket.socket( socket.AF_INET, socket.SOCK_DGRAM )
s.sendto( MSG, (HOSTIP, PORT) )</code></pre>
                    <p>Une utilisation typique de cette application serait : <span class="html">python client.py 127.0.0.1 12345</span> où <span class="em">127.0.0.1</span> est l'adresse IPv4 de l'hôte (dans ce cas, <span class="em">localhost</span>) où le serveur est en cours d'exécution et <span class="em">12345</span> est le port du serveur.</p>
                    <p>La première opération consiste à créer le <span class="em">socket</span>. Deux paramètres doivent être spécifiés lors de la création d'un <span class="em">socket</span>. Le premier paramètre indique la famille d'adresses et le second le type de socket. La deuxième opération est la transmission du message en utilisant <span class="html">sendto</span> au serveur. Il convient de noter que <span class="html">sendto</span> prend en argument le message à transmettre et un tuple qui contient l'adresse IPv4 du serveur et son numéro de port.</p>
                    <p>Le code présenté ci-dessus ne prend en charge que la pile de protocoles TCP/IPv4. Pour utiliser la pile de protocoles TCP/IPv6, le socket doit être créé en utilisant la famille d'adresses <span class="html">socket.AF_INET6</span>. Forcer le développeur de l'application à sélectionner TCP/IPv4 ou TCP/TPv6 lors de la création d'un socket est un obstacle majeur au déploiement et à l'utilisation de TCP/IPv6 dans l'Internet global [Cheshire2010]. Bien que la plupart des systèmes d'exploitation prennent en charge à la fois TCP/IPv4 et TCP/IPv6, de nombreuses applications n'utilisent encore que TCP/IPv4 par défaut. À long terme, l'API de socket devrait être capable de gérer TCP/IPv4 et TCP/IPv6 de manière transparente et ne devrait pas obliger le développeur d'application à spécifier systématiquement s'il utilise TCP/IPv4 ou TCP/IPv6.</p>
                    <p>Une autre limitation importante de l'API de socket telle que prise en charge par Python est qu'elle force l'application à traiter les adresses IP au lieu de traiter directement les noms de domaine. Cette limitation remonte aux premiers jours de l'API de socket dans UNix 2.2BSD. À cette époque, le DNS n'était pas largement disponible et seules les adresses IP pouvaient être utilisées. La plupart des applications s'appuient sur les noms de domaine pour interagir avec les serveurs et cette utilisation du DNS joue un rôle très important pour mettre à l'échelle les serveurs web et les réseaux de distribution de contenu. Pour utiliser des noms de domaine, l'application doit effectuer la résolution DNS en utilisant la méthode <span class="html">getaddrinfo</span>. Cette méthode interroge le DNS et construit la structure de données <span class="html">sockaddr</span> qui est utilisée par d'autres méthodes de l'API de socket. En Python, <span class="html">getaddrinfo</span> prend plusieurs arguments :</p>
                    <ul>
                        <li>
                            <p>Un <span class="html">nom</span> qui est le nom de domaine pour lequel le DNS sera interrogé.</p>
                        </li>
                        <li>
                            <p>Un <span class="html">numéro de port</span> facultatif qui est le numéro de port du serveur distant.</p>
                        </li>
                        <li>
                            <p>Une <span class="html">familel d'adresses</span> facultative qui indique la famille d'adresses utilisée pour la requête DNS. <span class="em">socket.AF_INET</span> (resp. <span class="em">socket.AF_INET6</span>) indique qu'une adresse IPv4 (IPv6) est attendue. En outre, l'API de socket Python permet à une application d'utiliser <span class="html">socket.AF_UNSPEC</span> pour indiquer qu'elle peut utiliser des adresses IPv4 ou IPv6.</p>
                        </li>
                        <li>
                            <p>Un <span class="html">type de socket</span> facultatif qui peut être soit <span class="html">socket.SOCK_DGRAM</span> ou <span class="html">socket.SOCK_STREAM</span>.</p>
                        </li>
                   </ul>
                   <p>Dans les hôtes Internet actuels capables de prendre en charge à la fois IPv4 et IPv6, toutes les applications devraient être capables de gérer à la fois les adresses IPv4 et IPv6. lorsqu'il est utilisé avec le paramètre <span class="html">socket.AF_UNSPEC</span>, la méthode <span class="html">socket.getaddrinfo</span> renvoie une liste de tuples contenant toutes les informations pour créer un socket.</p>
<pre><code>import socket socket.getaddrinfo('www.example.net',80,socket.AF_UNSPEC,socket.SOCK_STREAM)
[ (30, 1, 6, '', ('2001:db8:3080:3::2', 80, 0, 0)),
(2, 1, 6, '', ('203.0.113.225', 80))]</code></pre>
                   <p>Dans l'exemple ci-dessus, <span class="html">socket.getaddrinfo</span> renvoie deux tuples. Le premier correspond à <span class="html">sockaddr</span> contenant l'adresse IPv6 du serveur distant et le second correspond aux informations IPv4. En raison de certaines particularités d'IPv6 et d'IPv4, le format des deux tuples n'est pas exactement le même, mais les informations clés dans les deux cas sont l'adresse de la couche réseau (<span class="em">2001:db8:3080:3::2</span> et <span class="em">203.0.113.225</span>) et le numéro de port (<span class="em">80</span>). Les autres paramètres sont rarement utilisés.</p>
                    <p><span class="html">socket.getaddrinfo</span> peut être utilisé pour construire un client simple qui interroge le DNS et contacte le serveur en utilisant soit IPv4 ou IPv6 en fonction des adresses renvoyées par la méthode <span class="html">socket.getaddrinfo</span>. Le client ci-dessous itère sur la liste des adresses renvoyées par le DNS et envoie sa requête à la première adresse de destination pour laquelle il peut créer une <span class="em">socket</span>. D'autres stratégies sont bien sûr possibles. Par exemple, un hôte fonctionnant dans un réseau IPv6 pourrait préférer toujours utiliser IPv6 lorsque IPv6 est disponible. La plupart des systèmes d'exploitation aujourd'hui préfèrent par défaut utiliser IPv6 lorsque le DNS renvoie à la fois une adresse IPv4 et une adresse IPv6 pour un nom. Voir <a href="http://ipv6int.net/systems/" target="_blank">http://ipv6int.net/systems/</a> pour plus d'informations détaillées. Un autre exemple est l'approche "happy eyeballs" qui est en cours de discussion au sein de l'IETF [WY2011]. Par exemple, [WY2011] mentionne que certains navigateurs Web essaient d'utiliser la première adresse renvoyée par <span class="html">socket.getaddrinfo</span>. S'il n'y a pas de réponse dans un délai court (par exemple, 300 millisecondes), la deuxième adresse est essayée.</p>
<pre><code>import socket import sys
HOSTNAME=sys.argv[1]
PORT=int(sys.argv[2])
MSG="Hello, World!"
for a in socket.getaddrinfo(HOSTNAME, PORT, socket.AF_UNSPEC,socket.SOCK_DGRAM,0, socket.AI_PASSI address_family,sock_type,protocol,canonicalname, sockaddr=a
    try:
        s = socket.socket(address_family, sock_type)
    except socket.error:
        s = None
        print "Could not create socket"
        continue
    if s is not None:
        s.sendto(MSG, sockaddr)
        break</code></pre>
                    <p>Maintenant que nous avons décrit l'utilisation de l'API socket pour écrire un client simple utilisant le service de datagramme non connecté, examinons de plus près le service de transport de flux de bytes fiable. Comme expliqué ci-dessus, ce service est invoqué en créant une stocket de type <span class="html">socket.SOCK_STREAM</span>. Une fois qu'une socket a été créée, un client se connectera généralement au serveur distant, enverra des données, attendra une réponse et finira par fermer la connexion. Ces opérations sont effectuées en appelant les méthodes suivantes :</p>
                    <ul>
                        <li>
                            <p><span class="html">socket.connect</span> : cette méthode prend une structure de données <span class="html">sockaddr</span>, généralement renvoyée par <span class="html">socket.getaddrinfo</span>, en argument. Elle peut échouer et lever une exception si le serveur distant ne peut pas être atteint.</p>
                        </li>
                        <li>
                            <p><span class="html">socket.send</span> : cette méthode prend une chaîne de caractères en argument et renvoie le nombre d'octets qui ont effectivement été envoyés. La chaîne sera transmise comme une séquence d'octets consécutifs au serveur distant. Les applications sont censées vérifier la valeur renvoyée par cette méthode et devraient renvoyer les octets qui n'ont pas été envoyés.</p>
                        </li>
                        <li>
                            <p><span class="html">socket.recv</span> : cette méthode prend un entier en argument qui indique la taille du tampon qui a été alloué pour recevoir les données. Un point important à noter concernant l'utilisation de la méthode <span class="html">socket.recv</span> est que, comme elle s'exécute au-dessus d'un service de flux de bytes, elle peut renvoyer n'importe quelle quantité d'octets (jusqu'à la taille du tampon fourni par l'application). L'application doit collecter toutes les données reçues et il n'y a aucune garantie qu'une donnée envoyée par l'hôte distant en utilisant un seul appel à la méthode <span class="html">socket.send</span> sera reçue par la destination avec un seul appel à la méthode <span class="html">socket.recv</span>.</p>
                        </li>
                        <li>
                            <p><span class="html">socket.shutdown</span> : cette méthode est utilisée pour libérer la connexion sous-jacente. Sur certaines plates-formes, il est possible de spécifier la direction de transfert à libérer (par exemple, <span class="html">socket.SHUT_WR</span> pour libérer la direction sortante ou <span class="html">socket.SHUT_RDWR</span> pour libérer les deux directions).</p>
                        </li>
                        <li>
                            <p><span class="html">socket.close</span> : cette méthode est utilisée pour fermer la socket. Elle appelle <span class="html">socket.shutdown</span> si la connexion sous-jacente est encore ouverte.</p>
                        </li>
                    </ul>
                    <p>Avec ces méthodes, il est maintenant possible d'écrire un client HTTP simple. Ce client fonctionne sur IPv6 et IPv4 et écrit la page d'accueil du serveur distant sur la sortie standard. Il signale également le nombre d'appels à <pan class="html">socket.recv</pan> peut varier à chaque exécution. Il y a divers facteurs qui influencent le nombre de ces appels qui sont nécessaires pour récupérer des informations à partir d'un serveyr. Nous endiscuterons après avoir expliqué le fonctionnement du protocole de transport sous-jacent.</p>
<pre><code>#!/usr/bin/python
# A simple http client that retrieves the first page of a web site

import socket, sys

if len(sys.argv)!=3 and len(sys.argv)!=2:
    print "Usage : ",sys.argv[0]," hostname [port]"

hostname = sys.argv[1]
if len(sys.argv)==3 :
    port=int(sys.argv[2])
else:
    port = 80

READBUF=16384 # size of data read from web server
s=None

for res in socket.getaddrinfo(hostname, port, socket.AF_UNSPEC, socket.SOCK_STREAM):
    af, socktype, proto, canonname, sa = res
    # create socket
    try:
        s = socket.socket(af, socktype, proto)
    except socket.error:
        s = None
        continue
    # connect to remote host
    try:
        print "Trying "+sa[0]
        s.connect(sa)
    except socket.error, msg:
        # socket failed
        s.close()
        s = None
        continue
    if s :
        print "Connected to "+sa[0]
        s.send('GET / HTTP/1.1\r\nHost:'+hostname+'\r\n\r\n')
        finished=False
        count=0
        while not finished:
            data=s.recv(READBUF)
            count=count+1
            if len(data)!=0:
                print repr(data)
            else:
                finished=True
        s.shutdown(socket.SHUT_WR)
        s.close()
        print "Data was received in ",count," recv calls"
        break</code></pre>
                    <p>Comme mentionné précédemment, l'API de socket est très bas niveau. C'est l'interface vers le service de transport. Pour une tâche courante et simple, telle que récupérer un document depuis le Web, il existe des solutions bien plus simples. Par exemple, la bibliothèque standard de Python comprend plusieurs API de haut niveau pour les implémentations de divers protocoles de couche application, y compris HTTP. Par exemple, le module <span class="html">httplib</span> peut être utilisé pour accéder facilement aux documents via HTTP.</p>
<pre><code>#!/usr/bin/python
# A simple http client that retrieves the first page of a web site, using
# the standard httplib library

import httplib, sys

if len(sys.argv)!=3 and len(sys.argv)!=2:
    print "Usage : ",sys.argv[0]," hostname [port]"
    sys.exit(1)

path = '/'
hostname = sys.argv[1]
if len(sys.argv)==3 :
    port = int(sys.argv[2])
else:
    port = 80

conn = httplib.HTTPConnection(hostname, port)
conn.request("GET", path)
r = conn.getresponse()
print "Response is %i (%s)" % (r.status, r.reason)
print r.read()</code></pre>
                    <p>Un autre module, <span class="html">urllib2</span>, permet au programmeur d'utiliser directement des URL. C'est beaucoup plus simple que d'utiliser directement des sockets.</p>
                    <p>Mais la simplicité n'est pas le seul avantage de l'utilisation de bibliothèques de haut niveau. Elles permettent au programmeyr de manipuler des concepts de plus haut niveau (par exemple, je veux le contenu pointé par cette URLà, mais comprennent également de nombreuses fonctionnalités telles que le support transparent de l'utilisation de TLS ou d'IPv6.</p>
                    <p>Le deuxième type d'applications qui peuvent être écrites en utilisant l'API de socket sont les serveurs. Un serveur fonctionne généralement en permanence en attendant de traiter les demandes provenant de clients distants. Un serveur utilisant le service sans connexion commencera généralement par la création d'un socket avec <span class="html">socket.socket</span>. Ce socket peut être créé au-dessus de la pile TCP/IPv4 (<span class="html">socket.AF_INET</span>) ou de la pile TCP/IPv6 (<span class="html">socket.AF_INET6</span>), mais pas les deux par défaut. Si un serveur souhaite utiliser les deux piles réseau, il doit créer deux threads, l'un pour gérer le socket TCP/IPv4 et l'autre pour gérer le socket TCP/IPv6. Malheureusement, il est impossible de définir un socket qui peut recevoir des données à partir des deux piles réseau en même temps avec l'API de socket de Python.</p>
                    <p>Un serveur utilisant le service sans connexion utilisera généralment deux méthodes de l'API de socket en plus de celles que nous avons déjà discutées.</p>
                    <ul>
                        <li>
                            <p><span class="html">socket.bind</span> est utilisé pour lier un socket à un numéro de port et éventuellement à une adresse IP. La plupart des serveurs lient leur socket à toutes les interfaces disponibles sur les serveurs, mais il existe des situations où le serveur peut préférer être lié uniquement à des adresses IP spécifiques. Par exemple, un serveur fonctionnant sur un smartphone peut vouloir être lié à l'adresse IP de l'interface WIFI mais pas sur l'interface 3G qui est plus coûteuse.</p>
                        </li>
                        <li>
                            <p><span class="html">socket.recvfrom</span> est utilisé pour recevoir des données à partir de la pile réseau sous-jacente. Cette méthode renvoie à la fois l'adresse de l'expéditeur et les données reçues.</p>
                        </li>
                    </ul>
                    <p>Le code ci-dessous illustr un serveur très simple fonctionnant au-dessus du service de transport sans connexion qui imprime simplement sur la sortie standard tous les messages reçus. Ce serveur utilise la pile réseau TCP/IPv6.</p>
<pre><code>import socket, sys

PORT=int(sys.argv[1])
BUFF_LEN=8192

s=socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)
s.bind(('',PORT,0,0))
while True:
    data, addr = s.recvfrom( BUFF_LEN )
    if data=="STOP" :
        print "Stopping server"
        sys.exit(0)
        print "received from ", addr, " message:", data</code></pre>
                    <p>Un serveur qui utilise le service de flux de données fiable peut également être construit au-dessus de l'API socket. Un tel serveur commence par céer un socket qui est lié au port choisi pour le serveur. Ensuite, le serveur appelle la méthode <span class="html">socket.listen</span>. Cela informe la pile du réseau sous-jacente du nombre de tentatives de connexion de transport qui peuvent être mises en file d'attente dans la pile de réseau sous-jacente en attente d'être acceptées et traitées par le serveur. Le serveur a généralement un thread en attente sur la méthode <span class="html">socket.accept</span>. Cette méthode retourne dès qu'une tentative de connexion est reçue par la pile sous-jacente. Elle retourne un socket qui est lié à la connexion établie et l'adresse de l'hôte distant. Avec ces méthodes, il est possible d'écrire un serveur web très simple qui renvoie toujours une erreur <span class="html">404</span> à toutes les requêtes <span class="html">GET</span>  et une erreur <span class="html">501</span> à toutes les autres requêtes.</p>
<pre><code># An extremely simple HTTP server
    
import socket, sys, time

# Server runs on all IP addresses by default
HOST=''
# 8080 can be used without root priviledges
PORT=8080
BUFLEN=8192 # buffer size

s = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)
try:
    print "Starting HTTP server on port ", PORT
    s.bind((HOST,PORT,0,0))
except socket.error :
    print "Cannot bind to port :",PORT
    sys.exit(-1)

s.listen(10) # maximum 10 queued connections

while True:
    # a real server would be multithreaded and would catch exceptions
    conn, addr = s.accept()
    print "Connection from ", addr
    data=''

    while not '\n' in data : # wait until first line has been received
    data = data+conn.recv(BUFLEN)
    if data.startswith('GET'):
        # GET request
        conn.send('HTTP/1.0 404 Not Found\r\n')
        # a real server should serve files
    else:
        # other type of HTTP request
        conn.send('HTTP/1.0 501 Not implemented\r\n')

    now = time.strftime("%a, %d %b %Y %H:%M:%S", time.localtime())
    conn.send('Date: ' + now +'\r\n')
    conn.send('Server: Dummy-HTTP-Server\r\n')
    conn.send('\r\n')
    conn.shutdown(socket.SHUT_RDWR)
    conn.close()</code></pre>
                    <p>Ce serveur est loin d'être un serveur web de qualité de production. Un vrai serveur web utiliserait plusieurs threads et/ou une IO non-bloquante pour traiter un grand nombre de requêtes concurrentes. Il existe de nombreux <span class="em">logiciels de serveurs web de qualité de production</span> disponible. <span class="em">Apache</span> est l'un d'entre eux, très complexe mais largement utilisé. <span class="html">thttpd</span> et <span class="html">lighttpd</span> sont moins complexes et leur code source est probablement plus facile à comprendre. De plus, il devrait également gérer toutes les erreurs qui pourraient se produire lors de la réception de données sur une connexion de transport. Celles-ci sont hors du champ d'application de cette section et des informations supplémentaires sur les applications réseau plus complexes peuvent être trouvées ailelurs. Par exemple, [RG2010] fournit une discussion approfondie de l'utilisation de l'API de socket avec Python tandis que [SFR2004] reste une excellente source d'informations sur l'API de socket en C.</p>
                    <h3>3.6 Résumé :</h3>
                    <p>Dans ce chapitre, nous avons commencé par décrire les modèles client-serveur et pair à pair. Nous avons ensuite ddécrit en détail trois familles  importantes de protocoles dans la couche d'application. Internet identifie les hôtes en utilisant les adresses IPv4 de 32 bits ou IPv6 de 128 bits. Cependant, l'utilisation de ces adresses directement dans les applications serait difficile pour les humains qui les utilisent. Nous avons expliqué comment le système de noms de domaine (ou DNS) permet le mapapge des noms en adresses correspandantes. Nous avons décrit à la fois le protocole DNS qui s'exécute au-dessus de UDP et la hiérarchie des noms. Nous avons ensuite discuté de l'une des plus anciennes applications sur Internet : le courrier électronique. Nous avons décrit le format des messages électroniques et décrit le protocole SMTP qui est utilisé pour envouer des messages électroniques ainsi que le protocole POP qui est utilisé par les destinataires de courrier électronique pour récupérer leurs messages électroniques à partir de leur serveur. Enfin, nous avons expliqué les protocoles utilisés dans le World Wide Web et en particulier le protocole de transfert hypertexte (HTTP).</p>
                    <h3>3.7 Exercices :</h3>
                    <p>Cette section contient plusieurs exercices et petits défis sur les protocoles de la couche d'application.</p>
                    <h4>3.7.1 Le DNS :</h4>
                    <p>Le sytsème de noms de domaine (DNS) joue un rôle clé dans l'Internet d'aujourd'hui car il permet aux applications d'utiliser des noms de domaine complets (FQDN) au lieu d'adresses IPv4 ou IPv6. De nombreux outils permettent d'effectuer des requêtes via des serveurs DNS. Pour cet exercice, nous utiliserons <span class="html">dig</span> qui est installé sur la plupart des systèmes Unix.</p>
                    <p>Une utilisation typique de <span class="html">dig</span>est la suivante :</p>
<pre><code>dig @server -t type fqdn</code></pre>
                    <p>où :</p>
                    <ul>
                        <li>
                            <p><span class="html">server</span> est l'adresse IP ou le nom d'un serveur DNS ou d'un résolveur.</p>
                        </li>
                        <li>
                            <p><span class="html">type</span> est le type d'enregistrement DNS demandé pour la requête, tel que <span class="html">NS</span> pour un serveur de noms, <span class="html">A</span> pour une adresse IPv4, <span class="html">AAAA</span> pour une adresse IPv6, <span class="html">MX</span> pour un relais de messagerie,...</p>
                        </li>
                        <li>
                            <p><span class="html">fqdn</span> est le nom de domaine complet (<span class="html">Fully Qualified Domain Name</span>) qui est interrogé.</p>
                        </li>
                    </ul>
                    <ol>
                        <li>
                            <p>Qulles sont les adresses IP des résolveurs sur lesquels repose l'implémentation de <span class="html">dig</span> que vous utilisez ? Sur une machine Linux, la section <span class="em">Description</span> de la page de manuel de <span class="html">dig</span> vous indique où <span class="html">dig</span> trouve la liste des serveurs de noms à interroger.</p>
                        </li>
                        <li>
                            <p>Quelle est l'adresse IP correspondant à <span class="em">inl.info.ucl.ac.be</span> ? Quel type de requête DNS envoie <span class="html">dig</span> pour obtenir cette information</p>
                        </li>
                        <li>
                            <p>Quel type de demande DNS devez-vous envoyer pour obtenir les serveurs de noms responsables d'un domaine donné ?</p>
                        </li>
                        <li>
                            <p>Quels sont les serveurs de noms responsables du domaine de premier niveau <span class="em">be</span> ? Où sont-ils situés ? Est-il possible de les interroger en IPv6 ?</p>
                        </li>
                        <li>
                            <p>Lorsqu'il est exécuté dans aucun paramètre, <span class="html">dig</span> interroge l'un des serveurs DNS de la racine et récupère la liste des noms de tous les serveurs DNS de la racine. Pour des raisons techniques, il n'y a que 13 serveurs DNS de la racine différents. Cette information est égalemnt disponible sous forme de fichier texte à partir de <a href="http://www.internic.net/zones/named.root" target="_blank">http://www.internic.net/zones/named.root</a>. Quelles sont les adresses IP de tous ces serveurs ? Peuvent-ils être interrogés en IPv6 ? Vous pouvez obtenir des informations supplémentaires sur les serveurs DNS de la racine à partir de <a href="http://www.root-servers.org/" target="_blank">http://www.root-servers.org/</a>.</p>
                        </li>
                        <li>
                            <p>Supposons maintenant que vous résidiez dans un réseau où il n'y a pas de résolveur DNS et que vous devez commencer votre requête à partir de la racine DNS.</p>
                            <ul>
                                <li>
                                    <p>Utilisez <span class="html">dig</span> dans une requête à l'un de ces serveurs racine afin de trouver l'adresse IP du ou des serveurs DNS (enregistrement NS) responsables du domaine de premier niveau <span class="em">org</span>.</p>
                                </li>
                                <li>
                                    <p>Utilisez <span class="html">dig</span> pour envoyer une requête à l'un de ces serveurs DNS pour trouver l'adresse IP du ou des serveurs DNS (enregistrement NS) responsables de <span class="em">root-servers.org</span>.</p>
                                </li>
                                <li>
                                    <p>Continuez jusqu'à ce que vous trouviez le serveur responsable de <span class="em">www.root-servers.org</span>.</p>
                                </li>
                                <li>
                                    <p>Quelle est la durée de vie associée à cette adresse IP ?</p>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <p>Effectuez la même analyse pour un site Web populaire tel que <span class="em">www.google.com</span>. Quelle est la durée de vie associée à cette adresse IP ? Si vous effectuez la même requête plusieurs fois, recevez-vous toujours la même réponse ? Pouvez-vous expliquer pour quoi une durée de vie est associée aux réponses DNS</p>
                        </li>
                        <li>
                            <p>Utilisez <span class="html">dig</span> pour trouver les relais de courrier utilisés par les domaines <span class="em">uclouvain.be</span> et <span class="em">gmail.com</span>. Quelle est la <span class="em">TTL</span> de ces enregistrements (utilisez l'option <span class="html">+ttlid</span> lors de l'utilisation de <span class="html">dig</span>) ? Pouvez-vous expliquer les préférences utilisées par les enregistrements <span class="em">MX</span> ? Vous pouvez trouver plus d'informations sur les enregistrements MX dans la RFC 974.</p>
                        </li>
                        <li>
                            <p>Utilisez <span class="html">dig</span> pour interroger l'adresse IPv6 (enregistrement DNS AAAA) des hôtes suivants :</p>
                            <ul>
                                <li>
                                    <p class="em">www.sixxs.net</p>
                                </li>
                                <li>
                                    <p class="em">www.google.com</p>
                                </li>
                                <li>
                                    <p class="em">ipv6.google.com</p>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <p>Lorsque <span class="html">dig</span> est exécuté, la section d'en-tête de sa sortie indique l'identifiant DNS utilisé pour envoyer la requête. Votre implémentation de <span class="html">dig</span> génère-t-elle des identifiants</p>
<pre><code>dig -t MX gmail.com

; &lt;&lt;&gt;&gt; DiG 9.4.3-P3 &lt;&lt;&gt;&gt; -t MX gmail.com
;; global options: printcmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 25718</code></pre>
                        </li>
                        <li>
                            <p>Les implémentations DNS telles que <span class="html">dig</span> et surtout les résolveurs de noms tels que <span class="html">bind</span> ou <span class="html">unbound</span>, vérifient toujours que la réponse DNS reçue contient le même identifant que la requête DNS qu'ils ont envoyée. Pourquoi est-ce si important</p>
                            <ul>
                                <li>
                                    <p>Imaginez un attaquant capble d'envoyer des réponses DNS fasifiées pour, par exemple, associer <span class="em">www.bigbank.com</span> à sa propre adresse IP. Comment pourrait-il attauqyer une implémentation DNS qui :</p>
                                    <ul>
                                        <li>
                                            <p>envoie des requêtes DNS contenant toujours le même identifiant.</p>
                                        </li>
                                        <li>
                                            <p>envoie des requêtes DNS contenant des identifiants qui sont incrémentés de un après chaque requête.</p>
                                        </li>
                                        <li>
                                            <p>envoie des requêtes DNS contenant des identifiants aléatoires.</p>
                                        </li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <p>Le protocole DNS peut être utilisé à la fois sur UDP et sur TCP. La plupart des serveurs DNS préfèrent utiliser UDP car il consomme moins de ressources sur le serveur. Cependant, TCP est utile lorsque l'on s'attend à une réponse volumineuse ou lorsque celle-ci doit être garantie. On peut forcer l'utilisation de TCP en utilisant <span class="html">dig +tcp</span>. Utilisez TCP et UDP pour interroger un serveur DNS racine. Est-il plus rapide de recevoir une réponse via TCP ou via UDP</p>
                        </li>
                    </ol>
                    <h4>3.7.2 Protocoles de messagerie électronique Internet :</h4>
                    <p>De nombreux protocoles Internet sont basés sur ASCII, où le client envoie des requêtes sous forme d'une ligne de texte ASCII terminée par <span class="em">CRLF</span> et le serveur répond avec une ou plusieurs lignes de texte ASCII. L'utilisation de ces messages ASCII présente plusieurs avantages par rapport aux protocoles qui reposent sur des messages codés en binaire :</p>
                    <ul>
                        <li>
                            <p>Les messages échangés par le client et le serveur peuvent être facilement compris par un développeir ou un ingénieur réseau en lisant simplement les messages.</p>
                        </li>
                        <li>
                            <p>Il est souvent facile d'écrire un petit prototype qui implémente une partie du protocole.</p>
                        </li>
                        <li>
                            <p>Il est possible de tester un serve!r manuellement en utilisant Telnet. Telnet est un protocole qui permet d'obtenir un terminal sur un serveur distant. Pour cela, Telnet ouvre une connexion TCP avec le serveur distant sur le port 23. Cependant, la plupart des implémentations de <span class="em">telnet</span> pemrettent à l'utilisateur de spécifier un port alternatif comme <span class="html">telnet host port</span>. Lorsqu'il est utilisé avec un numéro de port en tant que paramètre, <span class="em">telnet</span> ouvre une connexion TCP vers l'hôte distant sur le port spécifié. <span class="em">telnet</span> peut donc être utilisé pour tester n'importe quel serveur utilisant un protocole basé sur ACII sur TCP. Notez que si vous devez arrêter une session <span class="em">telnet</span> en cours d'exécution, Ctrl+C ne fonctionnera pas car il sera envoyé par <span class="em">telnet</span> au serveur distant via la connexion TCP/ Sur de nombreuses implémentations de <span class="em">telnet</span>, vous pouvez taper <span class="html">Ctrl+J</span> pour suspendre la connexion TCP et revenir à l'interface telnet.</p>
                        </li>
                    </ul>
                    <ol>
                        <li>
                            <p>Supposons qu'Alice envoie un e-mail depuis son compte <span class="em">alice@yahoo.com</span> à Bob qui utilise <span class="em">bob@yahoo.com</span> Quels protocoles sont impliqués dans la transmission de cet email</p>
                        </li>
                        <li>
                            <p>Même question lorsque Alice envoie un e-mail à son amie Trudy, <span class="em">trudy@gmail.com</span>.</p>
                        </li>
                        <li>
                            <p>Avant l'avènement des webmails et des clients de messagerie riches en fonctionnalités, les e-mails étaient écrits et lus à l'aide d'outils en ligne de commande sur les serveurs. En utilisant votre compte sur <span class="em">sirius.info.ucl.ac.be</span>, utilisez l'outil en ligne de commande <span class="html">/bin/mail</span> pour vous envoyer un e-mail sur cet hôte. Ce serveur stocke les e-mails loaux dans le répertoire <span class="html">/var/mail</span> avec un fichier par utilisateur. Vérifier avec <span class="html">/bin/more</span> le contenu de votre fichier de messagerie et essayez de comprendre quelles lignes ont été ajoutées par le serveur dans l'en-tête de votre e-mail.</p>
                        </li>
                        <li>
                            <p>Utilisez votre outil de messagerie électronique préféré pour vous envoyer un message électronique contenant une seule ligne de texte. La plupart des outils de messagerie électronique ont la possibilité d'afficher la source du message, utilisez cette fonction pour regarder le message que vous avez envoyé et le message que vous avez reçu. Pouvez-vous trouver une explication pour toutes les lignes qui ont été ajoutées à votre e-mail d'une signe ligne ? Depuis la RFC 821, SMTP a beaucoup évolué en raison notamment de l'utilisation croissante de la messagerie électronique et du besoin deprotéger le système de messagerie électronique contre les spammeurs. Il est peu probable que vous puissiez expliquer toutes les lignes supplémentaires que vous trouverez dans les en-têtes de courrier électronique, mais nous les examinerons ensemble.</p>
                        </li>
                        <li>
                            <p>La première version du protocole SMTP a été définie dans le RFC 821. La norme actuelle pour SMTP dans le RFC 5321. En considérant uniquement le RFC 821, quelles sont les prinicpales commandes du protocole <span class="em">SMTP</span> ? Une description plus courte du protocole SMTP peut être trouvée sur Wikipédia à l'adresse <a href="http://fr.wikipedia.org/wiki/Simple_Mail_Transfer_Protocol" target="_blank">http://fr.wikipedia.org/wiki/Simple_Mail_Transfer_Protocol</a>.</p>
                        </li>
                        <li>
                            <p>Lors de l'utilisation de SMTP, comment peut-on reconnaître une réponse positive d'une réponse négative ?</p>
                        </li>
                        <li>
                            <p>Un serveur SMTP est un processus daemon qui peut échouer en raison d'un bogue ou d'un manqye de ressources (par exemple, la mémoire). Les administrateurs réseau installent souvent des outils qui se connectent régulièrement à leurs serveurs pour vérifier qu'ils fonctionnent correctement. Il xiste de nombreux <span class="em">outils de serveillance</span> disponible. <span class="em">Nagios</span> est un système de surveillance open source très populaire. Une solution simple consiste à ouvrir une connexion TCP sur le port 25 de l'hôte du serveur SMTP. Notez que l'utilisation de <span class="em">telnet</span> pour se connecter à un hôte distant sur le port 25 peut ne pas fonctionner dans tous les réseaux. En raison du problème de <span class="em">spam</span>, de nombreux réseaux <span class="em">ISP</span> n'autorisent pas leurs clients à utiliser directement le port TCP 25 et les obliger à utiliser le relais de messagerie de L'ISP pour tranférer leur corrier électronique. Grâce à cela, si un logiciel envoyant du spam a été installé sur le PC de l'un des clients de l'ISP, ce logiciel ne pourra pas envoyer une énorme quantité de spam. Si vous vous connectez à <span class="em">nostromo.info.ucl.ac.be</span> à partir des postes fixes du laboratoire INGI, vous ne devriez pas être bloqué. Que se passe-til si vous tapez la commande suivante :</p>
<pre><code>telnet cnp3.info.ucl.ac.be 25</code></pre>
                            <p><span class="em">Attention :</span> Ne faites pas cela sur un serveur SMTP aléatoire. Les exerices proposés dans cette section doivent être exécutés uniqyement sur le serveur SMTP dédié à ces exercices : <span class="em">cnp3.info.ucl.ac.be</span>. Si vous les essayez sur un serveur SMTP de production, l'administrateur de ce serveur pourrait se fâcher.</p>
                        </li>
                    </ol>
                    <ol>
                        <li>
                            <p>Continuez la session SMTP quer vous avez commencée ci-dessus en envoyant la commande de salutation (<span class="html">HELO</span> suivie du nom de domaine complet de votre hôte) et terminez la session en envoyant la commande <span class="html">QUIT</span>.</p>
                        </li>
                        <li>
                            <p>La session SMTP minimale ci-dessis permet de vérifier que SMTP fonctionne. Cependant, cela n'implique pas toujours que le courrier peut être livré. Par exemple, les grands serveurs SMTP utilisent souvent une base de données pour stocker toutes les adresses e-mail qu'ils desservent. Pour vérifier le bon fonctionnement d'un tel serveur, une possibilité et d'utiliser la commande <span class="html">VRFY</span>. Ouvrez une session SMTP sur le serveur SMTP du laboratoire (<span class="em">cnp3.info.ucl.ac.be</span>) et utilisez cette commande pour vérifier que votre compte est actif.</p>
                        </li>
                        <li>
                            <p>Maintenant que vous connaissez les bases de l'ouverture et de la fermeture d'une session SMTP, vous pouvez maintenant envoyer des courriels manuellement en utilisant les commandes <span class="html">MAIL FROM :</span>, <span class="html">RCPT TO:</span> et <span class="html">DATA</span>. Utilisez ces commandes pour envoyer manuellement un courriel à <span class="em">INGI2141@cnp3.info.ucl.ac.be</span>. N'oubliez pas d'inclure les lignes <span class="em">From:</span>, <span class="em">To:</span> et <span class="em">Subkject:</span> dans votre en-tête.</p>
                        </li>
                        <li>
                            <p>En utilisant SMTP, est-il possible d'envoyer un courriel qui contient exactement l'art ASCII suivant ?</p>
<pre><code>.
..
...</code></pre>
                        </li>
                    </ol>
                    <ol>
                        <li>
                            <p>La plupart des agents de messagerie électronique vous permettent d'envoyer des emails en copie carbone (<span class="em">cc:</span>) et également en copie carbone cachée (<span class="em">bcc:</span>) à un destinataire. Comment un serveur SMTP prend-il en charge ces deux types de destinataires ?</p>
                        </li>
                        <li>
                            <p>Au début, les emails étaient lus en utilisant des outils tels que <span class="html">/bin/mail</span> ou des lecteurs de courrier électronique avancés basés sur du texte tels que <span class="html">pine</span> ou <span class="html">elm</span>. Aujourd'hui, les emails sont stockés sur des serveurs dédiés et récupérés en utilisant des protocoles tels que <span class="html">POP</span> ou <span class="html">IMAP</span>. Du point de vue de l'utilisateur, pouvez-vous énumérer les avantages et les inconvénients de ces deux protocoles ?</p>
                        </li>
                        <li>
                            <p>Le protocole TCP prend en charge 65536 (2^15) numéros de ports différents. De nombreux numéros de port ont été réservés pour certaines applications. Le dépôt officiel des numéros de prorts réservés est géré par l'<span class="html">Internet Assigned Numbers Authority (IANA)</span> sur <a href="http://www.iana.org/assignments/port-numbers" target="_blank">http://www.iana.org/assignments/port-numbers</a>. Sur les hôtes Unix, un sous-ensemble des affectations de port est souvent placé dans <span class="em">/etc/services</span>. En utilisant cette information, quel est le numéro de port par défaut pour le protocole POP3 ? Fonctionne-t-il sur UDP ou TCP ?</p>
                        </li>
                        <li>
                            <p>Le protocole POP (Post Office Protocol) est un procole assez simple décrit dans la RFC 1939. POP fonctionne en trois phases. La première phase est la <span class="em">phase d'autorisation</span> où le client fourni un nom d'utilisateur et un mot de passe. La deuxième phase est la <span class="em">phase de transaction</span> où le client peut récupérer des e-mails. La dernière phase est <span class="em">la phase de mise à jour</span> où le client finalise la transaction. Quels sont les principales commandes POP et leurs paramètres ? Lorsqu'un serveur POP renvoie une réponse, comment pouvez-vous déterminer facilement si la réponse est positive ou négative</p>
                        </li>
                        <li>
                            <p>Sur les smarthphones, les utilisateurs veulent souvent éviter de télécharger de gros e-mails sur une connexion sans fil lente. Comment un client POP pourrait-il télécharger uniquement les e-mails qui sont inférieurs à 5 KBytes</p>
                        </li>
                        <li>
                            <p>Ouvrez une session POP avec le serveur POP du laboratoire (<span class="em">nostromo.info.ucl.ac.be</span>) en utilisant le nom d'utilisateur et le mot de passe que vous avez reçus. Vérifiez que votre nom d'utilisateur et votre mot de passe sont acceptés par le serveur.</p>
                        </li>
                        <li>
                            <p>Le serveur POP du laboratoire contient un script qui s'exécute toutes les minutes et envoie deux messages électroniques à votre compte si votre dossier de messagerie est vide. Utilisez POP pour récupérer ces deux e-mails et fournissez le message secret à votre assistant d'enseignement.</p>
                        </li>
                    </ol>
                    <h3>3.7.3 Le HTTP :</h3>
                    <ol>
                        <li>
                            <p>Quelles sont les principales méthodes supportées par la première version du HyperText Transfert Protocol (HTTP) définie dans la RFC 1945 ? Voir la section 5 de la RFC 1945. Quels sont les principaux types de réponses envoyées par un serveur http ? Voir la section 6.1 de la RFC 1945.</p>
                        </li>
                        <li>
                            <p>Les administrateurs système qui sont responsables des serveurs Web veulent souvent surveiller ces serveurs et vérifier qu'ils fonctionnent correctement. Comme un serveur HTTP utilise TCP sur le port 80, la solution la plus simple est d'ouvrir une connexion TCP sur le port 8° et de vérifier que la connexion TCP est acceptée par l'hôte distant. Cependant, comme HTTP est un protocole basé sur ASCII, il est également très facile d'écrire un petit script qui télécharge une page Web sur le serveur et compare son contenu avec celui attendu. Utilsez <span class="em">telnet</span> pour vérifier qu'un serveur Web fonctionne sur l'hôte <span class="em">rembrandt.info.ucl.ac.be</span>. La commande miniale envoyée à un serveur HTTP est <span class="html">GET / HTTP/1.0</span> suivie de CRLF et d'une ligne vide.</p>
                        </li>
                        <li>
                            <p>Au lieu d'utiliser <span class="em">telnet</span> sur le port 80, il est également possible d'utiliser un outil en ligne de commande tel que <span class="html">curl</span>. Utilisez <span class="html">curl</span> avec l'option <span class="html">--trace-ascii tracefile</span> pour stocker dans <span class="em">tracefile</span> toutes les informations échangées par <span class="html">curl</span> lors de l'accès au serveur.</p>
                            <ul>
                                <li>
                                    <p>Quelle est la version d'HTTP utilisée par <span class="html">curl</span> ?</p>
                                </li>
                                <li>
                                    <p>Pouvez-vous expliquer les différents en-têtes placés par <span class="html">curl</span> dans la requête ?</p>
                                </li>
                                <li>
                                    <p>Pouvez-vous expliquer les différents en-têtes trouvés dans la réponse ?</p>
                                </li>
                                <li>
                                    <p>HTTP 1.1, spécifié dans la RFC 2616, oblige le client à utiliser l'en-tête <span class="html">Host:</span> dans toutes ses requêtes. HTTP 1.0 ne définit pas l'en-tête <span class="html">Host:</span>, mais la plupart des implémentations le supportent. En utilisant <span class="html">telnet</span> et <span class="html">curl</span>, récupérez la première page du serveur Web <span class="em">http://totem.info.ucl.ac.be/</span> en envoyant des requêtes HTTP avec et sans l'en-tête <span class="html">Host:</span>. Expliquez la différence entre les deux. Utilisez <span class="html">dig</span> pour trouver l'adresse IP utilisée par <span class="em">totem.info.ucl.ac.be</span>.</p>
                                </li>
                                <li>
                                    <p>En utilisant <span class="html">dig</span> et <span class="html">curl</span>, déterminez sur quel hôte physique les sites <span class="em">http://www.info.ucl.ac.be/</span>, <span class="em">http://www.info.ucl.ac.be/</span> et <span class="em">http://totem.info.ucl.ac.be/</span> sont hébergés.</p>
                                </li>
                                <li>
                                    <p>Utilisez <span class="html">curl</span> avec l'option <span class="html">--trace-ascii filename</span> pour récupérer <span class="em">http://www.google.com/</span>. Expliquez ce que ferait un navigateur tel que Firefox lors de la récupération de cette URL.</p>
                                </li>
                                <li>
                                    <p>Ldes en-têtes envoyés dans une requête HTTP permettent au client de fournir des informations supplémentaires au serveur. L'un de ces en-têtes est l'en-tête <span class="em">Language</span> qui permet d'indiquer la langue préférée du client. La liste des balises de langue disponibles peut être trouvée sur <a href="http://www.loc.gov/standards/iso639-2/php/code_list.php" target="_blank">http://www.loc.gov/standards/iso639-2/php/code_list.php</a>. Des informations supplémentaires sur la prise en charge de plusieurs langues dans les protocoles Internet peuvent être trouvées dans la RFC 5646. Par exemple, la commande "<span class="html">curl -HAccept-Language: en http://www.google.be</span>" enverra à <span class="em">http://www.google.be/</span> une requête HTTP infiquant l'anglais (<span class="em">en</span>) comme langue préférée. Google fournit-il une page différente en français (<span class="em">fr</span>) et en wallon (<span class="em">wa</span>) ? Même question pour <span class="em">http://www.uclouvain.be/</span> (étant donné la taille de la page d'accueil, utilisez "<span class="html">diff</span>" pour comparer les différentes pages récupérées à partir de <span class="em">www.uclouvain.be</span>).</p>
                                </li>
                                <li>
                                    <p>Comparez la taille des pages web <span class="em">http://www.yahoo.com</span> et <span class="em">http://www.google.com</span> en les téléchargeant avec <span class="html">curl</span>.</p>
                                </li>
                                <li>
                                    <p>Qu'est-ce qu'un cookie HTTP ? Énumérez certains avantages et inconvénients de l'utilisation de cookies sur les serveurs Web.</p>
                                </li>
                                <li>
                                    <p>Vous êtes maintenant responsable du site <span class="em">http://www.belgium.be</span>. Le gouvernement a construit deux <span class="em">centres de données</span> contenant chacun 1000 serveurs à Anvers et Namur. Ce site web contient des informations statiques et votre objectif est d'équilibrer la charge entre les différents serveurs et de garantir que le service reste disponible même si l'un des centres de données est déconnecté d'Internet en raison d'inondations ou d'autres catastrophes naturelles. Quelles sont les techniques que vous pouvez utiliser pour atteindre cet objectif ?</p>
                                </li>
                            </ul>
                        </li>
                    </ol>
                </article>
                <article>
                    <h2 id="couche_transport">Partie 4 : La couche Transport :</h2>
                    <h3>4.1 Principes d'un protocole de transport fiable :</h3>
                    <p>Dans cette section, nous décrivons un protocole de transport fiable s'exécutant au-dessus d'un service de couche réseau sans connexion. Pour cela, nous supposons d'abord que la couche réseau fournit un service parfait, c'est-à-dire :</p>
                    <ul>
                        <li>
                            <p>Le service de couche réseau sans connexion ne corrompt jamais les unités de données de service (SDU).</p>
                        </li>
                        <li>
                            <p>Le service de couche réseau sans connexion ne rejette jamais les SDU.</p>
                        </li>
                        <li>
                            <p>Le service de couche réseau sans connexion ne retarde jamais, ne réordonne jamais et ne duplique jamais les SDU.</p>
                        </li>
                        <li>
                            <p>Le service de couche réseau sans connexion peut prendre en charge des SDU de n'importe quelle taille.</p>
                        </li>
                    </ul>
                    <p>Nous retirerons ensuite chacune de ces hypothèses l'une après l'autre afin de mieux comprendre les mécanismes utilisés pour résoudre chaque imperfection.</p>
                    <h4>4.1.1 Transfert de données fiable sur une couche réseau parfaite :</h4>
                    <p>L'entité de la cuche transport interagit à la fois avec un utilisateur dans la couche application et une entité dans la couche réseau. Selon le modèle de référence, ces interactions seront effectuées en utilisant les primitives <span class="html">DATA.req</span> et <span class="html">DATA.ind</span>. Cependant, pour simplifier la présentation et éviter la confusion entre une primitive <span class="em">DATA.req</span> émise par l'utilisateur de l'entité de la couche transport et une <span class="em">DATA.req</span> émise par l'entité de la couche transport elle-même, nous utiliserons la terminologie suivante :</p>
                    <ul>
                        <li>
                            <p>Les interactions entre l'utilisateur et l'entité de la couche transport sont représentées en utilisant les primitives classiques <span class="em">DATA.req</span> et <span class="em">DATA.ind</span>.</p>
                        </li>
                        <li>
                            <p>Les interactions entre l'entité de la couche transport et le service de la couche réseau sont représentées en utilisant <span class="html">send</span> à la place de <span class="em">DATA.req</span> et <span class="html">recvd</span> à la place de <span class="em">DATA.ind</span>.</p>
                        </li>
                    </ul>
                    <p>Cela est illustré dans la figure ci-desosus.</p>
                    <figure>
                        <img src="../images/interactions_couche_transport_utilisateur_fournisseur_couche_reseau.PNG" alt="">
                        <figcaption>Figure 4.1 : Interactions entre la couche de transport, son utilisateur et son fournisseur de couche réseau</figcaption>
                    </figure>
                    <p>Lorsqu'il fonctionne au-dessus d'un service de réseau sans connexion parfait, une entité de niveau transport peut simplement émettre un <span class="html">send(SDU)</span> à l'arrivée d'un <span class="html">DATA.req(SDU)</span> lorsqu'il reçoit un <span class="html">recvd(SDU)</span>. Un tel protocole simple est suffisant lorsqu'un seul SDU est envoyé.</p>
                    <figure>
                        <img src="../images/simple_protocole_transport.PNG" alt="">
                        <figcaption>Figure 4.2 : Le protocole de transport le plus simple</figcaption>
                    </figure>
                    <p>Malheureusement, cela n'est pas toujours suffisant pour assurer une livraison fiable des SDU. Considérons le cas où un client envoie des dizaines de SDU à un serveur. Si le serveur est plus rapide que le client, il sera capable de recevoir et de traiter tous les segments envoyés par le client et de livrer leur contenu à son utilisateur. Cependant, si le serveur est plus lent que le client, des problèmes peuvent survenir. L'entité de la couche transport contient des trampons pour stocker les SDU qui ont été reçus en tant que demande <span class="html">Data</span> de l'application mais qui n'ont pas encore été envoyés via le service de réseau. Si l'application est plus rapide que la couche réseau, le tampon devient plein et le système d'exploitation suspend l'application pour permettre à l'entité de transport de vider sa file d'attente de transmission. L'entité de transport utilise également un tampon pour stocker les segments reçus de la couche réseau qui n'ont pas encore été traités par l'application. Si l'application est lente à traiter les données, ce tampon devient plein et l'entité de transport n'est plus capable d'accepter les segments de la couche réseau. Les tampons de l'entité de transport ont une taille limitée et s'ils débordent, l'entité de transport est obligée de jeter les segments reçus. Au niveau de la couche application, la plupart des serveurs sont implémentés sous forme de processus. En revanche, la couche réseau et la couche de transport sont généralement implémentées à l'intérieur du système d'exploitation et la quantité de mémoire qu'elles peuvent utiliser est limitée par la quantité de mémoire allouée à l'ensemble du noyau.</p>
                    <p>Pour résoudre ce problème, notre protocole de transport doit inclure une mécanisme de feedback qui permet au récepteur d'informer l'émetteur qu'il a traité un segment et qu'un autre peut être envoyé. Ce feedback est nécessaire même si la couche réseau fournit un service parfait. Pour inclure un tel feedback, notre protocole de transport doit traiter deux types de segments :</p>
                    <ul>
                        <li>
                            <p>des segments de données transportant une SDU.</p>
                        </li>
                        <li>
                            <p>des segments de contrôle transportant un accusé de réception indiquant que le segment précédent a été correctement traité.</p>
                        </li>
                    </ul>
                    <p>Ces deux types de segments peuvent être distingués à l'aide d'un segment composé de deux parties :</p>
                    <ul>
                        <li>
                            <p>l'en-tête qui contient un bit mis à <span class="html">0</span> dans les segments de données et mis à <span class="html">1</span> dans les segments de contrôle.</p>
                        </li>
                        <li>
                            <p>la charge utile qui contient la SDU fournie par l'application utilisateur.</p>
                        </li>
                    </ul>
                    <p>L'entité de transport peut avoir être modélisée comme une machine à états finis, contenant deux états pour le récepteur et deux états pour l'émetteur. La figure ci-dessous fournit une représentation graphique de cette machine à états avec l'émetteur au-dessus et le récepteur en dessous.</p>
                    <figure>
                        <img src="../images/machine_etats_finis_protocole_transport_simple.jpg" alt="">
                        <figcaption>Figure 4.3 : Machine à états finis du protocole de transport le plus simple</figcaption>
                    </figure>
                    <p>La machine à états finis ci-dessus montre que l'émetteur doit attendre un accusé de réception du récepteur avant de pouvoir transmettre la prochaine unité de données de service (SDU). La figure ci-dessous illustre l'échange de quelques segments entre deux hôtes.</p>
                    <h4>4.1.2 Transfert fiable de données sur un service réseau imparfait :</h4>
                    <p>La couche de transport doit gérer les imperfections du service de la couche réseau. Il existe trois types d'imperfections qui doivent être pris en compte par la couche de transport :</p>
                    <ol>
                        <li>
                            <p>Les segments peuvent être corrompus par des erreurs de transmission.</p>
                        </li>
                        <li>
                            <p>Les segments peuvent être perdus.</p>
                        </li>
                        <li>
                            <p>Les segments peuvent être réordonnés ou dupliqués.</p>
                        </li>
                    </ol>
                    <figure>
                        <img src="../images/diagramme_sequence_temporelle_fonctionnement_protocole_transport_simple.PNG" alt="">
                        <figcaption>Figure 4.4 : Diagramme de séquence temporelle illustrant le fonctionnemnt du protocole de transport le plus simple</figcaption>
                    </figure>
                    <p>Pour faire face à ces types d'impections, les protocoles de transport reposent sur différents types de mécanismes. Le premier problème est celui des erreurs de transmission. Les segments envoyés par une entité de transport sont traités par les couches réseau et liaison de données, puis finalement transmis par la couche physique. Toutes ces couches sont imparfaites. Par exemple, la couche physique peut être affectée par différents types d'erreurs :</p>
                    <ul>
                        <li>
                            <p>des erreurs isolées aléatoires où la valeur d'un seul bit a été modifiée en raison d'une erreur de transmission.</p>
                        </li>
                        <li>
                            <p>des erreurs aléatoires en rafale où les valeurs de n bits consécutifs ont été modifiées en raison d'erreurs de transmission.</p>
                        </li>
                        <li>
                            <p>des créations aléatoires de bits et des suppressions aléatoires de bits où des bits ont été ajoutés ou supprimés en raison d'erreurs de transmission.</p>
                        </li>
                    </ul>
                    <p>La seule solution pour se protéger contre les erreurs de transmission est d'ajouter de la redondance aux segments envoyés. La <span class="em">théorie de l'information</span> définit deux mécanismes qui peuvent être utilisés pour transmettre des informations sur un canal de transmission affecté par des erreurs aléatoires. Ces deux mécanismes ajoutent de la redondance aux informations envoyées, pour permettre au récepteur de détecter ou parfois même de corriger les erreurs de transmission. Une discussion détaillée de ces mécanismes dépasse le cadre de ce chapitre, mais il est utile de considérer un mécanisme simple pour comprendre son fonctionnement et ses limites.</p>
                    <p>La <span class="em">théorie de l'information</span> définit des <span class="em">schémas de codage</span>. Il existe différents types de schémas de code qui fonctionnent sur des chaînes sur des chaînes binaires. Un schéma de codage est une fonction qui fait correspondre des informations encodées sous forme d'une chaîne de <span class="em">m</span> bits à une chaîne de <span class="em">n</span> bits. Le schéma de codage est le codage de parité pair. Ce schéma de codage prend une chaôine source de <span class="em">m</span> bits et produit une chaîne codée de <span class="em">m+1</span> bits où les premiers <span class="em">m</span> bits de la chaîne codée sont les bits de la chaîne source et le dernier bit de la chaine codée est choisi de soorte que la chaîne source et le dernier bit de la chaîne codée est choisi de sorte que la chaîne codée contienne toujours un nombre pair de bits égaux à <span class="em">1</span>. Par exemple :</p>
                    <ul>
                        <li>
                            <p><span class="em">1001</span> est encodé en <span class="em">10010</span></p>
                        </li>
                        <li>
                            <p><span class="em">1101</span> est encodé en <span class="em">11011</span></p>
                        </li>
                    </ul>
                    <p>Ce schéma de parité a été utilisé dans certaines RAM ainsi que pour encoder des caractères envoyées sur une ligne série. Il est facile de montrer que ce schéma de codage permet au récepteur de détecter une seule erreur de transmission, mais il ne peut pas la corriger. Cependant, si deux bits ou plus sont en erreur, le récepteur ne pourra pas toujours détecter l'erreur.</p>
                    <p>Certains schémas de codage permettent au récepteur de corriger certaines erreurs de transmission. Par exemple, considérons le schéma de codage qui code chaque bit source comme suit :</p>
                    <ul>
                        <li>
                            <p><span class="em">1</span> est encodé en <span class="em">111</span></p>
                        </li>
                        <li>
                            <p><span class="em">0</span> est encodé en <span class="em">000</span></p>
                        </li>
                    </ul>
                    <p>Par exemple, considérons un émetteur qui envoie <span class="em">111</span>. S'il y a une erreur d'un bit, le récepteur pourrait recevoir <span class="em">011</span> ou <span class="em">101</span> ou <span class="em">101</span>. Dans ces trois cas, le récepteur décodera le modèle de bits reçu comme un <span class="em">1</span> car il contient une majorité de bits égaux à <span class="em">1</span>. S'il y a deux bits en erreur, le récepteur ne pourra plus récupérer l'erreur de transmission.</p>
                    <p>Ce système de codage simple oblige l'émetteur à transmettre trois bits pour chaque bit source. Cependant, il permet au récepteur de corriger les erreurs d'un seul bit. Des systèmes de codage plus avancés qui permettent de récupérer les erreurs sont utilisés dans plusieurs types de couches physiques.</p>
                    <p>Les protocoles de transport utilisent des schémas de détection d'erreurs, mais aucun des protocoles de transport largement utilisés ne repose sur des schémas de correction d'erreurs. Pour détecter les erreurs, un segment est généralement divisé en deux parties :</p>
                    <ul>
                        <li>
                            <p>Un <span class="em">en-tête (header)</span> qui contient les champs utilisés par le protocole de transport pour assurer une livraison fiable. L'en-tête contient une somme de contrôle (checksum) ou un Cyclical Redundancy Check (CRC) [Wiliams1993] qui est utilisé pour détecter les erreurs de transmission.</p>
                        </li>
                        <li>
                            <p>Une <span class="html">charge utile (payload)</span> qui contient les données utilisateur transmises par la couche application.</p>
                        </li>
                    </ul>
                    <p>Certains en-têtes de segment incluent également une <span class="em">longueur</span>, qui indique la longueur totale du segment ou la longueur du payload.</p>
                    <p>Le schéma de détection d'erreurs le plus simple est le checksum. Un checksum est essentiellement une somme arithmétique de tous les octets dont est composé un segment. Il existe différents types de checksums. Par exemple, un checksum de huit bits peut être calculé comme la somme arithmétique de tous les octets (à la fois de l'en-tête et de la partie finale) du segment. Le checksum est calculé par l'émetteur avant l'envoi du segment et le récepteur vérifie le checksum à la réception de chaque segment. Le récepteur rejette les segments reçus avec un checksum invalide. Les checksums peuvent être facilement implémentés en logiciel, mais leurs capacités de détection d'erreurs sont limitées. Les Cyclical Redundancy Checks (CRC) ont de meilleures capacités de détection d'erreurs [SGP98], mais nécessitent plus d eCPU lorsqu'ils sont implémentés en logiciel.</p>
                    <hr>
                    <p>Note : Checksums, CRCs, ... :</p>
                    <p>La plupart des protocoles de la suite de protocoles TCP/IP se basent sur le simple checksum Internet pour vérifier que le segment reçu n'a pas été affecté par des erreurs de transmission. Malgré sa popularité et sa facilité d'implémentation, le checksum Internet n'est pas le seul mécanisme de checksum disponible. Les Cyclical Redundancy Checks (CRC) sont des schémas de détection d'erreurs très puissnats qui sont utilisés notamment sur les disques, par de nombreux protocoles de couche liaison de données et des formats de fichiers tels que zip ou png. Ils peuvent être facilement implémentés de manière efficace en matériel et ont de meilleures capacités de détection d'erreurs que le checksum Internet [SGP98]. Cependant, lorsque les premiers protocoles de transport ont été conçus, les CRC étaient considérés comme trop coûteux en termes de CPU pour les implémentations en logiciel et d'autres mécanismes de checksum ont été utilisés à la place. La communauté TCP/IP a choisi le checksum Internet, la communauté OSI a choisi le checksum Fletcher [Sklower89]. Maintenant, il existe des techniques efficaces pour calculer rapidement les CRC en logiciel [Feldmeier95]. Le protocole SCTP avait initialement choisi le checksum Adler-32 mais l'a récemment remplacé par un CRC (voir RFC 3309).</p>
                    <hr>
                    <p>La deuxième imperfection de la couche réseau est que des segments peuvent être perdus. Comme nous le verrons plus tard, la principale cause de la perte de paquets dans la couche réseau est le manque de tampons dans les routeurs intermédiaires. Comme le récepteur envoie un segment d'accusé de réception après avoir reçu chaque segment de données, la solution la plus simple pour traiter les pertes est d'utiliser un timer de retransmission. Lorsque l'expéditeur envoie un segment, il lancce un time de retransmission. La valeur de ce timer de retransmission doit être supérieure au <span class="html">temps de trajet aller-retour (round-trip-time)</span>, c'est-à-dire le délai entre la transmmission d'un segment de données et la réception de l'accusé de réception correspondnat. Lorsque le timer de retransmission expire, l'expéditeur suppose que le segment de données a été perdu et le retransmet. Cela est illustré dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/utilisation_timers_retransmission_recuperer_segments_perdus.PNG" alt="">
                        <figcaption>Figure 4.5 : Utilisation de timers de retransmission pour récupérer des segments perdus.</figcaption>
                    </figure>
                    <p>Malheureusement, les timers de retransmission seules ne sont pas suffisantes pour récupérer les pertes de segment. Prenons, par exemple, la situation ci-dessous où un accessé de réception est perdu. Dans ce cas, l'expéditeur retransmet le segment de données qui n'a pas été accusé de réception. Malheureusement, comme illustré dans la figure ci-dessous, le récepteur considère la retransmission comme un nouveau segment dont la charge utile doit être livrée à son utilisateur.</p>
                    <figure>
                        <img src="../images/limitations_timers_retransmission.PNG" alt="">
                        <figcaption>Figure 4.6 : Limitations des timers de retransmission</figcaption>
                    </figure>
                    <p>Pour résoudre ce problème, les protocoles de transport associent un <span class="html">numéro de séquence</span> à chaque segment de données. Ce <span class="em">numéro de séquence</span> est l'un des champs trouvés dans l'en-tête des segments de données. Nous utilisons la notation <span class="html">D(S,...)</span> pour indiquer un segment de données dont le numéro de séquence est réglé sur <span class="em">S</span>. Le numéro de séquence est encodé sous forme de chaîne de bits de longueur fixe. Le protocole de transport le plus simple est l'<span class="html">Alternating Bit Protocol (ABP)</span>.</p>
                    <p>L'Alternatif Bit Protocol utilise un seul bit pour coder le numéro de séquence. Il peut être facilement implémenté. L'expéditeur et le récepteur ne nécessitent qu'une machine à états finis à quatre états.</p>
                    <p>L'état initial de l'expéditeur est <span class="html">Wait for D(0,...)</span>. Dans cet état, l'expéditeur attend une <span class="em">Data.request</span>. Le premier segment de données qu'il envoie utilise le numéro de séquence <span class="em">0</span>. Après avoir envoyé ce segment, l'expéditeur attend un accessé de réception <span class="em">OK0</span>. Un segment est retransmis à l'expiration du timer de retransmission ou si un accusé de réception avec un numéro de séquence incorrect a été reçu.</p>
                    <p>Le récepteur attend d'abord <span class="em">D(0,...)</span>. Si le segment contient un <span class="em">CRC</span> correct, il passe l'unité de données de service (SDU) à son utilisateur et envoie <span class="em">OK0</span>. Si le segment contient un CRC invalide, il est immédiatement rejeté. Ensuite, le récepteur attend <span class="em">D(1,...)</span>. Dans cet état, il peut recevoir un <span class="em">D(0,...)</span> en double ou un segment de données avec un CRC invalide. Dans les deux cas, il renvoie un segment <span class="em">OK0</span> pour permettre à l'émetteur de récupérer d'une éventuelle perte du précédent segment <span class="em">OK0</span>.</p>
                    <hr>
                    <p>Note : Gérer les segments corrompus :</p>
                    <p>Le FSM (Finite State Machine) du récepteur de l'Alternating Bit Protocol rejette tous les segments qui contiennent un CRC invalide. C'est l'approche la plus sûre car le segment reçu peut être complétement différent du segment envoyé par l'hôte distant. Un récepteur ne devrait pas tenter d'extraire des infomrations d'un segment corrompu car il ne peut pas savoir quelle partie du segment a été affectée par l'erreur.</p>
                    <hr>
                    <p>La figure ci-dessous illustre le fonctionnement du protocole alternatif de bit (ABP).</p>
                    <figure>
                        <img src="../images/alternating_bit_protocol_FSM_emetteur.PNG" alt="">
                        <figcaption>Figure 4.7 : Alternating Bit Protocol : FSM de l'émetteur</figcaption>
                    </figure>
                    <p>L'alternating Bit Protocol peut récupérer des erreurs de transmission et des pertes de segments. Cependant, il a un inconvénient important. Considérons deux hôtes qui sont directement connectés par une liaison satellite de 50 Kbits/sec ayant un délai de propagation de 250 millisecondes. Si ces hôtes envoient des segments de 1000 bits, alors le débit maximum pouvant être atteint par l'Alternating Bit Protocol est d'un segment toutes les 20 + 250 + 250 = 520 millisecondes si nous ignorons le temps de transmission de l'accusé de réception. Ceci est inférieur à 2 Kbits/sec !</p>
                    <figure>
                        <img src="../images/alternating_bit_protocol_FSM_recepteur.PNG" alt="">
                        <figcaption>Figure 4.8 : Alternating Bit Protocol : FSM du récepteur</figcaption>
                    </figure>
                    <h4>Go-back-n et selective repeat :</h4>
                    <p>Pour surmonter les limitations de performance de l'Alternating Bit Protocol, les protocoles de transport se reposent sur le <span class="html">pipelining</span>. Cette technique permet à un émetteur de transmettre plusieurs segments consécutifs sans être obligé d'attendre un accusé de réception après chaque segment. Chaque segment de données contient un numéro de séquence encodé dans un champ de <span class="em">n</span> bits.</p>
                    <figure>
                        <img src="../images/fonctionnement_alternating_bit_protocol.PNG" alt="">
                        <figcaption>Figure 4.9 : Fonctionnement de l'Alternating Bit Protocol</figcaption>
                    </figure>
                    <p>Le <span class="em">ppelining</span> permet à l'émetteur de transmettre des segments à un début plus élevé, mais il est nécessaire de s'assurer que le destinataire ne devienne pas surchargé. Sinon, les segments envoyés par l'émetteur ne sont pas correctement reçus par la destination. Les protocoles de transport qui se reposent sur le pipelining  permettent à l'émetteur de transmettre <span class="html">W</span> sgments non accusés avant d'être obligé d'attendre un accusé de réception de l'entité réceptrice.</p>
                    <figure>
                        <img src="../images/pipelining_ameliorer_performances_protocoles_transport.jpg" alt="">
                        <figcaption>Figure 4.10 : Utilisation du pipelining pour améliorer les performances de protocoles de transport</figcaption>
                    </figure>
                    <p>Cela est implémenté en utilisant une <span class="html">fenêtre glissante (sliding window)</span>. La sliding window est l'ensemble des numéros de séquence consécutifs que l'émetteur peut utiliser lorsqu'il transmet des segments sans être obligé d'attendre un accusé de réception. La figure ci-dessous montre une sliding window contenant cinq segments (<span class="em">6</span>, <span class="em">7</span>, <span class="em">8</span>, <span class="em">9</span> et <span class="em">10</span>). Deux de ces numéros de séquence (<span class="em">6</span> et <span class="em">7</span>) ont été utilisés pour envoyer des segments et seuls trois numéros de séquence (<span class="em">8</span>, <span class="em">9</span> et <span class="em">10</span>) restent dans la sliding window. La sliding window est dite fermée une fois que tous les numéros de séquence contenus dans la sliding window ont été utilisés.</p>
                    <figure>
                        <img src="../images/sliding_window.png" alt="">
                        <figcaption>Figure 4.11 : La sliding window</figcaption>
                    </figure>
                    <p>La figure ci-dessous illustre le fonctionnement de la sliding window. La sliding window affichée contient trois segments. L'émetteur peut donc transmettre trois segments avant d'être contraint d'attendre un accusé de réception. La sliding window se déplace vers les numéros de séquence supérieurs lors de la réception des accusés de réception. Lorsque le premier accusé de réception (<span class="em">OK0</span>) est reçu, il permet à l'émetteur de déplacer sa sliding window vers la droite et le numéro de séquence <span class="em">3</span> devient disponible. Ce numéro de séquence est utilisé plus tard pour transmettre la SDU <span class="em">d</span>.</p>
                    <figure>
                        <img src="../images/exemple_sliding_window.jpg" alt="">
                        <figcaption>Figure 4.12 : Exemple de sliding window</figcaption>
                    </figure>
                    <p>En pratique, étant donné que l'en-tête de segment code le numéro de séquence sous forme de chaîne de <span class="em">n</span> bits, seuls les numéros de séquence compris entre 0 et 2<sup>n</sup>-1 peuvent être utilisés. Cela implique que le même numéro de séquence est utilisé pour différents segments et que la sliding window va s'enrouler. Cela est illustré dans la figure ci-dessous en supposant que 2 bits sont utilisés pour coder numéro de séquence dans l'en-tête de segment. Notez qu'à la réception de <span class="em">OK1</span>, l'expéditeur fait glisser sa fenêtre et peut réutiliser le numéro de séquence <span class="em">0</span>.</p>
                    <figure>
                        <img src="../images/utilisation_sliding_window_avec_arithmetique_modulo.jpg" alt="">
                        <figcaption>Figure 4.13 : Utilisation de la sliding window avec l'arithmétique modulo</figcaption>
                    </figure>
                    <p>Malheureusement, les pertes de segments ne disparaissent pas simplement parce qu'un protocole de transport utilise une sliding window. Pour récupérer des pertes de segments, un protocole de sliding window doit définir :</p>
                    <ul>
                        <li>
                            <p>une heureustique pour détecter les pertes de segments.</p>
                            <li>
                                <p>une <span class="html">stratégie de retransmission</span> pour retransmettre les segments perdus.</p>
                            </li>
                        </li>
                    </ul>
                    <p>Le protocole de sliding window le plus simple utilise la récupération <span class="html">Go-Back-N</span>. De manière intuitive, <span class="em">Go-Back-N</span> fonctionne comme suit. Un récepteur <span class="em">Go-Back-N</span> est le plus simple possible. Il n'accepte que les segments qui arrivent dans l'ordre. Lorsque <span class="em">Go-Back-N</span> reçoit un segment de données, il renvoie toujours un accusé de réception contenant le numéro de séquence du dernier segment in-sequence qu'il a reçu. Cet accusé de reception est dit <span class="html">cumulatif</span>. Lorsqu'un récepteur <span class="em">Go-Back-N</span> envoie un accusé de réception pour le numéro de séquence <span class="html">x</span>, il reconnaît implicitement la réception de tous les segments dont le numéro de séquence est antérieur à <span class="em">x</span>. Un avantage clé de ces accusés de réception cumulatifs est qu'il est facile de récupérer la perte d'un accusé de réception. Par exemple, considérez un récepteur <span class="em">Go-Back-N</span> qui a reçu les segments <span class="em">1</span>, <span class="em">2</span> et <span class="em">3</span>. Il a envoyé <span class="em">OK1</span>, <span class="em">OK2</span> et <span class="em">OK3</span>. Malheureusement, <span class="em">OK1</span> et <span class="em">OK2</span> ont été perdus. Grâce aux accusés de réception cumultatifs, lorsque le récepteur reçoit <span class="em">OK3</span>, il sait que les trois segments ont été correctement reçus.</p>
                    <p>La figure ci-dessous montre le FSM d'un simple récepteur <span class="em">Go-Back-N</span>. Ce récepteur utilise deux variables : <span class="html">lastack</span> et <span class="html">next</span>. <span class="em">next</span> est le prochain numéro de séquence attendu et <span class="em">lastack</span> est le numéro de séquence du dernier segment de données qui a été accusé de réception. Le récepteur n'accepte que les segments reçus dans l'ordre. <span class="html">maxseq</span> est le nombre de différents numéros de séquence (2<sup>n</sup>).</p>
                    <figure>
                        <img src="../images/go-back-n_recepteur_FSM.PNG" alt="">
                        <figcaption>Figure 4.14 : Go-Back-N : FSM du récepteur</figcaption>
                    </figure>
                    <p>Un expéditeur <span class="em">Go-Back-N</span> est également très simple. Il utilise un tampon d'envoi qui peut stocker une sliding window entière de segments. La taille de la sliding window peut être fixe pour un protocole donné ou négociée pendant la phase d'établissement de la connexion. Nous verrons plus tard qu'il est également possible de modifier la taille de la sliding window durant la durée de vie de la connexion. Les segments sont envoyés avec un numéro de séquence croissant (modulo <span class="em">maxseq</span>). L'expéditeur doit attendre un accusé de réception une fois que son tampon d'envoi est plein. Lorsqu'un expéditeur <span class="em">Go-Back-N</span> reçoit un accusé de réception, il supprime du tampon d'envoi tous les segments accusés de réception et utilise un timer de retransmission pour détecter les pertes de segments. Un simple expéditeur <span class="em">Go-Back-N</span> maintient un timer de retransmission par connexion. Ce timer est lancé lorsque le premier segment est envoyé. Lorsque l'expéditeur <span class="em">Go-Back-N</span> reçoit un accysé de réception, il redémarre le timer de retransmission uniquement s'il reste des segments non accusés de réception dans son tampon d'envoi. Lorsque le time de retransmission expire, l'expéditeur <span class="em">Go-Back-N</span> suppose que tous les segments non accusés de réception actuellement stockés dans son tampon d'envoi ont été perdus. Il retransmet donc tous les segments non accusés de réception du tampon et redémarre son timer de retransmission.</p>
                    <figure>
                        <img src="../images/go-back-n_emetteur_FSM.PNG" alt="">
                        <figcaption>Figure 4.15 : Go-Back-N : FSM de l'émetteur</figcaption>
                    </figure>
                    <p>L'opération de <span class="em">Go-Back-N</span> est illustrée dans la figure ci-dessous. Dans cette figure, notez qu'à la réception du segment hors séquence <span class="html">D(2,c)</span>, le récepteur renvoie un accusé de réception cumulatif <span class="em">C(OK,0)</span> qui reconnaît tous les segments qui ont été reçus dans l'ordre. Le segment perdu est retransmis à l'expiration du timer de retransmission.</p>
                    <figure>
                        <img src="../images/go-back-n_exemple.PNG" alt="">
                        <figcaption>Figure 4.16 : Go-Back-N : exemple</figcaption>
                    </figure>
                    <p>Le principal avantage de <span class="em">Go-Back-N</span> est qu'il peut être facilement implémenté et qu'il peut également offrir de bonnes performances lorsque seuls quelques segments sont perdus. Cependant, lorsque de nombreuses pertes surviennent, les performances de <span class="em">Go-Back-N</span> diminuent rapidement pour deux raisons :</p>
                    <ul>
                        <li>
                            <p>le récepteur <span class="em">Go-Back-N</span> n'accepte pas les segments hors séquence.</p>
                        </li>
                        <li>
                            <p>l'émetteur <span class="em">Go-Back-N retransmet tous les segments non reconnus une fois qu'il a détecté une perte.</span></p>
                        </li>
                    </ul>
                    <p>La <span class="html">Selective repeat</span> est une meilleure stratégie pour récupérer les pertes de segments. Intuitivement, la <span class="em">selective repeat</span> permet au récepteur d'accepter les segments hors séquence. De plus, lorsqu'un émetteur de <span class="em">selective repeat</span> détecte des pertes, il ne retransmet que les segments qui ont été perdus et non les segments qui ont été correctement reçus.</p>
                    <p>Un récepteur de <span class="em">selective repeat</span> maintient une sliding window de <span class="em">W</span> segments et stocke dans un tampon les segments hors séquence qu'il reçoit. La figure ci-dessous montre une fenêtre de réception de cinq segments sur un récepteur qui a déjà reçu les segments <span class="em">7</span> et <span class="em">9</span>.</p>
                    <figure>
                        <img src="../images/fenetre_reception_avec_selective_repeat.jpg" alt="">
                        <figcaption>Figure 4.17 : La fenêtre de réception avec selective repeat</figcaption>
                    </figure>
                    <p>Un récepteur de <span class="em">selective repeat</span> rejette tous les segments ayant un CRC invalide et maintient la variable <span class="em">lastack</span> en tant que numéro de séquence du dernier segment qu'il a reçu. Le récepteur inclut toujours la valeur de <span class="em">lastack</span> dans les accus"s de réception qu'il envoie. Certains protocoles permettent au récepteur de <span class="em">selective repeat</span> d'accuser réception des segments hors séquence qu'il a reçus. Cela peut être fait, par exemple, en plaçant la liste des numéros de séquence des segments reçus correctement mais hors séquence dans les accusés de réception avec la valeur <span class="em">lastack</span>.</p>
                    <p>Lorsqu'un réception de <span class="em">selective repeat</span> reçoit un segment de données, il vérifie d'abord si le segment se trouve à l'intérieur de sa fenêtre de réception. Si c'est le cas, le segment est placé dans le tampon de réception. Sinon, le segment reçu est rejeté et un accusé de réception contenant <span class="em">lastack</span> est envoyé à l'émetteur. Le récepteur supprime ensuite tous les segments consécutifs commençant par <span class="em">lastack</span> (s'il y a) du tampon de réception. Les payloads de ce ces segments sont livrés à l'utilisateur, <span class="em">lastack</span> et la fenêtre de réception sont mis à jour, et un accusé de réception reconnaissant le dernier segment reçu dans la séquence est envoyé.</p>
                    <p>L'émetteur de <span class="em">selective repeat</span> maintient un tampon d'envoi pouvant stocker jusqu'à <span class="em">W</span> segments non acquittés. Ces segments sont envoyés tant que le tampon d'envoi n'est pas plein. Plusieurs implémentations d'un émetteur de <span class="em">selective repeat</span> sont possibles. Une implémentation simple consiste à assicier un timer de retransmission à chaque segment. Le timer est démarré lorsque le segment est envoyé et annulé lors de la réception d'un accusé de réception qui couvre ce segment. Lorsqu'un timer de retransmission expire, le segment correspondant est retransmis et ce timer de retransmission est redémarré. Lorsqu'un accusé de réception est reçu, tous les segments courverts par cet accusé de réception sont supprimés du tampon d'envoi et la sliding window est mise à jour.</p>
                    <p>La figure ci-dessous illustre le fonctionnement de la <span class="em">selective repeat</span> lorsque des segments sont perdus. Dans cette figure, <span class="em">C(OK,x)</span> est utilisé pour indiquer que tous les segments, jusqu'au nuémro de séquence <span class="em">x</span> inclus, ont été reçus correctement.</p>
                    <figure>
                        <img src="../images/selective_repeat_exemple.PNG" alt="">
                        <figcaption>Figure 4.18 : Selective repeat : exemple</figcaption>
                    </figure>
                    <p>Les acquittements cumulatifs purs fonctionnent bien avec la stratégie <span class="em">Go-Back-N</span>. Cependant, avec uniquement des acquittements cumulatifs, un émetteur de <span class="em">selective repeat</span> ne peut pas facilement déterminer quels segments de données ont été correctemnt reçus après la perte d'un segment de données. Par exemple, dans la figure ci-dessus, le deuxième <span class="em">C(OK,0)</span> n'informe pas explicitement l'émetteur de la réception de <span class="em">D(2,c)</span> etl'émetteur pourrait retransmettre ce segment même s'il a déjà été reçu. Une solution possible pour améliorer les performances de la <span class="em">selective repeat</span> consiste à fournir des informations supplémentaires sur les segments reçus dans les acquittements renvoyés par le récepteur. Par exemple, le récepteur pourrait ajouter dans l'acquittement renvoyé la liste des numéros de séquence de tous les segments qui ont déjà été reçus. De tels acquittements sont parfois appelés des acquittements sélectifs. Cela est illustré dans la figure ci-dessous.</p>
                    <p>Dans la figure ci-dessus, lorsque l'émetteur reçoit <span class="html">C(OK,0,[2])</span>, il sait que tous les segments jusqu'à et y compris <span class="em">D(0,...)</span> ont été correctement reçus. Il sait également que le segment <span class="em">D(2,...)</span> a été reçu et peut annuler le timer de retransmission associé à ce segment. Cependant, ce segment ne doit pas être supprimé du tampon d'envoi avant la réception d'un acquittement cumylatif (<span class="em">C(OK,2)</span> dans la figure ci-dessus) qui couvre ce segment.</p>
                    <hr>
                    <p>Note : Taille maximale de la fenêtre avec Go-Back-N et la selective repeat :</p>
                    <p>Un protocole de transport qui utilise <span class="em">n</span> bits pour encoder son numéro de séquence peut envoyer jusqu'à <span class="em">2<sup>n</sup></span> segments différents. Cependant, pour assurer une livraison fiable des segments, <span class="em">Go-Back-N</span> et <span class="em">selective repeat</span> ne peuvent pas utiliser une fenêtre de <span class="em">2<sup>n</sup></span> segments. Considérons d'abord <span class="em">Go-Back-N</span> et supposons qu'un émetteur envoie <span class="em">2<sup>n</sup></span> segments. Ces segments sont reçus dans l'ordre par la destination, mais tous les accusés de réception retournés sont perdus. L'émetteur va alors retransmettre tous les segments et ils seront tous acceptés par le récepteur et livrés une deuxième fois à l'utilisateur. Il est facile de voir que ce problème peut être évité si la taille maximale de la fenêtre d'envoi est de <span class="em">2<sup>n</sup>-1</span> segments. Un problème similaire se produit avec <span class="em">selective repeat</span>. Cependant, comme le récepteur accepte les segments hors séquence, une fenêtre d'envoi de <span class="em">2<sup>n</sup>-1</span> segments n'est pas suffisante pour assurer une livraison fiable de tous les segments. Il peut être facilement démontré que pour éviter ce problème, un émetteur de <span class="em">selective repeat</span> ne peut pas utiliser une fenêtre qui est plus grande que <span class="em">2<sup>n</sup></span> segments.</p>
                    <hr>
                    <p><span class="em">Go-Back-N</span> ou <span class="em">selective repeat</span> sont utilisés par les protocoles de transport pour assurer un transfert de données fiable au-dessus d'un service de couche réseau peu fiable. Jusqu'à présent, nous avons supposé que la taille de la sliding window était fixe pour toute la durée de la connexion. En pratique, une entité de la couche transport est généralement implémentée dans le système d'exploitation et partage la mémoire avec d'autres parties du système. De plus, une entité de la couche transport doit prendre en charge plusieurs connexions de transport simultanément (éventuellement des centaines ou des milliers). Cela implique que la mémoire pouvant être utilisée pour prendre en charge le tampon d'envoi ou de réception d'une connexion de transport peut changer pendant la durée de la connexion. Pour une discussion sur la façon dont le tampon d'envoi peut changer, voir par exemple [SMM1998]. Ainsi, un protocole de transport doit permettre à l'émetteur et au récepteur d'ajuster leurs tailles de fenêtre.</p>
                    <p>Pour résoudre ce problème, les protocoles de transport permettent au destinataire d'annoncer la taille actuelle de sa fenêtre de réception dans tous les accusés de réception qu'il envoie. La fenêtre de réception annoncée par le destinataire limite la taille du tampon d'envoi utilisé par l'émetteur. En pratique, l'émetteur maintient deux variables d'état : <span class="html">swin</span>, la taille de sa fenêtre d'envoi (qui peut être ajustée par le système) et <span class="html">rwin</span>, la taille de la fenêtre de réception annoncée par le destinataire. À tout moment, le nombre de segments non acquittés ne peut pas être supérieur à <span class="html">min(swin,rwin)</span>. Notez que si la fenêtre de réception se réduit, il se peut que l'émetteur ait déjà envoyé un segment qui n'est plus dans sa fenêtre. Ce segment sera rejeté par le destinataire et l'émetteur le retransmettra ultérieurement. L'utilisation de fenêtres dynamiques est illustrée dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/fenetre_reception_dynamique.PNG" alt="">
                        <figcaption>Figure 4.19 : Fenêtre de réception dynamique</figcaption>
                    </figure>
                    <p>Le récepteur peut ajuster sa fenêtre de réception annoncée en fonction de sa consommation de mémoire actuelle, mais aussi pour limiter la bande passante utilisée pour l'émetteur. En pratique, le tampon de réception peut également se réduire car l'application peut ne pas être en mesure de traiter les données reçues assez rapidement. Dans ce cas, le tampon de réception peut être complètement plein et la fenêtre de réception annoncée peut se réduire à <span class="em">0</span>. Lorsque l'émetteur reçoit un accusé de réception avec une fenêtre de réception définie à <span class="em">0</span>, il est bloqué jusqu'à ce qu'il reçoive un accusé de réception avec une fenêtre de réception positive. Malheureusement, comme illustré dans la figure ci-dessous, la perte de cet accusé de réception pourrait causer une impasse car l'émetteur attend un accusé de réception pendant que le récepteur attend un segment de données.</p>
                    <figure>
                        <img src="../images/risque_deadlock_fenetres_dynamiques.jpg" alt="">
                        <figcaption>Figure 4.20 : Risque de deadlock avec fenêtres dynamiques</figcaption>
                    </figure>
                    <p>Pour résoudre ce problème, les protocoles de transport s'appuient sur un timer spécial : le <span class="html">timer de persistance</span>. Ce timer est lancé par l'émetteur chaque fois qu'il reçoit un accusé de réception annonçant une fenêtre de réception définie à <span class="em">0</span>. Lorsque le timer expire, l'émetteur retransmet un ancien segment afin de forcer le récepteur à envoyer un nouvel accusé de réception, et donc envoyer la taille de la fenêtre de réception actuelle.</p>
                    <p>Pour conclure notre description des mécanismes de base que l'on retrouve dans les protocoles de transport, nous devons encore discuter de l'impact des segments arrivant dans le mauvais ordre. Si deux segments consécutifs sont réordonnés, le récepteur s'appuie sur leurs numéros de séquence pour réordonner dans son tampon de réception. Malheureusement, comme les protocoles de transport réutilisent le même numéro de séquence pour différents segments, s'il y a un retard prolongé d'un segment, il peut quand même accepté par le récepteur. Cela est illustré dans la figure ci-dessous où le segment <span class="em">D(1,b)</span> est retardé.</p>
                    <figure>
                        <img src="../images/ambiguites_causees_delais_excessifs.jpg" alt="">
                        <figcaption>Figure 4.21 : Ambiguïtés causées par des délais excessifs</figcaption>
                    </figure>
                    <p>Pour résoudre ce problème, les protocoles de transport combinent deux solutions. Tout d'abord, ils utilisent 32 bits ou plus pour coder le numéro de séquence dans l'en-tête de segment. Cela augmente les frais généraux, mais augmente également le délai entre la transmission de deux segments différents ayant le même numéro de séquence. Deuxièmement, les protocoles de transport exogent que la couche réseau impose une <span class="html">durée de vie maximale du segment (MSL pour Maximum Segment Lifetime)</span>. La couche réseau doit garantir qu'aucun parquet ne reste dans le réseau pendant plus de MSL secondes. Sur Internet, la MSL est supposée être de 2 minutes RFC 793. Comme nous le verrons dans le prochain chapitre, Internet n'applique pas strictement cette MSL. Cependant, il est raisonnable de s'attendre à ce que la plupart des paquets sur Internet ne restent pas dans le réseau pendant plus de 2 minutes. Il existe quelques exceptions à cette règle, telles que RFC 1149, dont l'implémentation est décrite dans <a href="http://www.blug.linux.no/rfc1149/" target="_blank">http://www.blug.linux.no/rfc1149/</a>, mais il y a peu de liens réels prenant en charge RFC 1149 sur Internet. Notez que cela limite la bande passante maximale d'un protocole de transport. S'il utilise <span class="em">n</span> bits pour coder ses numéros de séquence, il ne peut pas envoyer plus de 2<sup>n</sup> segments toutes les MSL secondes.</p>
                    <p>Les protocoles de transport doivent souvent envoyer des données dans les deux sens. Pour réduire les frais généraux causés par les accusés de réception, la plupart des protocoles de transport utilisent le <span class="html">piggybacking</span>. Grâce à cette technique, une entité de transport peut placer à l'intérieur de l'en-tête des segments de données qu'elle envoie, les accusés de réception et la fenêtre de réception qu'elle annonce pour la direction opposée du flux de données. Le principal avantage du <span class="em">piggybacking</span> est de réduire les frais généraux car il n'est pas nécessaire d'envoyer un segment complet pour transporter un accusé de réception. Cela est illustré ci-dessous ou le numéro d'accusé de réception est souligné dans les segments de données. Le <span class="em">piggybacking</span> n'est utilisé que lorsque les données circulent dans les deux sens. Un récepteur générera un accusé de réception pur lorsqu'il ne transmet pas de données dans la direction opposée, comme indiqué en bas de la figure.</p>
                    <figure>
                        <img src="../images/piggybacking.jpg" alt="">
                        <figcaption>Figure 4.22 : Piggybacking</figcaption>
                    </figure>
                    <p>Le dernier point à discuter concernant les mécanismes de transfert de données utilisés par les protocoles de transport est la fourniture d'un service de flux de bytes. Comme indiqué dans le premier chapitre, le service de flux de bytes est largement utilisé dans la couche transport. Les protocoles de transport qui fournissent un service de flux de bytes associent un numéro de séquence à tous les bytes qui sont envoyés et placent le numéro de s"quence du premier byte du segment dans l'en-tête du segment. Cela est illustré dans la figure ci-dessous. Dans cet exemple, l'expéditeur choisit de mettre deux bytes dans chacun des trois premiers segments. C'est dû à des raisons graphiques, un vrai protocole de transport utiliserait des segments plus grands en pratique. Cependant, la division du flux de bytes en segments combinée avec les pertes et les retransmissions expliquent pourquoi le service de flux de bytes ne préserve pas les limites des SDU.</p>
                    <figure>
                        <img src="../images/fourniture_service_flux_bytes.jpg" alt="">
                        <figcaption>Figure 4.23 : Fourniture du service de flux de bytes</figcaption>
                    </figure>
                    <h4>Établissement et libération de la connexion :</h4>
                    <p>Les derniers points à discuter concernant le protocole de transport sont les mécanismes utilisés pour établir et libérer une connexion de transport.</p>
                    <p>Nous avons expliqué dans les premiers chapitres les primitives de service utilisées pour établir une connexion. L'approche la plus simple pour établir une connexion de transport serait de définir deux segments de contrôle spéciaux : <span class="html">CR</span> et <span class="html">CA</span>. Le segment <span class="em">CR</span> est envoyé par l'entité de transport qui souhaite initier une connexion. Si l'entité distante souhaite accepter la connexion, elle répond en envoyant un segment <span class="em">CA</span>. La connexion de transport est considérée comme établie une fois que le segment <span class="em">CA</span> a été reçu et que les segments de données peuvent être envoyés dans les deux directions.</p>
                    <figure>
                        <img src="../images/etablissement_naif_connexion_transport.jpg" alt="">
                        <figcaption>Figure 4.24 : Établissement naïf de connexion de transport</figcaption>
                    </figure>
                    <p>Malheureusement, ce schéma n'est pas suffisant pour plusieurs raisons. Tout d'abord, une entité de transport a généralement besoin de maintenir plusieurs connexions de transport avec des entités distantes. Parfois, différents utilisateurs (c'est-à-dire des processus) s'exécutant au-dessus d'une entité de transport donnée demandent l'établissement de plusieurs connexions de transport vers différents utilisateurs attachés à la même entité de transport distante. Ces différentes connexions de transport doivent être clairement séparées pour garantir que les données d'une connexion ne sont pas transmises aux autres connexions. Cela peut être réalisé en utilisant un identifiant de connexion, choisi par les entités de transport et placé à l'intérieur de cheque segment pour permettre à l'entité qui reçoit un segment d'associer facilement ce segment à une connexion établie.</p>
                    <p>Deuxièmement, comme la couche réseau est imparfaite, le segment <span class="em">CR</span> ou <span class="em">CA</span> peut être perdu, retardé ou subir des erreurs de transmission. Pour résoudre ces problèmes, les segments de contrôle doivent être portégés en utilisant un CRC ou une somme de contrôle pour détecter les erreurs de transmission. De plus, puisque le segment <span class="em">CA</span> accuse réception de la réception du segment <span class="em">CR</span>, le segment <span class="em">CR</span> peut être protégé en utilisant un timer de retransmission.</p>
                    <p>Malheureusement, ce schéma n'est pas suffisant pour garantir la fiabilité du service de transport. Considérons par exemple une connexion de transport de courte durée où un transfert unique mais important (par exemple un cirement bancaire) est envoyé. Une telle connexion de courte durée commence par un segment <span class="em">CR</span> reconnu par un segment <span class="em">CA</span>, puis le segment de données est envoyé, reconnu et la connexion se termine. Malheureusement, le service de la couche réseau étant peu fiable, des retards combinés à des retransmissions peuvent entraîner la situation représentée dans la figure ci-dessous, où un <span class="em">CR</span> retardé et des segments de données d'une connexion précédente sont acceptés par l'entité réceptrice comme des segments valides, et les données correspondantes sont livrées à l'utilisateyr. La duplication des SDU n'est pas acceptable, et le protocole de transport doit résoudre ce problème.</p>
                    <figure>
                        <img src="../images/dupliquer_connexions_transport.jpg" alt="">
                        <figcaption>Figure 4.25 : Dupliquer les connexions de transport ?</figcaption>
                    </figure>
                    <p>Pour éviter ces duplicatas, les protocoles de transport exigent que la couche réseau limite la <span class="em">durée de vie maximale d'un segment (Maximum Segment Lifetime ou MSL)</span>. L'organisation du réseau doit garantir qu'auncun segment ne reste dans le réseau pendant plus de MSL secondes. Sur l'Internet d'aujourd'hui, le MSL est de deux minutes. Pour éviter les connexions de transport duppliquées, les entités de protocoles de transport doivent être capables de distinguer en toute sécurité entre un segment <span class="em">CR</span> en double et un nouveau segment <span class="em">CR</span>, sans forcer chaque entité de transport à se souvenir de toutes les connexions de transport qu'elle a établies dans le passé.</p>
                    <p>Une solution classique pour éviter de se souvenir des connexions de transport précédentes afin de détecter les duplicatas est d'utiliser une horloge à l'intérieur de chaque entité de transport. Cette <span class="html">horloge de transport</span> a les caractéristiques suivantes :</p>
                    <ul>
                        <li>
                            <p>L'<span class="em">horloge de transport</span> est implémenté comme un compteur à <span class="em">k</span> bits et son cycle d'horloge est tel que <span class="em">2<sup>k</sup> * cycles &gt;&gt; MSL</span>. De plus, le compteur d'<span class="em">horloge de transport</span> est incrémenté à chaque cycle d'horloge et après chaque établissement de connexion. Cette horloge est illustrée dans la figure ci-dessous.</p>
                        </li>
                        <li>
                            <p>L'<span class="em">horloge de transport</span> doit continuer à être incrémentée même si l'entité de transport s'arrête ou redémarre.</p>
                        </li>
                    </ul>
                    <figure>
                        <img src="../images/horloge_transport.jpg" alt="">
                        <figcaption>Figure 4.26 : Horloge de transport</figcaption>
                    </figure>
                    <p>Il convient de noter que les horloges de transport n'ont pas besoin d'être synchronisées avec l'horloge en temps réel et ne le sont généralemnt pas. La synchronisation précise des horloges en temps réel est un problème intéressant, mais qui est hors du champ d'application de ce document. Voir [Mills2006] pour une discussion détaillée sur la synchronisation de l'horloge en temps réel.</p>
                    <p>L'<span class="em">horloge de transport</span> est combiné avec un échange de trois segments, appelé le "<span class="html">three way handshake</span>", pour détecter les duplicatas. Ce <span class="em">three way handshake</span> se déroule comme suit :</p>
                    <ol>
                        <li>
                            <p>L'entité de transport initiale envoie un segment <span class="em">CR</span>. Ce segment demande l'établissement d'une connexion de transport. Il contient un identificateur de connexion (non montré dans la figure) et un numéro de séquence (<span class="em">seq=x</span> dans la figure ci-dessous) dont la valeur est extraite de l'<span class="em">horloge de transport</span>. La transmission du segment <span class="em">CR</span> est protégée par un timer de retransmission.</p>
                        </li>
                        <li>
                            <p>L'entité de transport distante traite le segment <span class="em">CR</span> et crée un état pour la tentative de connexion. À ce stade, l'entité distante ne sait pas encore s'il s'agit d'une nouvelle tentative de connexion ou d'un segment en double. Elle renvoie un segment <span class="em">CA</span> qui contient un numéro d'accusé de réception pour confirmer la réception du segment <span class="em">CR</span> (<span class="em">ack=x</span> dans la figure ci-dessous) et un numéro de séquence (<span class="em">seq=y</span> dans la figure ci-dessous) dont la valeur est extraite de son horloge de transport. À ce stade, la connexion n'est pas encore établie.</p>
                        </li>
                        <li>
                            <p>L'entité initiale reçoit le segment <span class="em">CA</span>. Le numéro d'accusé de réception de ce segment confirme que l'entité distante a correctement reçu le segment <span class="em">CA</span>. La connexion de transport est considérée comme établie par l'entité initiale et la numérotation des segments de données commence au numéro de séquence <span class="em">x</span>. Avant d'envoyer des segments de donénes, l'entité initiale doit reconnaître les segments <span class="em">CA</span> reçus en envoyant un autre segment <span class="em">CA</span>.</p>
                        </li>
                        <li>
                            <p>L'entité distante considère que la connexion de transport est établie après avoir reçu le segment qui accuse réception de son segment <span class="em">CA</span>. La numérotation des segments de données envoyés par l'entité distante commence au numéro de séquence <span class="em">y</span>.</p>
                        </li>
                    </ol>
                    <p>Le <span class="em">three way handshake</span> est illustré dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/three-way_handshake.jpg" alt="">
                        <figcaption>Figure 2.27 : Three-way handshake</figcaption>
                    </figure>
                    <p>Grâce au <span class="em">three way handshake</span>, les entités de transport évitent les connexions de transport en double. Cela est illustré par les trois scénarios ci-dessous.</p>
                    <p>Le premier scénario est lorsque l'entité distante reçoit un ancien segment <span class="em">CR</span>. Elle considère ce segment <span class="em">CR</span> comme une tentative d'établissement de connexion et répond en envoyant un segment <span class="em">CA</span>. Cependant, l'hôte initiateur ne peut pas faire correspondre le segment <span class="em">CA</span> reçu avec une tentative de connexion précédente. Il envoie un segment de contrôle (<span class="html">REJECT</span> dans la figure ci-dessous) pour annuler la tentative de connexion fictive. L'entité distante annule la tentative de connexion lors de la réception de ce segment de contrôle.</p>
                    <figure>
                        <img src="../images/three-way_handshake_recuperation_CR_duplique.jpg" alt="">
                        <figcaption>Figure 4.28 : Three way handshake : récupération d'un CR en double</figcaption>
                    </figure>
                <p>Le deuxième scénario est lorsque l'entité initiatrice envoie un segment <span class="em">CR</span> qui n'atteint pas l'entité distante et reçoit un segment <span class="em">CA</span> en double d'une tentative de connexion précédente. Ce segment <span class="em">CA</span> en double ne peut contenir un accusé de réception valide pour le segment <span class="em">CR</span> car le numéro de séquence du segment <span class="em">CR</span> a été extrait de l'horloge de transport de l'entité initiatrice. Le segment <span class="em">CA</span> est donc rejeté et le segment <span class="em">CR</span> est retransmis à l'expiration du timer de retransmission.</p>
                    <figure>
                        <img src="../images/three-way_handshake_recuperation_CR_duplique.jpg" alt="">
                        <figcaption>Figure 4.29 : Three way handshake : récupération d'un CA en double</figcaption>
                    </figure>
                    <p>Le dernier scénario est moins probable, mais il est également important de le considérer. L'entité distante reçoit un acien segment <span class="em">CR</span>. Elle note la tentative de connexion et l'acknowledge en envoyant un segment <span class="em">CA</span>. L'entité initatrice n'a pas de tentative de connexion correspondante et répond en envoyant un <span class="em">REJECT</span>. Malheureusement, ce segment n'arrive jamais à l'entité distante. À la place, l'entité distante reçoit une retransmission d'un ancien segment <span class="em">CA</span> qui contient le même numéro de s&quence que le premier segment <span class="em">CR</span>. Ce segment <span class="em">CA</span> ne peut pas être accepté par l'entité distante comme une confirmation de la connexion de transport car son numéro d'acknowledgement ne peut pas avoir la même valeur que le numéro de séquence du premier segment <span class="em">CA</span>.</p>
                    <figure>
                        <img src="../images/three-way_handshake_recuperation_CR_duplique.jpg" alt="">
                        <figcaption>Figure 4.28 : Three way handshake : récupération de CR et CA en double</figcaption>
                    </figure>
                    <p>Lorsque nous avons discuté du service orienté connexion, nous avons mentionné qu'il existe deux types de libération de connexion : la <span class="html">libération abrupte</span> et la <span class="html">libération gracieuse</span>.</p>
                    <p>La première solution pour livérer une connexion de transport consiste à définir un nouveau segment de contrôle (par exemple, le segment <span class="em">DR</span>) et à considérer que la connexion est libérée une fois que ce segment a été envoyé ou reçu. Cela est illustré dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/liberation_abrupte_connexion.jpg" alt="">
                        <figcaption>Figure 4.31 : Libération abrupte de la connexion</figcaption>
                    </figure>
                    <p>Comme l'entité qui envoie le segment <span class="em">DR</span> ne peut pas savoir si l'autre entité a déjà envoyé toutes ses données sur la connexion, des SDU peuvent être perdus lors d'une libération abrupte de la connexion.</p>
                    <p>La deuxième méthode pour libérer une connexion de transport consiste à libérer indépendamment les deux directions de transfert de données. Une fois qu'un utilisateur du service de transport a envoyé toutes ses SDU, il effectue une <span class="html">DISCONNECT.req</span> pour sa direction de transfert de données. L'entité de transport envoie un segment de contrôle pour demander la libération de la connexion après la livraison de toutes les SDU précédentes à l'utilisateur distant. Cela est généralement fait en plaçant dans le <span class="em">DR</span> le prochain numéro de séquence et en délivrant la <span class="html">DISCONNECT.ind</span> seulement après toutes les <span class="html">DATA.ind</span> précédentes. L'entité distante confirme la réception du segment <span class="em">DR</span> et la libération de la direction de transfert de données correspondante en renvoyant un accusé de réception. Ceci est illustré dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/liberation_connexion_gracieuse.png" alt="">
                        <figcaption>Figure 4.32 : Libération gracieuse de la connexion</figcaption>
                    </figure>
                    <h3>L'User Datagram Protocol (UDP) :</h3>
                    <p>L'<span class="em">User Datagram Protocol (UDP)</span> est défini dans la RFC 768. Il fournit un service de transport non fiable sans connexion au-dessus du service de couche réseau non fiable sans connexion. Les principales caractéristiques du service UDP sont les suivantes :</p>
                    <ul>
                        <li>
                            <p>Le service UDP ne peut pas livrer des SDU de taille supérieure à 65507 octets. Cette limitation est due au fait que la couche réseau (IPv4 et IPv6) ne peut pas transporter de paquets supérieurs à 64 Ko. Comme UDP ne comprend aucun mécanisme de segmentation/réassemblage, il ne peut pas diviser une SDU avant de l'envoyer.</p>
                        </li>
                        <li>
                            <p>Le service UDP ne garantit pas la livraison des SDU (des pertes et des déséquencements peuvent se produire).</p>
                        </li>
                        <li>
                            <p>Le service UDP ne livrera pas une SDU corrompue à la destination.</p>
                        </li>
                    </ul>
                    <p>Comparé au service de couche réseau sans connexion, le principal avantage du service UDP est qu'il permet plusieurs applications s'exécutant sur un hôte d'échanger des SDU avec plusieurs autres applications s'exécutant sur des hôtes distants. Prenons deux hôtes, par exemple un client et un serveur. Le service de couche réseau permet au client d'envoyer des informations au serveur, mais si une application s'exécutant sur le client souhaite contacter une application particulière s'exécutant sur le serveur, un mécanisme d'adressage supplémentaire est requis autre que l'adresse IP qui identifie un hôte, afin de différencier l'application s'exécutant sur un hôte. Cet adressage supplémentaire est fourni par les <span class="em">numéros de port</span>. Lorsqu'un serveur utilisant UDP est activé sur un hôte, ce serveur enregistre un <span class="em">numéro de port</span>. Ce <span class="em">numéro de port</span> sera utilisé par les clients pour contacter le processus serveur via UDP.</p>
                    <p>La figure ci-dessous montre une utilisation typique des numéros de port UDP. Le processus client utilise le numéro de port <span class="em">1234</span> tandis que le processus serveur utilise le numéro de port <span class="em">5678</span>. Lorsque le client envoie une demande, elle est identifiée comme provenant du numéro de port <span class="em">1234</span> sur l'hôte client et destinée au numéro de port <span class="em">5678</span> sur l'hôte serveur. Lorsque le processus serveur répond à cette demande, l'implémentation UDP du serveur enverra la réponse en tant qu'origine du port <span class="em">5678</span> sur l'hôte serveur et destinée au port <span class="em">1234</span> sur l'hôte client.</p>
                    <figure>
                        <img src="../images/utilisation_numeros_port_UDP.PNG" alt="">
                        <figcaption>Figure 4.33 : Utilisation des numéros de port UDP</figcaption>
                    </figure>
                    <p>UDP utilise un seul format de segment illustré dans la figure ci-dessous. L'en-tête UDP contient quatre champs :</p>
                    <ul>
                        <li>
                            <p>Un port source de 16 bits.</p>
                        </li>
                        <li>
                            <p>Un port de destination de 16 bits</p>
                        </li>
                        <li>
                            <p>Un champ de longueur de 16 bits.</p>
                        </li>
                        <li>
                            <p>Un checksum de 16 bits.</p>
                        </li>
                    </ul>
                    <figure>
                        <img src="../images/format_entete_UDP.PNG" alt="">
                        <figcaption>Figure 4.34 : Format de l'en-tête UDP</figcaption>
                    </figure>
                    <p>Comme les numéros de port sont encodés sur un champ de 16 bits, il ne peut y avoir jusqu'à 65535 processus serveurs différents qui sont liés à un port UDP différent en même temps sur un serveur donné. En pratique, cette limite n'est jamais atteinte. Cependant, il convient de noter que la plupart des implémentations divisent la plage des numéros de port UDP autorisés en trois plages différentes :</p>
                    <ul>
                        <li>
                            <p>Les numéros de port privilégiés (1 &lt; port &lt;1024).</p>
                        </li>
                        <li>
                            <p>Les numéros de port enregistrés (officiellemnt 1024 &lt;: port &lt; 49152).</p>
                        </li>
                    </ul>
                    <p>Dans la plupart des variantes Unix, seuls les processus disposant de privilèges d'administrateur système peuvent être liés à des numéros de port inférieurs à <span class="em">1024</span>. Des serveurs bien connus tels que <span class="html">DNS</span>, <span class="html">NTP</span> ou <span class="html">RPC</span> utilisent des numéros de port privilégiés. Lorsqu'un client a besoin d'utiliser UDP, il n'a généralement pas besoin d'un numéro de port spécifique. Dans ce cas, l'implémentation UDP alloue le premier numéro de port disponible dans la plage éphémère. La plage de numéros de port enregistrés doit être utilisée par les serveurs. En théorie, les développeurs de serveurs de réseau devrianet enregistrer leur numéro de port officiellement via l'IANA, mais peu de développeurs le font.</p>
                    <hr>
                    <p>Note : Calcul de la somme de contrôle UDP :</p>
                    <p>La somme de contrôle du segment UDP est calculée sur :</p>
                    <ul>
                        <li>
                            <p>Un pseudo en-tête contenant l'adresse IP source, l'adresse IP de destination et un champ de bits de 32 bits contenant le byte le plus significatif mis à <span class="em">0</span>, le second mis à <span class="em">17</span> et la longueur du segment UDP dans les deux octets inférieurs.</p>
                        </li>
                        <li>
                            <p>L'ensemble du segment UDP, y compris son en-tête.</p>
                        </li>
                    </ul>
                    <p>Ce pseudo en-tête permet au récepteur de détecter les erreurs affectant les adresses source ou destination IP placées dans la couche IP inférieure. Il s'agit d'une violation du principe de couche qui date de l'époque où UDP et IP étaient des éléments d'un seul protocole. Il convient de noter que si l'algorithme de somme de contrôle calcule la valeur '0x0000', alors la valeur 'Oxffff' est transmise. Un segment UDP dont la somme de contrôle est définie sur 'OxOOOO' est un segment pourquel l'émetteur n'a pas calculé de somme de contrôle lors de la transmission. Certains serveurs <span class="em">NFS</span> ont choisi de désactiver les sommes de contrôle UDP pour des raisons de performance, mais cela a causé des problèmes difficiles à diagnostiquer. En pratique, il y a rarement de bonnes raisons de désactiver les sommes de contrôle UDP. Une discussion détaillée de l'implémentation de la somme de contrôle INternet peut être trouvée dans la RFC 1071.</p>
                    <hr>
                    <p>Plusieurs types d'applications dépendent d'UDP. En règle, UDP est utilisé pour les applications où le délai doit être minimisé ou où les pertes peuvent être récupérées par l'application elle-même. Une première catégorie d'applications basées sur UDP comprend des applications où le client envoie une demande courte et s'attend à une réponse rapide de courte. Le <span class="em">DNS</span> est un exemple d'une application UDP souvent utilisée dans les réseaux étendus. Cependant, das les réseaux locaux, de nombreux systèmes distribués reposent sur l'<span class="html">appel de procédure à distance (RPC pour Remote Procedure Call)</span> qui est souvent utilisé sur UDP. Dans les environnements Unix, le <span class="html">système de fichiers réseau (NFS pour Network File System)</span> est construit sur RPC et s'exécute souvent sur UDP. Une deuxième catégorie d'applications basées sur UDP concerne les jeux vidéo interactifs qui doivent échanger fréquemment de petits messages, tels que la position du joueur ou ses actions récentes. Beaucoup de ces jeux utilisent UDP pour minimiser le délai et peuvent récupérer les pertes. Une troisième catégorie d'applications concerne les applications multimédias telles que la voix sur IP interactive ou la vidéo sur IP interactive. Ces applications interactives s'attendent à un délai inférieur à environ 200 millisecondes entre l'émetteur et le récepteur et peuvent récupérer les pertes directement dans l'application.</p>
                    <h3>4.3 Le Transmission Control Protocol (TCP) :</h3>
                    <p>Le <span class="html">Transmission Control Protocol (TCP)</span> a été initialement défini dans la RFC 793. Plusieurs parties du protocole ont été améliorées depuis la publication de la spécification de protocole originale. Une présentation détaillée de tous les documents de normalisation concernant TCP peut être trouvée dans le RFC 4614. Cependant, les bases du protocole restent les mêmes et une implémentation qui ne prend en charge que la RFC 793 devrait inter-opérer avec les implémentations d'aujourd'hui.</p>
                    <p>TCP fournit un service de transport orienté connexion fiable de flux d'octets sur le service de réseau sans connexion et non fiable fourni par IP. TCP est utilisé par un grand nombre d'applications, notamment :</p>
                    <ul>
                        <li>
                            <p>Courrier électronique (<span class="html">SMTP</span>, <span class="html">POP</span>, <span class="html">IMAP</span></p>
                        </li>
                        <li>
                            <p>World Wide Web (<span class="html">HTTP</span>, ...)</p>
                        </li>
                        <li>
                            <p>La plupart des protocoles de transfert de fichiers (<span class="html">ftp</span>, applications de partage de fichiers peer-to-peer, ...</p>
                        </li>
                        <li>
                            <p>Accès informatique à distance : <span class="html">telenet</span>, <span class="html">ssh</span>, <span class="html">X11</span>, <span class="html">VNC</span>, ...</p>
                        </li>
                        <li>
                            <p>Applications multimédias non interactives : flash</p>
                        </li>
                    </ul>
                    <p>Sur l'Internet mondial, la plupart des applications utilisées dans le grand réseau dépendent de TCP. De nombreuses études ont rapporté que TCP était responsable de plus de 90% des données échangées sur l'Internet mondial. Plusieurs chercheurs ont analysé l'utilisation de TCP et UDP dans l'Internet mondial. La plupart de ces études ont été réalisées en collectant tous les paquets transmis sur une liaison donnée pendant une période de quelques heures ou jours, puis en analysant leurs en-têtes pour déduire le protocole de transport utilisé, le type d'application, ... Les études récentes comprennent <a href="http://www.caida.org/research/traffic-analysis/tcpudpratio/" target="_blank">http://www.caida.org/research/traffic-analysis/tcpudpratio/</a>, <a href="https://research.sprintlabs.com/packstat/packetoverview.php" target="_blank">https://research.sprintlabs.com/packstat/packetoverview.php</a> ou <a href="http://www.nanog.org/meetings/nanog43/presentations/Labovitz_internetstats_N43.pdf" target="_blank">http://www.nanog.org/meetings/nanog43/presentations/Labovitz_internetstats_N43.pdf</a>.</p>
                    <p>Pour finir ce service, TCP s'appuie sur un format de segment simple qui est représenté dans la figure ci-dessous. Chaque segment TCP contient un en-tête décrit ci-dessous et, éventuellement, un payload. La longueur par défaut de l'en-tête TCP est de vingt octets, mais certains en-têtes TCP contiennent des options.</p>
                    <figure>
                        <img src="../images/format_entete_TCP.PNG" alt="">
                        <figcaption>Figure 4.35 : Format de l'en-tête TCP</figcaption>
                    </figure>
                    <p>Un en-tête TCP contient les champs suivants :</p>
                    <ul>
                        <li>
                            <p>Les ports source et destination. Les ports source et destination jouent un rôle important dans TCP, car ils permettent l'identification de la connexion à laquelle appartient un segment TCP. Lorsqu'un client ouvre une connexion TCP, il sélectionne généralement un numéro de port TCP éphémère comme port source et contacte le serveur en utilisant le numéro de port du serveur. Tous les segments envoyés par le client sur cette connnexion ont les mêmes ports source et destination. Le serveur envoie des segments qui contiennent comme source (resp. destination) le port de destination (resp. source) des segments envoyés par le client (voir la figure <span class="html">fig-tcpports</span>). Une connexion TCP est toujours identifiée par cinq informations :</p>
                            <ul>
                                <li>
                                    <p>l'adresse IP du client.</p>
                                </li>
                                <li>
                                    <p>l'adresse IP du serveur.</p>
                                </li>
                                <li>
                                    <p>le port choisi par le client.</p>
                                </li>
                                <li>
                                    <p>le port choisi par le serveur.</p>
                                </li>
                                <li>
                                    <p>TCP</p>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <p>Les champs <span class="html">sequence number</span> (32 bits), <span class="html">acknowledgement number</span> (32 bits) et <span class="html">window</span> (16 bits) sont utilisés pour assurer un transfert de données en utilisant un protocole basé sur la fenêtre. Dans un flux de données TCP, chaque octet du flux consomme un numéro de séquence. Leur utilisation sera décrite en détail dans la section <span class="html">TCPReliable</span>.</p>
                        </li>
                        <li>
                            <p>Le <span class="html">Urgent pointer</span> est utilisé pour indiquer que certaines données doivent être considérées comme urgentes dans un flux de données TCP. Cependant, il est rarement utilisé en pratique et ne sera pas décrit ici. Des détails supplémentaires sur l'utilisation de ce pointeur peuvent être trouvés dans les RFC 793, 1122 ou [Stevens1994].</p>
                        </li>
                        <li>
                            <p>Le champ <span class="html">flags</span> contient un ensemble de bits indicateurs qui indiquent comment un segment doit être interprété par l'entité TCP qui le reçoit :</p>
                            <ul>
                                <li>
                                    <p>Le drapeau <span class="html">SYN</span> est utilisé lors de l'établissement de la connexion.</p>
                                </li>
                                <li>
                                    <p>Le drapeau <span class="html">FIN</span> est utilisé lors de la libération de la connexion.</p>
                                </li>
                                <li>
                                    <p>Le <span class="html">RST</span> est utilisé en cas de problèmes ou lorsqu'un segment invalide a été reçu.</p>
                                </li>
                                <li>
                                    <p>Lorsque le drapeau <span class="html">ACK</span> est activé, cela indique que le champ <span class="html">acknowledgment</span> contient un numéro valide. Sinon, le contenu du champ <span class="em">acknowledgment</span> doit être ignoré par le récepteur.</p>
                                </li>
                                <li>
                                    <p>Le drapeau <span class="html">URG</span> est utilisé avec le <span class="html">Urgent pointer</span>.</p>
                                </li>
                                <li>
                                    <p>Le drpeau <span class="html">PSH</span> est utilisé comme notification de l'émetteur pour indiquer au récepteur qu'il doit transmettre toutes les données qu'il a reçues au processus de réception. Cependant, en pratique, les implémentations de TCP ,e permettent pas aux utilisateurs de TCP d'indiquer quand le drapeau <span class="em">PSH</span> doit être activé, et il y a donc peu d'utilisations réelles de ce drapeau.</p>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <p>Le champ <span class="html">somme de contrôle</span> contient la valeur de la somme de contrôle Internet calculée sur l'ensemble du segment TCP et sur un pseudo en-tête comme pour UDP.</p>
                        </li>
                        <li>
                            <p>Le champ <span class="html">Reserved</span> était initialement réservé pour une utilisation future. Il est mainteant utilisé par RFC 3168.</p>
                        </li>
                        <li>
                            <p>Le champ <span class="html">TCP Header Length (THL)</span> ou <span class="html">Data Offset</span> est un champ de quatre bits qui indique la taille de l'en-tête TCP en mots de 32 bits. La taille maximale de l'en-tête TCP est donc de 64 octets.</p>
                        </li>
                        <li>
                            <p>L'<span class="html">Optional header extension</span> est utilisée pour ajouter des informations optionnelles à l'en-tête TCP. Grâce à cette extension d'en-tête, il est possible d'ajouter de nouveaux champs à l'en-tête TCP qui n'étaient pas prévus dans la spécification originale. Cela a permis à TCP d'évoluer depuis le début des années 80. Les détails de l'extension d'en-tête TCP sont expliqués dans les sections <span class="html">TCPOpen</span> et <span class="html">TCPReliable</span>.</p>
                        </li>
                    </ul>
                    <figure>
                        <img src="../images/utilisation_ports_source_destination_TCP.png" alt="">
                        <figcaption>Figure 4.36 : Utilisation des ports source et destination TCP</figcaption>
                    </figure>
                    <p>Le reste de cette section est organisé comme suit. Nous expliquons d'abord l'établissement et la libération d'une connexion TCP, puis nous discutons des mécanismes utilisés par TCP pour fournir un service de flux de données fiable. Nous terminons la section par une discussion sur la congestion du réseau et expliquons les mécanismes que TCP utilise pour éviter l'effondrement de la congestion.</p>
                    <h4>4.3.1 Établisemment d'une connexion TCP :</h4>
                    <p>Une connexion TCP est établie en utilisant un "three way handshake". La phase d'établissement de la connexion utilise le <span class="em">numéro de séquence</span>, le <span class="em">numéro d'acquittement</span> et le drapeau <span class="em">SYN</span>. Lorsqu'une connexion TCP est établie, les deux hôtes communiquants négocient le numéro de séquence initial à utiliser dans les deux sens de la connexion. Pour cela, chaque entité TCP maintient un compteur de 32 bits, qui est censé être incrémenté d'au moins une unité toutes les 4 microsecondes et après chaque établissement de connexion. Ce compteur de 32 bits a été spécifié dans la RFC 793. Un compteur de 32 bits qui est incrémenté toutes les 4 microsecondes se rembobine en environ 4,5 heures. Cette période est beaucoup plus longue que le Maximum Segment Lifetime qui est fixé à 2 minutes sur Internet (RFC 791, RFC 1122). Lorsqu'un hôte client veut ouvrir une connexion TCP avec un hôte serveur, il crée un segment TCP avec :</p>
                    <ul>
                        <li>
                            <p>Le drapeau <span class="em">SYN</span> activé.</p>
                        </li>
                        <li>
                            <p>Le <span class="em">numéro de séquence</span> défini sur la valeur actuelle du compteur de 32 bits de l'entité TCP de l'hôte client.</p>
                        </li>
                    </ul>
                    <p>À la réception de ce segment (qui sest souvent appelé un <span class="html">segment SYN</span>), l'hôte serveur répond avec un segment contenant :</p>
                    <ul>
                        <li>
                            <p>Le drapeau <span class="em">SYN</span> activé.</p>
                        </li>
                        <li>
                            <p>Le <span class="em">numéro de séquence</span> défini sur la valeur actuelle du compteur de 32 bits de l'entité TCP de l'hôte serveur.</p>
                        </li>
                        <li>
                            <p>Le drapeau <span class="em">ACK</span> activé.</p>
                        </li>
                        <li>
                            <p>Le <span class="em">numéro d'acquittement</span> défini sur le <span class="em">numéro de séquence</span> du segment <span class="em">SYN</span> reçu incrémenté de 1 (mod 2<sup>32</sup>). Lorsqu'une entité TCP envoie un segment ayant <span class="em">x+1</span> comme numéro d'acquittement, cela indique qu'elle a reçu toutes les données ayant le numéro de séquence <span class="em">x+1</span>. Comme le drapeau <span class="em">SYN</span> a été activé dans un segment ayant le numéro de séquence <span class="em">x</span>, cela implique que l'activation du drapeau <span class="em">SYN</span> dans un segment consomme un numéro de séquence.</p>
                        </li>
                    </ul>
                    <p>Ce segment est appelé un segment <span class="html">SYN+ACK</span>. L'acquittement confirme au client que le serveur a correctement reçu le segment <span class="em">SYN</span>. Le numéro de séquence du segment <span class="em">SYN+ACK</span> est utilisé par l'hôte serveur pour vérifier que le client a reçu le segment. À la récepion du segment <span class="em">SYN+ACK</span>, l'hôte client répond avec un segment contenant :</p>
                    <ul>
                        <li>
                            <p>Le drapeau <span class="em">ACK</span> activé.</p>
                        </li>
                        <li>
                            <p>Le <span class="em">numéro d'acquittement</span> défini sur le <span class="em">numéro de séquence</span> du segment <span class="em">SYN+ACK</span> reçu incréme,yé de 1 (mod 2<sup>32</sup>).</p>
                        </li>
                    </ul>
                    <p>À ce stade, la connexion TCP est ouverte et tant le client que le serveur sont autorisés à envoyer des segments TCP contenant des données. Ceci est illsutré dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/etablissement_connexion_TCP.jpg" alt="">
                        <figcaption>Figure 4.37 : Établissement d'une connexion TCP</figcaption>
                    </figure>
                    <p>Dans la figure ci-dessus, la connexion est considérée comme établie par le client une fois qu'il a reçu le segment <span class="em">SYN+ACK</span>, tandis que le serveur considère que la connexion est établie à la réception du segment <span class="em">ACK</span>. Le premier segment de données envoyé par le client (serveur) a son <span class="em">numéro de séquence</span> mis à <span class="em">x+1</span> (resp. <span class="em">y+1</span>).</p>
                    <hr>
                    <p>Note : Calcul de l'ISN (numéro de séquence initial) de TCP :</p>
                    <p>Dans la spécification TCP originale RFC 793, chaque entité TCP maintenait une horloge pour calculer le numéro de séquence initial (<span class="em">ISN</span>) placé dans les segments <span class="em">SYN</span> et <span class="em">SYN+ACK</span>. Cela rendait l'ISN prévisible et posait un problème de sécurité. Le problème de sécurité typique était le suivant : considérons un serveur qui fait confiance à un hôte en fonction de son adresse IP et permet à l'administrateur système de se connecter à partir de cet hôte sans fournir de mot de passe. Considérons maintenant un attaquant qui connaît cette configuration particulière et est capable d'envoyer des paquets IP ayant l'adresse du client comme source. Il peut envoyer de faux segments TCP au serveur, mais ne reçoit pas les réponses du serveur. S'il peut prédire l'ISN choisi par le serveur, il peut envoyer un faux segment <span class="em">SYN</span> et peu après le faux segment <span class="em">ACK</span> confirmant la réception du segment <span class="em">SYN+ACK</span> envoyé par le serveur. Une fois que la connexion TCP est ouverte, il peut l'utiliser pour envoyer n'importe quelle commande au serveur. Pour contrer cette attaque, les implémentations TCP actuelles ajoutent de l'aléatoire à l'ISN. L'une des solutions, proposée dans la RFC 1948, consiste à calculer l'ISN comme suit : <span class="html">ISN = M + H(localhost, localport, remotehost, remoteport, secret)</span> où <span class="em">M</span> est la valeur actuelle de l'horloge TCP et <span class="em">H</span> est une fonction de hachage cyptographique. "<span class="em">localhost</span>" et "<span class="em">remotehost</span>" (resp. "<span class="em">localport</span>" et "<span class="em">remoteport</span>") sont les adresses IP (numéros de port) de l'hôte local et distant et "<span class="em">secret</span>" est un nombre aléatoire connu uniquement par le serveur. Cette méthode permet au serveur d'utiliser des ISN différents pour différents clients en même temps. Les mesures effectuées avec les premières implémentations de cette technique ont montré qu'il était difficile de l'implémenter correctement, mais les implémentations TCP d'aujoud'hui génèrent de bons ISN.</p>
                    <hr>
                    <p>Un serveur pourrait bien sûr refuser d'ouvrir une connexion TCP lors de la réception d'un segment <span class="em">SYN</span>. Ce refus peut être dû à diverses raisons. Il peut ne pas y avoir de processus serveur écoutant sur le port de destination du segment <span class="em">SYN</span>. Le serveur pourrait toujours refuser les établissements de connexion de ce client particulier (par exemple, pour des raisons de sécurité) ou le serveur peut ne pas avoir suffisamment de ressources pour accepter une nouvelle connexion TCP à ce moment-là. Dans ce cas, le serveur répondrait avec un segment TCP avant son drapeau <span class="em">RST (Reset)</span> positionné et contenant le numéro de séquence du segment <span class="em">SYN</span> reçu comme numéro d'acquittement. Cela est illustré dans la figure ci-dessous. Nous discutons des autres utilisations du drapeau <span class="em">RST</span> de TCP plus tard (voir <span class="em">TCPRelease</span>).</p>
                    <figure>
                        <img src="../images/etablissement_connexion_TCP_rejete_par_pair.jpg" alt="">
                        <figcaption>Figure 4.38 : Établissement de connexion TCP rejeté par pair</figcaption>
                    </figure>
                    <p>L'établissement d'une connexion TCP peut être décrit comme une machine à états finis à quatre états, comme illustré ci-dessous. Dans cette FSM, <span class="em">!X</span> (resp. <span class="em">?Y</span>) indique la transmission du segment <span class="em">X</span> (resp. la réception du segment <span class="em">Y</span>) pendant la transition correspondante.</p>
                    <figure>
                        <img src="../images/TCP_FSM_etablissement_connexion.jpg" alt="">
                        <figcaption>Figure 4.39 : FSM TCP pour l'établissement de la connexion</figcaption>
                    </figure>
                    <p>Un hôte client démarre dans l'état <span class="html">Init</span>. Il senvoie ensuite un segment <span class="em">SYN</span> est entre dans l'état <span class="html">SYN Sent</span> où il attend un segment <span class="em">SYN+ACK</span>. Ensuite, il répond avec un segment <span class="em">ACK</span> est entre dans l'état <span class="html">Established</span> où les données peuvent être échangées. En revanche, un hôte serveur démarre dans l'état <span class="em">Init</span>. Lorsqu'un processus serveur commence à écouter sur un port de destination, l'entité TCP sous-jacente crée un bloc de contrôle TCP et une file d'attente pour traiter les segments <span class="em">SYN</span> entrants. À la réception d'un segment <span class="em">SYN</span>, l'entité TCP du serveur répond avec un segment <span class="em">SYN+ACK</span> et entre dans l'état <span class="html">SYN RCVD</span>. Elle reste dans cet état jusqu'à la réception d'un segment <span class="em">ACK</span> qui accuse réception de son segment <span class="em">SYN+ACK</span>, elle entre dans l'état <span class="em">Established</span>.</p>
                    <p>Outre ces deux chemins dans l'automate de création de connexion TCP, il existe un troisième chemin qui correspond au cas où le client et le serveur envoient tous deux un segment <span class="em">SYN</span> pour ouvrir une connexion TCP. Bien sûr, une telle configuration simultanée ne peut se produire que si le port source choisi par le client est égal au port de destination choisi par le serveur. Cela peut arriver lorsque l'hôte peut servir à la fois de client et de serveur, ou dans des applications peer-to-peer lorsque les hôtes communicants n'utilisent pas de numéros de port éphémères. Dans ce cas, le client et le serveur envoient un segment <span class="em">SYN</span> et entrent dans l'état <span class="Em">SYN Sent</span>. À la réception du segment <span class="em">SYN</span> envoyé par l'autre hôte, ils répondent en envoyant un segment <span class="em">SYN+ACK</span> et entrent dans l'état <span class="em">SYN RCVD</span>. Le <span class="em">SYN+ACK</span> qui arrive de l'autre hôte permet de passer à l'état <span class="em">Established</span>. La figure ci-dessous illsutre une telle création simultanée d'une connexion TCP.</p>
                    <figure>
                        <img src="../images/etablissement_simultane_connexion_TCP.jpg" alt="">
                        <figcaption>Figure 4.40 : Établissement simultané d'une connexion TCP</figcaption>
                    </figure>
                    <hr>
                    <p>Attaques de Déni de Service (DoS pour Denial of Service) :</p>
                    <p>Lorsqu'une entité TCP ouvre une connexion TCP, elle crée un <span class="html">bloc de contrôle de transmission (TCB pour Transmission Control Block)</span>. Le TCB contient l'état entier maintenu par l'entité TCP pour chaque connexion TCP. Pendant l'établissement de la connexion, le TCB contient l'adresse IP locale, l'adresse IP distante, le numéro de port local, le numéro de port distant, le numéro de séquence local actuel, le dernier numéro de séquence reçu de l'entité distante. Jusqu'au milieu des années 1990, les implémentations TCP avaient une limite sur le nombre de connexions TCP qui pouvaient être dans l'état <span class="em">SYN RCVD</span> à un moment donné. De nombreuses implémentations fixaient cette limite à environ 100 TCB. Cette limite était considérée comme suffisante même pour les serveurs HTTP à charge élevée étant donné le faible délai entre la réception d'un segment <span class="em">SYN</span> et la réception du segment <span class="em">ACK</span> qui termine l'établissemnt de la connexion TCP. Lorsque la limite de 100 TCB dans l'état <span class="em">SYN Rcvd</span> est atteinte, l'entité TCP supprime tous les segments <span class="em">TCP SYN</span> reçus qui ne correspondent pas à un TCB existant.</p>
                    <p>Cette limite de 100 TCB dans l'état <span class="em">SYN Rcvd</span> a été choisie pour protéger l'entité TCP contre le risque de surcharge de sa mémoire avec trop de TCB dans l'état <span class="em">SYN Rcvd</span>. Cependant, elle était également la raison d'un nouveau type d'attaque de Déni de Service (DoS) RFC 4987. Une attaque DoS est définie comme une attaque où un attaquant peut causer une attaque DoS sur un lien de 2 Mbps utilisé par une entreprise en envoyant plus de 2 Mbps de paquets à travers ce lien. Dans ce cas, l'attaque DoS était plus subtile. COmme une entité TCP supprime tous les segments <span class="em">SYN</span> reçus d!s qu'elle a 100 TCB dans l'état <span class="em">SYN Rcvd</span>, un attaquant devait simplement envoyer quelques 100 segments <span class="em">SYN</span> chaque seconde à un serveur et ne jamais répondre aux segments <span class="em">SYN+ACK</span> reçus. Pour éviter d'être détecté, les attaquants envoyaient bien sûr ces segments <span class="em">SYN</span> avec une adresse différente de leur propre adresse IP. L'envoi d'un paquet avec une adresse IP source différente de l'adresse attribuée à l'hôte est appelé l'envoi d'un paquet falsifié. Sur la plupart des implémentations TCP, une fois qu'un TCB est entré dans l'état <span class="em">SYN Rcvd</span>, il reste dans cet état pendant plusieurs secondes, en attendant une retransmission du segment <span class="em">SYN</span> initial. Cette attaque a été plus tard appelée une attaque de <span class="html">SYN flood</span> et les serveurs de l'ISP nommé panix ont été parmi les premiers à être affectés par cette attaque.</p>
                    <p>Pour éviter les attaques de <span class="em">SYN flood</span>, les implémentations TCP récentes n'entrent plus dans l'état <span class="em">SYN Rcvd</span> dès réception d'un segment <span class="em">SYN</span>. Au lieu de cela, elles répondent directemnt avec un segment <span class="em">SYN+ACK</span> et attendent la réception d'un <span class="em">ACK</span> valide. Ce stratagème d'implémentation est possible uniquement si l'implémentation TCP est capable de vérifier que le segment <span class="em">ACK</span> reçu accuse réception du segment <span class="em">SYN+ACK</span> envoyé précédemment sans stocker le numéro de séquence initial de ce segment <span class="em">SYN+ACK</span> dans un TCB. La solution pour résoudre ce problème, connu sous le nom de <span class="html">cookieq SYN</span>, consiste à calculer les 32 bits de l'ISN comme suit :</p>
                    <ul>
                        <li>
                            <p>Les bits de poids fort contiennt les bits de poids faible d'un compteur qui est incrémenté lentement.</p>
                        </li>
                        <li>
                            <p>Les bits de poids fauble contiennent une valeur de hachage calculée sur les adresses IP locales et distantes et les numéros de port, ainsi qu'un secret aléatoire connu uniquement du serveur.</p>
                        </li>
                    </ul>
                    <p>L'avantage des <span class="em">cookies SYN</span> est que, en les utilisant, le serveur n'a pas besoin de créer un TCB lors de la réception du segment <span class="em">SYN</span> et peut toujours vérifier le segment <span class="Em">ACK</span> renvoyé en recomputant le <span class="em">cookie SYN</span>. Le principal inconvénient est qu'ils ne sont pas entièrement compatibles avec les options TCP. C'est pourquoi ils ne sont pas activés par défaut sur un système typique.</p>
                    <hr>
                    <p>Retransmission du premier segment SYN :</p>
                    <p>Comme IP founit un service de connexion sans fiabilité et sans état, les segments <span class="em">SYN</span> et <span class="em">SYN+ACK</span> pour ouvrir une connexion TCP peuvent être perdus. Les implémentations actuelles de TCP démarrent un timer de retransmission lorsqu'elles envoient le premier segment <span class="em">SYN</span>. Ce timer est souvent réglé sur trois secondes pour la première retransmission, puis il double après chaque retransmissionRFC 2988. Les implémentations de TCP imposent également un nombre maximal de retransmissions pour le segment <span class="em">SYN</span> initial.</p>
                    <hr>
                    <p>Comme expliqué précédemment, les segments TCP peuvent contenir une extension d'en-tête facultative. Dans les segments <span class="em">SYN</span> et <span class="em">SYN+ACK</span>, ces options sont utilisées pour négocier certains paramètres et l'utilisation d'extensions à la spécification de base de TCP.</p>
                    <p>Le premier paramètre négocié lors de l'établissement d'une connexion TCP est la <span class="html">taille maximale de segment (MSS pour Maximum Segment Size)</span>. La <span class="em">MSS</span> est la taille du plus grand segment qu'une entité TCP est capable de traiter. Selon la RFC 679, toutes les implémentations TCP doivent être capables de recevoir des segments TCP contenant 536 octets de payload. Cependant, la plupart des implémentations TCP sont capables de traiter des segments plus grands. De telles implémentations TCP utilisent l'option TCP <span class="em">MSS</span> dans le segment <span class="em">SYN/SYNN+ACK</span> pour indiquer le plus grand segment qu'elles sont capables de traiter. La valeur <span class="em">MSS</span> indique la taille maximum du payload des segments TCP. Le client (resp. le serveur) stocke dans sa TCB la valeur <span class="em">MSS</span> annoncée par le serveur (resp. le client).</p>
                    <p>Une autre utilisation des options TCP pendant l'établissement d'une connexion consiste à activer les extensions TCP. Par exemple, prenons en considération la RFC 1323 (qui est discutée dans <span class="em">TCPReliable</span>). La RFC 1323 définit des extensions TCP pour prendre en charge les horodatages et les fenêtres plus grandes. Si le client prend en charge la RFC 1323, il ajoute une option RFC 1323 à son segment <span class="em">SYN</span>. Si le seveur comprend cette oprion RFC 1323 et souhaite l'utiliser, il répond avec une option RFC 1323 et souhaite l'utiliser, il répond avec une option RFC 1323 dans le segment <span class="em">SYN+ACK</span> et l'extension définie dans la RFC 1323 est utilisée tout au long de la connexion TCP. Sinon, si le <span class="em">SYN+ACK</span> du serveur ne contient pas l'option RFC 1323, le client n'est pas autorisé à utiliser cette extension et les options d'en-tête TCP correspondantes tout au long de la connexion TCP. Le mécanisme d'options TCP est dlexible et permet l'extension de TCP tout en maintenant la compatibilité avec les anciennes implémentations.</p>
                    <p>Les options TCP sont codées en utilisant un format Type Longueur Valeur où :</p>
                    <ul>
                        <li>
                            <p>Le premier octet indique le type de l'option.</p>
                        </li>
                        <li>
                            <p>Le deuxièmpe octet indique la longueur totale de l'option (y compris les deux premiers octets) en octets.</p>
                        </li>
                        <li>
                            <p>Les derniers octets sont spécifiques à chaque type d'option.</p>
                        </li>
                    </ul>
                    <p>La RFC 793 définit l'option TCP <span class="em">Maximum Segment Size (MSS)</span> qui doit être comprise par toutes les implémentations TCP. Cette option (type 2) a une longueur de 4 octets et contient un mot de 16 bits qui indique le MSS pris en charge par l'émetteur du segment <span class="em">SYN</span>. L'option MSS ne peut être utilisée que dans les segments TCP ayant le drapeau <span class="em">SYN</span> activé.</p>
                    <p>La RFC 793 définit deux options spéciales qui doivent être prises en charge par toutes les implémentations TCP. La première option est <span class="html">End of option</span>. Elle est codée sur un seul octet ayant la valeur <span class="em">0x00</span> et peut être utilisée pour séassurer que l'extension d'en-tête se termine sur une limite de 32 bits. L'option <span class="html">No-Operation</span>, codée comme un octet unique ayant la valeur <span class="em">0x01</span>, peut être utilisée lorsque l'extension d'en-tête TCP contient plusieurs options TCP qui doivent être alignées sur des limites de 32 bits. Toutes les autres options sont codées en utilisant le format <span class="html">TLV</span>. La liste complète de toutes options TCP peut être trouvée à l'adresse suivante : <a href="http://www.iana.org/assignments/tcp-parameters/" target="_blank">http://www.iana.org/assignments/tcp-parameters/</a>.</p>
                    <hr>
                    <p>Note : Le principe de robustesse :</p>
                    <p>Le traitement des options TCP  par les implémentations TCP est l'une des nombreuses applications du principe de robustesse, généralemnt attribué à Jon Postel et souvent cité comme <q>Sois libéral dans ce que tu acceptes, et conservateur dans ce que tu envoies</q> (RFC 1122).</p>
                    <p>En ce qui concerne les options TCP, le principe de robustesse implique qu'ne implémentation TCP devrait être capable d'accepter des options TCP qu'elle ne comprend pas, en particulier dans les segments <span class="em">SYN</span> reçus, et qu'lle devrait être capable d'analyser n'importe quel segment reçu sans planter, même si le segment contient une option TCP inconnue. De plus, un serveyr ne doit pas envoyer dans le segment <span class="em">SYN+ACK</span> ou plus tard, des options qui n'ont pas été proposées par le client dans le segment <span class="em">SYN</span>.</p>
                    <hr>
                    <h4>4.3.2 Libération de la connexion TCP :</h4>
                    <p>TCP, comme la plupart des protocoles de transport orientés connexion, prend en charge deux types de libération de la connexion :</p>
                    <ul>
                        <li>
                            <p>Une libération de connexion gracieuse, où chaque utilisateur TCP libérer sa propre direction de transfert de données;</p>
                        </li>
                        <li>
                            <p>Une libération de connexion abrupte, où soit un utilisateur ferme les deux directions de transfert de données, soit une entité TCP est forcée de fermer la connexion (par exemple, parce que l'hôte distant ne répond plus ou en raison d'un manque de ressources).</p>
                        </li>
                    </ul>
                    <p>Le mécanisme de libération de connexion abrupte est très simple et repose sur un seul segment ayant le bit <span class="html">RST</span> activé. Un segment TCP contenant le bit <span class="em">RST</span> peut être envoyé pour les raisons suivantes :</p>
                    <ul>
                        <li>
                            <p>Un segment non-<span class="em">SYN</span> a été reçu pour une connexion TCP inexistante RFC 793;</p>
                        </li>
                        <li>
                            <p>Par extension, certaines implémentations répondent avec un segment <span class="em">RST</span> à un segment qui est reçu sur une connexion existante mais avec un en-tête invalide RFC 3360. Cela provoque la fermeture de la connexion correspondante et a causé des attaques de sécurité RFC 4953</p>
                        </li>
                        <li>
                            <p>Par extension, certaines implémentations envoient un segment <span class="em">RST</span> lorsqu'elles doivent fermer une connexion TCP existante (par exemple, parce qu'il n'y a pas assez de ressources pour prendre en charge cette connexion ou parce que l'hôte distant est considéré comme inaccessible). Des mesures ont montré que cette utilisation de TCP <span class="em">RST</span> était répandue [AW05].</p>
                        </li>
                    </ul>
                    <p>Lorsqu'un segment <span class="em">RST</span> est envoyé par une entité TCP, il doit contenir la valeur actuelle du numéro de séquence pour la connexion (ou <span class="em">0</span> s'il n'appartient à aucune connexion existante) et le numéro d'acquittement doit être défini sur le prochain numéro de séquence attendu en séquence sur cette connexion.</p>
                    <hr>
                    <p>Notes : Les guerres de TCP RST :</p>
                    <p>Les implémenteurs de TCP doivent veiller à ce que deux entités TCP n'entrent jamais dans une guerre de <span class="em">RST</span> où l'hôte A envoie un segment <span class="em">RST</span> en réponse à un segment <span class="em">RST</span> qui ne transportent pas de données, une entité TCP n'est jamais autorisée à envoyer un segment <span class="em">RST</span> en réponse à un autre segment <span class="em">RST</span>.</p>
                    <hr>
                    <p>La manière normale de terminer une connexion TCP est d'utiliser la libération de connexion TCP gracieuse. Ce mécanisme utilise le drapeau <span class="html">FIN</span> de l'en-tête TCP et permet à chaque hôte de libérer sa propre direction de transfert de données. Comme le drapeau <span class="em">SYN</span>, l'utilisation du drapeau <span class="em">FIN</span> dans l'en-tête TCP consomme un numéro de séquence. La figure <span class="em">Fig-tcprelease</span> montre la partie de la FSM TCP utilisée lorsqu'une connexion TCP est libérée.</p>
                    <figure>
                        <img src="../images/FSM_liberation_connexion_TCP.jpg" alt="">
                        <figcaption>Figure 4.41 : FSM pour la libération de connexion TCP</figcaption>
                    </figure>
                    <p>À partir de l'état <span class="em">Established</span>, il y a deux chemins principaux à travers cette FSM.</p>
                    <p>Le premier chemin est lorsque l'hôte reçoit un segment avec le numéro de séquence <span class="em">x</span> et le drapeau <span class="em">FIN</span> est défini. L'utilisation du drapeau <span class="em">FIN</span> indique que le byte avant le numéro de séquence <span class="em">x</span>  était le dernier byte du flux de bytes envoyé par l'hôte distant. Une fois que toutes les données ont été livrées à l'utilisateur, l'entité TCP envoie un segment <span class="em">ACK</span> dont le champ <span class="em">ack</span> est défini à <span class="em">(x + 1) mod 232</span> pour accuser réception du segment <span class="em">FIN</span>. Le segment <span class="em">FIN</span> est soumis aux mêmes mécanismes de retransmission qu'un segment TCP normal. En particulier, sa transmission est protégée par le timer de retransmission. À ce stade, la connexion TCP entre dans l'état <span class="html">CLOSE_WAIT</span>. Dans cet état, l'hôte peut encore envoyer des données à l'hôte distant. Une fois que toutes ses données ont été envoyées, il envoie un segment <span class="em">FIN</span> et entre dans l'état <span class="html">LAST_ACK</span>. Dans cet état, l'entité TCP attend l'accusé de réception de son segment <span class="em">FIN</span>. Il peut toujours retransmettre des segments de données non accusés, par exemple si le timer de retransmission expire. À la réception de l'accusé de réception pour le segment <span class="em">FIN</span>, la connexion TCP est complètement fermée et son TCB peut être supprimé.</p>
                    <p>Le deuxi!me chemin est lorsque l'hôte décide d'abord d'envoyer un segment <span class="em">FIN</span>. Dans ce cas, il entre dans l'état <span class="html">FIN_WAIT1</span>. Dans cet état, il peut retransmettre des segments non accusés mais ne peut pas envoyer de nouveaux segments de données. Il attend un accusé de réception de son segment <span class="em">FIN</span>, mais peut recevoir un segment <span class="em">FIN</span> envoyé par l'hôte distant. Dans le premier cas, la connexion TCP entre dans l'état <span class="html">FIN_WAIT2</span>. Dans cet état, de nouveaux segments de données de l'hôte distant sont encore acceptés jusqu'à la réception du segment <span class="em">FIN</span>. L'accusé de réception de ce segment <span class="em">FIN</span> est envoyé une fois que toutes les données reçues avant le segment <span class="em">FIN</span> ont été livrées à l'utilisateur et la connexion entre dans l'état <span class="html">TIME_WAIT</span>. Dans le deuxième cas, un segment <span class="em">FIN</span> est reçu et la connexion entre dans l'état <span class="html">Closing</span> une fois que toutes les données reçues de l'hôte distant ont été livrées à l'utilisateur. Dans cet état, aucun nouveau segment de données ne peut être envoyé et l'hôte attend un accusé de réception de son segment <span class="Em">FIN</span> avant d'entrer dans l'état <span class="em">TIME_WAIT</span>.</p>
                    <p>L'état <span class="em">TIME_WAIT</span> est différent des autres états du FSM TCP. Une entité TCP entre dans cet état après avoir envoyé le dernier segment <span class="em">ACK</span> sur une connexion TCP. Ce segment indique à l'hôte distant que toutes les données qu'il a envoyées ont été correctement reçues et qu'il peut libérer en toute sécurité la connexion TCP et supprimer le TCP correspondant. Après avoir envoyé le dernier segment <span class="Em">ACK</span>, une connexion TCP entre en <span class="em">TIME_WAIT</span> et reste dans cet état pendant 2 secondes MSL. Pendant cette période, le TCB de la connexion est maintenu. Cela garantit que l'entité TCP qui a envoyé le dernier <span class="em">ACK</span> maintient suffisamment d'état pour pouvoir retransmettre ce segment si ce segment <span class="em">ACK</span> est perdu et que l'hôte distant retransmet son dernier segment <span class="em">FIN</span> ou un autre. Le délai de 2 secondes MSL garantit que tout segment en double sur la connexion serait traité correctement sans provoquer la transmission d'un segment <span class="em">RST</span>. Sans l'état <span class="em">TIME_WAIT</span> et le délai de 2 secondes MSL, la libération de la connexion ne serait pas gracieuse lorsque le dernier segment <span class="em">ACK</span> est perdu.</p>
                    <hr>
                    <p>Note : TIME_WAIT sur les serveurs TCP occupés :</p>
                    <p>Le délai de 2 secondes MSL dans l'état <span class="em">TILME_WAIT</span> est un problème opérationnel important sur les serveurs ayant des milliers de connexions TCP ouvertes simultanément [FTY99]. Considéerons par exemple un serveur web occupé qui traite 10000 connexions TCP chaque seconde. SI chacune de ces connexions reste dans l'état <span class="em">TIME_WAIT</span> pendant 4 minutes, cela implique que le serveur devrait maintenir plus de 2 millions de TCB à tout moment. POur cette raison, certaines implémentations TCP préfèrent effectuer une libération abrupte de la connexion en envoyant un segment <span class="em">RST</span> pour fermer la connexion [AW05] et supprimer immédiatement le TCB correspondnat. Cependant, si le segment <span class="em">RST</span> est perdu, l'hôte distant continue de maintenir un TCB pour une connexion qui n'existe plus. Cette optimisation réduit le nombre de TCB maintenus par l'hôte envoyant le segment <span class="em">RST</span> mais au coût potentiel d'une augmentation du traitement sur l'hôte distant lorsque le segment <span class="em">RST</span> est perdu.</p>
                    <hr>
                    <h4>4.3.3 Transmission fiable de données TCP :</h4>
                    <p>Les mécanismes de transfert de données TCP d'origine ont été définis dans la RFC 793. Basé sur l'expérience d'utilisation de TCP sur l'Internet mondial croissant, cette partie de la spécification TCP a été mise à jour et améliorée plusieurs fois, tout en préservant la compatibilité ascendante avec les anciennes implémentations TCP. Dans cette section, nous passons en revue les principaux mécanismes de transpert de données utilisés par TCP.</p>
                    <p>TCP est un protocole de transport basé sur une fenêtre qui fournit un service de flux de bytes bidirectionnel. Cela a plusieurs implications sur les champs de l'en-tête TCP et les mécanismes utilsés par TCP. Les trois champs de l'en-tête TCP sont les suivants :</p>
                    <ul>
                        <li>
                            <p><span class="em">sequence number</span>. TCP utilise un numéro de séquence de 32 bits. Le <span class="em">sequence number</span> placé dans l'en-tête d'un segment TCP contenant des données est le numéro de séquence du premier octet du payload du segment TCP.</p>
                        </li>
                        <li>
                            <p><span class="em">acknowledgment number</span>. TCP utilise des acquittements positifs cumulatifs. Chaque segment TCP contient le numéro de séquence de la prochaine byte que l'expéditeur de l'acquittement s'attend à recevoir de l'hôte distant. En théorie, l'<span class="em">acknowledgment number</span> n'est valide que si le drapeau <span class="em">ACK</span> de l'en-tête TCP est défini. En pratique, presque tous les segments TCP ont leur drapeau <span class="em">ACK</span> défini. En pratique, seuls les segments <span class="em">SYN</span> n'ont pas leur drapeau <span class="em">ACK</span> défini.</p>
                        </li>
                        <li>
                            <p><span class="em">window</span>. Un récepteur TCP utilise ce champ de 16 bits pour indiquer la taille actuelle de sa fenêtre de réception exprimée en octets.</p>
                        </li>
                    </ul>
                    <hr>
                    <p>Note : Le bloc de contrôle de transmission :</p>
                    <p>Pour chaque connexion TCP établie, une implémentation TCP doit maintenir un bloc de contrôle de transmission (TCB). Un TCB contient toutes les informations requises pour envoyer et recevoir des segments sur cette connexion RFC 793. Cela inclut :</p>
                    <ul>
                        <li>
                            <p>L'adresse IP distante.</p>
                        </li>
                        <li>
                            <p>L'adresse IP distante.</p>
                        </li>
                        <li>
                            <p>Le numéro de port TCP local.</p>
                        </li>
                        <li>
                            <p>Le numéro de port distant.</p>
                        </li>
                        <li>
                            <p>L'état actuel du FSM TCP.</p>
                        </li>
                        <li>
                            <p>La <span class="em">taille maximale de segment</span> (MSS).</p>
                        </li>
                        <li>
                            <p><span class="html">snd.nxt</span> : le numéro de séquence du prochain octet dans le flux de données (le premier octet d'un nouveau segment de données que vous envoyez utilise ce numéro de séquence).</p>
                        </li>
                        <li>
                            <p><span class="html">snd.una</span> : le numéro de séquence le plus ancien qui a été envoyé mais qui n'a pas encore été accusé de réception.</p>
                        </li>
                        <li>
                            <p><span class="html">snd.wnd</span> : la talle actuelle de la fenêtre d'envoi (en octets).</p>
                        </li>
                        <li>
                            <p><span class="html">rcv.nxt</span> : le numéro de séquence du prochain octet  qui est attendu pour être reçu depuis l'hôte distant.</p>
                        </li>
                        <li>
                            <p><span class="html">rcv.wnd</span> : la taille actuelle de la fenêtre de réception annoncée par l'hôte distant.</p>
                        </li>
                        <li>
                            <p><span class="html">sending buffer</span> (tampon d'envoi) : un tampon utilisé pour stocké toutes les données non acquittées.</p>
                        </li>
                        <li>
                            <p><span class="html">received buffer</span> (tampon de réception) : un tampon pour stocker toutes les données reçues de l'hôte distant qui n'ont pas encore été livrées à l'utilisateur. Les données peuvent être stockées dans le tampon de réception car elles n'ont pas été reçues dans l'ordre ou parce que l'utilisateur est trop lent pour les traiter.</p>
                        </li>
                    </ul>
                    <p>Une implémentation TCP complète contient des informations supplémentaires dans son TCB, notamment pour prendre en charge le <span class="Em">Urgent pointer</span>. Cependant, cette partie de TCP n'est pas discutée dans ce livre. Référez-vous à la RFC 793 et à la RFC 2140 pour plus de détails sur le TCB.</p>
                    <hr>
                    <p>La spécification originale de TCP peut être catégorisée comme un protocole de transport qui fournit un service de flux de données et utilise <span class="em">Go-Back-N</span>.</p>
                    <p>Pour envoyer de nouvelles données sur une connexion établie, une entité TCP effectue les opérations suivantes sur la TCB correspondante. Elle vérifie d'abord que le <span class="Em">tampon d'envoi</span> ne contient pas plus de donénes que la fenêtre de réception annoncée par l'hôte distant (<span class="em">rcv.wnd</span>. Si la fenêtre n'est pas pleine, jusqu'à <span class="em">MSS</span> octets de données sont placés dans le payload d'un segment TCP. Le <span class="em">numéeo de séquence</span> de ce segment est le numéro de séquence du premier octet du payload. Il est défini sur le premier numéro de séquence disponible : <span class="em">snd.nxt</span> et <span class="em">snd.nxt</span> est incrémenté de la longueur du payload du segment TCP. Le <span class="em">numéro d'acquittement</span> de ce segment est défini sur la valeur actuelle de <span class="em">rcv.nxt</span> et le champ de la fenêtre du segment TCP est calculé en fonction de l'ocupation actuelle du <span class="em">tampon de réception</span>. Les données sont conservées dans le <span class="em">tampon d'envoi</span> au cas où elles devraient être retransmises ultérieurement.</p>
                    <p>Lorsqu'un segment TCP avec le drapeau <span class="em">ACK</span> est reçu, les opérations suivantes sont effectuées. <span class="em">rcv.wnd</span> est défini sur la valeur du champ de la fenêtre du segment reçu. Le <span class="em">numéro d'acquittement</span> est comparé à <span class="em">snd.una</span>. Les données nouvellemnt reconnues sont supprimées du <span class="em">tampon d'envoi</span> et <span class="em">snd.una</span> est mis à jour. Si le segment TCP contenait des données, le <span class="em">numéro de séquence</span> est comparé à <span class="em">rcv.nxt</span>. S'ils sont égaux, le segment a été reçu en séquence et les données peuvent être livrées à l'utilisateur et <span class="em">rcv.nxt</span> est mis à jour. Le contenu du <span class="em">tampon de réception</span> est vérifié pour voir si d'autres données déjà présentes dans ce tampon peuvent être livrées en séquence à l'utilisateur. Si c'est le cas, <span class="em">rcv.nxt</span> est mis à jour à nouveau. Sinon, le payload du segment est placé dans le <span class="em">tampon de réception</span>.</p>
                    <h5>Stratégies de transmission de segments :</h5>
                    <p>Dans un protocole de transport tel que TCP, qui offre un flux de données par octets, une question pratique qui a été laissée au choix de l'implémentation dans la RFC 793 est de décider quand un nouveau segment TCP contenant des données doit être envoyé. Il existe deux choix d'implémentation simples et extrêmes. Le premier choix d'implémentation consiste à envoyer un segment TCP dès que l'utilisateur a demandé la transmission de certaines données. Cela permet à TCP de fournir un service à faible latence. Cependant, si l'utilisateur envoie des données une par une, TCP placerait chaque octet de l'utilisateur dans un segment contenant 20 octets d'en-tête TCP. Ce segment TCP est ensuite placé dans une en-tête <span class="em">IP</span>. Nous décrivons <span class="em">IPv4</span> et <span class="em">IPv6</span> dans le prochain chapitre. La taille minimale de l'en-tête IPv4 (resp. IPv6) est de 20 octets (resp. 40 octets). C'est une surcharge énorme qui n'est pas acceptable dans les réseaux étendus. Une deuxième soloution consisterait à ne transmettre un nouveau segment TCP que lorsque l'utilisateur a produit MSS octets de données. Cette solution réduit la surcharge, mais au prix d'un délai potentiellement très élevé.</p>
                    <p>Une solution élégante à ce problème a été proposée par John Nagle dans la RFC 896. John Nagle a observé que la surcharge causée par l'en-tête TCP était un problème dans les connexions à grande distance, mais moins dans les connexions en réseau local où la bande passante disponible est généralement plus élevée. Il a proposé les règles suivantes pour décider d'envoyer un nouveau segment de données lorsqu'une nouvelle donnée a été produite par l'utilisateur ou lorsqu'un nouveau segment <span class="em">ack</span> a été reçu.</p>
<pre><code>if rcv.wnd &gt;= MSS and len(data) &gt;= MSS :
    send one MSS-sized segment
else:
    if there are unacknowledged data:
        place data in buffer until acknowledgement has been received
    else:
        send one TCP segment containing all buffered data</code></pre>
                    <p>La première règle garantit qu'une connexion TCP utilisée pour le transfert de données en vrac envoie toujours des segments TCP complets. La deuxième règle envoie un segment TCP partiellement rempli toutes les fois que le temps d'aller-retour est écoulé.</p>
                    <p>Cet algorithme, appelé l'algorithme de Nagle, nécessite quelques lignes de code dans toutes les implémentations TCP. Ces lignes de code ont un impact énorme sur les paquets échangés dans les réseaux TCP/IP. Des chercheurs ont analysé la distribution des tailles de paquets en capturant et en analysant tous les paquets passant par un lien donné. Ces études ont montré plusieurs résultats importants :</p>
                    <ul>
                        <li>
                            <p>Dans les réseaux TCP/IPv4, une grande fraction des paquets sont des segments TCP ne contenant qu'un accusé de réception. Ces paquets représentent généralment 40 à 50% des paquets passant par le lien étudié.</p>
                        </li>
                        <li>
                            <p>Dans les réseaux TCP/IPv4, la plupart des octets sont échangés dans de longs paquets, généralement des paquets contenant jusq'uà 1460 octets de payload, qui est la MSS par défaut pour les hôtes connectés à un réseau Ethernet, le type de LAN le plus populaire.</p>
                        </li>
                    </ul>
                    <p>La figure ci-dessous fournit une distribution des tailles de paquets mesurées sur une liaison. Elle montre une distribution tri-modale de la taille des paquets. 50% des paquets contiennent des accusés de réception TCP purs et occupent 40 octets. Environ 20% des paquets contiennet environ 500 octets de données utilisateur et 12% des paquets contiennent 1460 octets de données utilisateur. Lorsque ces mesures ont été prises, certains hôtes avaient une MSS par défaut de 552 octets (par exemple, les dérivés BSD Unix) ou 536 octets (la MSS par défaut spécifiée dans le RFC 793). Aujourd'hui, la plupart des implémentations TCP dérivent la MSS de la taille maximale de paquet de l'interface LAN qu'elles utilisent (Ethernet dans la plupart des cas). Cependant, la plupart des données utilisateur sont transportées dans des paquets volumineux. Cette distribution de la taille des paquets a des implications sur la conception des routeurs, comme nous le discutons dans le prochain chapitre.</p>
                    <figure>
                        <img src="../images/distribution_taille_paquets_internet.png" alt="">
                        <figcaption>FIgure 4.42 : Distribution de la taille des paquets dans l'Internet</figcaption>
                    </figure>
                    <p>Des mesures récentes indiquent que ces distributions de tailles de paquets sont toujours valables dans l'Internet actuel, bien que la distribution des paquets tendre à devenir bimodale avec de petits paquets correspondant aux <span class="em">ACK TCP</span> purs (40-64 octets selon l'utilisation des options TCP) et de grands paquets de 1460 octets transportant la plupart des données utilisateur.</p>
                    <h5>Fenêtres TCP :</h5>
                    <p>D'un point de vue de performance, l'une des principales limitations de la spécification TCP d'origine est le champ <span class="em">window</span> de 16 bits dans l'en-tête TCP. Comme ce champ indique la taille actuelle de la fenêtre de réception en octets, il llimite la fenêtre de réception TCP à 65535 octets. Cette limitation n'était pas un problème grave lorsque TCP a été conçu, car à cette époque, les réseaux étendus à haute vitesse offraient une bande passanta maximale de 56 kbps. Cependant, dasn les réseaux actuels, cette limitation n'est plus aceceptable. Le tableau ci-dessous fournit une estimation approximative du débit maximum pouvant être atteint par une connexion TCP avec une fenêtre de 64 Ko en fonction du temps de trajet aller-retour de la connexion. Une estimation précise de la bande passante maximale pouvant être atteinte par une connexion TCP doit tenir compte de la surchange des en-têtes TCP et IP.</p>
                    <table  class="tableBalises" role="presentation">
                        <thead>
                            <tr>
                                <th>RTT</th>
                                <th>Débit maximal</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>1 msec</td>
                                <td>524 Mbps</td>
                            </tr>
                            <tr>
                                <td>10 msec</td>
                                <td>52.4 Mbps</td>
                            </tr>
                            <tr>
                                <td>100 msec</td>
                                <td>5.24 Mbps</td>
                            </tr>
                            <tr>
                                <td>500 msec</td>
                                <td>1.05 Mbps</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>Pour résoudre ce problème, une extension compatible à l'arrière-plan qui permet à TCP d'utiliser des fenêtres de réception plus grandes a été proposée dans la RFC 1323. Aujourd'hui; la plupart des implémentations TCP prennent en charge cette option. L'idée de base est que, au lieu de stocker <span class="em">snd.wnd</span> et <span class="em">rcv.wnd</span> en tant qu'entiers sur 16 buts dans le <span class="em">TCB</span>, ils doivent être stockés en tant qu'entiers sur 32 bits. Comme l'en-tête de segment TCP ne contient que 16 bits pour placer le champ <span class="em">window</span>, il est impossible de copier la valeur de <span class="em">snd.wnd</span> dans chaque segment TCP envoyé. À la place, l'en-tête contient <span class="html">snd.wnd &gt;&gt; S</span> où <span class="em">S</span> est le facteur d'échelle (0 &lt;= S &lt;= 14) négocié lors de l'établissement de la connexion. Le client ajoute son facteur d'échelle proposé en tant qu'option TCP dans le segment <span class="em">SYN</span>. Si le serveur prend en charge la RFC 1323, il place dans le segment <span class="em">SYN+ACK</span> le facteur d'échelle qu'il utilise lors de l'annonce de sa propre fenêtre de réception. Les facteurs d'échelle local et distant sont inclus dans le TCB. Si le serveur ne prend pas en charge la RFC 1323, il ignore l'option reçue et aucune mise à l'échelle de fenêtre définies dans la RFC 1323, les implémentations TCP peuvent utiliser un tampon de réception allant jusqu'à 1 Go. Avec un tel tampon de réception, le débit maximal qu'une seule connexion TCP peut atteindre devient :</p>
                    <table class="tableBalises" role="presentation">
                        <thead>
                            <tr>
                                <th>RTT</th>
                                <th>Débit maximal</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>1 mssec</td>
                                <td>8590 Gbps</td>
                            </tr>
                            <tr>
                                <td>10 mssec</td>
                                <td>859 Gbps</td>
                            </tr>
                            <tr>
                                <td>100 mssec</td>
                                <td>86 Gbps</td>
                            </tr>
                            <tr>
                                <td>500 mssec</td>
                                <td>17 Gbps</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>Ces débits sont acceptables dans les réseaux d'aujourd'hui. Cependant, il existe déjà des serveurs ayant des interfaces de 10 Gbps... Les premières implémentations TCP avaient des tampons d'envoi et de réception fixes. Consultez <a href="http://fasterdata.es.net/tuning.html" target="_blank">http://fasterdata.es.net/tuning.html</a> pour plus d'informations sur la façon d'ajuster une implémentation TCP. Les implémentations haute performance d'aujourd'hui sont capables d'ajuster autoatiquement la taille des tampons d'envoi et de réception pour mieux prendre en charge les flux à haute bande passanta [SMM1998].</p>
                    <p>Le temps d'attente de retransmission TCP :</p>
                    <p>Dans un protocole de transport <span class="em">Go-Back-N</span> tel que TCP, le temps d'attente de retransmission doit être correctement défini pour obtenir de bonnes performances. Si le temps d'attente de retransmission expire trop tôt, la bande passante est gaspillée en retransmettant des segments qui ont déjà été correctement reçus; tandis que si le temps d'attente de retransmission expire trop tard, la bande passante est gaspillée parce que l'émetteur est inactif en attendant l'expiration de son temps d'attente de retransmission.</p>
                    <p>Un bon réglage du temps d'attente de retransmission dépend clairement d'une estimation précise du temps aller-retour de chaque connexion TCO. Le temps aller-retour diffère entre les connexions TCP, mais peut également changer au cours de la durée de vie d'une seule connexion. Par exemple, la figure ci-dessous montre l'évolution du temps aller-retour entre deux hôtes pendant une période de 45 secondes.</p>
                    <figure>
                        <img src="../images/evolution_round-trip-time_entre_2hotes.png" alt="">
                        <figcaption>Figure 4.43 : Évolution du temps aller-retour entre deux hôtes</figcaption>
                    </figure>
                    <p>La solution la plus simple pour mesurer le temps aller-retour sur une coneexion TCP est de mesurer le délai entre la transmission d'un segment de données et la réception d'un accusé de réception correspondnat. En théorie, une implémentation TCP pourrait stocker l'horodatage de chaque segment de données transmis et calculer une nouvelle estimation pour le temps aller-retour lors de la réception de l'accusé de réception correspondant. Cependant, l'utilisation de mesures aussi fréquentes introduit beaucoup de bruit en pratique et de nombreuses implémentations mesurent toujours le temps aller-retour une fois par temps aller-retour en enregistrant le temps de transmission d'un segment à la fois RFC 2988. Comme illustré dans la figure ci-dessous, cette mesure fonctionne bien lorsqu'il n'y a pas de pertes de segments.</p>
                    <figure>
                        <img src="../images/mesurer_round-trip-time.jpg" alt="">
                        <figcaption>Figure 4.44 : Comment mesurer le temps aller-retour ?</figcaption>
                    </figure>
                    <p>Cependant, lorsqu'un segment de données est perdu, comme illustré dans la partie inférieure de la figure, la mesure devient ambigüe car l'émetteur ne peut déterminer si l'accusé de réception reçu a été déclenché par la première transmission du segment <span class="em">123</span> ou par sa retransmission. L'utilisation d'estimations incorrectes du temps de trajet aller-retour pourrait conduire à des valeurs incorrectes du délai de retransmission. Pour cette raison, Phil Karn et Craig Patridge ont proposé, dans [KP91], d'ignorer les mesures de temps de trajet aller-retour effectuées pendant les retransmisions.</p>
                    <p>Pour éviter cette ambiguïté dans l'estimation du temps de trajet aller-retour lors de la retransmission de segments, les implémentations récentes de TCP reposent sur l'option <span class="html">timestamp</span> définie dans la RFC 1323. Cette option permet à un émetteur TCP de placer deux horodatages de 32 bits dans chaque segment TCP qu'il envoie. Le premier horodatage, <span class="html">TS Value (TSval)</span>, est choisi par l'émetteur du segment. Il pourrait par exemple s'agir de la valeur actuelle de son horloge en temps réel. Certains expertes en sécurité ont soulevé des préoccupations selon lesquelles l'utilisation de l'horloge en temps réel pour définir le <span class="em">TSval</span> dans l'option <span class="em">timestamp</span> peut divulguer des informations telles que le temps de fonctionnement du système. Des solutions proposées pour résoudre ce problème peuvent être trouvées dans [CNPI09]. La deuxième valeur, <span class="html">TS Echo Reply (TSecr)</span>, est le dernier <span class="em">TSval</span> qui a été reçu de l'hôte distant et stocké dans le <span class="em">TCB</span>. La figure ci-dessous montre comment l'utilisation de cette option <span class="em">timestamp</span> permet de dissocier la mesure du temps de trajet aller-retour lorsqu'il y a des retransmissions.</p>
                    <figure>
                        <img src="../images/dissiper_ambiguite_mesures_round-trip-time_option_timestamp_RFC1323.png" alt="">
                        <figcaption>Figure 4.45 : Dissiper l'ambiguïté des mesures de temps aller-retour avec l'option timestamp de RFC 1323</figcaption>
                    </figure>
                    <p>Une fois que les mesures de temps aller-retour ont été collectées pour une connexion TCP donnée, l'entité TCP doit calculer le temps d'attente de retransmission. Comme les mesures de temps aller-retour peuvent changer pendant la durée de vie d'une connexion, le temps d'attente de retransmission peut également charger. Au début d'une connexion, l'entité TCP qui envoie un segment <span class="em">SYN</span> ne connaît pas le temps aller-retour pour atteindre l'hôte distant et le temps d'attente de retransmission initial est généralmenet fixé à 3 secondes RFC 2988. Comme un client TCP établit souvent plusieurs connexions parallèles ou successives avec le même serveur, la RFC 2140 a proposé de réutiliser pour une nouvelle connexion certaines informations qui ont été collectées dans le TCB d'une connexion précédente, telles que le temps aller-retour mesuré. Cependant, cette solution n'a pas été largement implémentée.</p>
                    <p>La spécification TCP originale proposée dans la RFC 793 propose d'inclure deux variables supplémentaires dans le TCB :</p>
                    <ul>
                        <li>
                            <p><span class="html">srtt</span> (smoothed round)trop-time) : le temps aller-retour lissé calculé comme <span class="html">(&#945; * srtt) + ((1 - &#945;) * rtt)</span> où <span class="em">rtt</span> est le temps aller)retour mesuré selon la proccédure ci-dessus  et <span class="em">&#945;</span> un facteur de lissage (par exemple 0,8 ou 0,9).</p>
                        </li>
                        <li>
                            <p><span class="html">rto</span> (retransmission timeout) : le temps d'attente de retransmission est calculé comme <span class="html">rto = min(60, max(1, &#946; * srtt))</span> où <span class="em">&#946;</span> est utilisé pour tenir compte de la variance du délai (valeur : 1,3 à 3,0). Les constantes <span class="em">60</span> et <span class="em">1</span> sont utilisées pour garantir que le <span class="em">rto</span> n'est pas plus grand qu'une minute ni plus petit qu'une seconde.</p>
                        </li>
                    </ul>
                    <p>Cependant, en pratique, ce calcul pour le temps d'attente de retransmission ne fonctionnait pas bien. Le principal problème était que le <span class="em">rto</span> calculé ne prenait pas correctement en compte les variations du temps aller-retour mesuré. Van Jacobson a proposé dans son article fondateyr [Jacobson1988] un algorithme pour calculer le <span class="em">rto</span> et l'a implémenté dans la distribution BSD Unix. Cet algorithme fait maintenant partie de la norme TCP RFC 3988.</p>
                    <p>L'algorithme de Jacobson utilise deux variables d'état, <span class="em">srtt</span> le temps aller-retour lissé et <span class="html">rttvar</span> l'estimation de la variance du temps aller-retour et deux paramètres : <span class="em">&#945;</span> et <span class="em">&#946;</span>. lorsqu'une connexion TCP démarre, le premier <span class="em">rto</span> est fixé à 3 secondes. Lorsqu'une première estimation du temps aller-retour est disponible, le <span class="em">srtt</span>, le <span class="em">rttvar</span> et le <span class="em">rto</span> sont calculés comme suit :</p>
<pre><code>srtt = rtt
rttvar = rtt / 2
rto = srtt + 4 * rttvar</code></pre>
                    <p>Ensuite, lorsque d'autres mesures de temps aller-retour sont collectées, <span class="em">srtt</span> et <span class="em">rtttvar</span> sont mis à jour comme suit :</p>
<pre><code>rttvar = (1 - &#946;) * rrtvar + &#946; * |srtt - rtt|
srtt = (1 - &#945;) * srtt + &#945; * rtt
rto = srtt + 4 * rttvar</code></pre>
                    <p>Les valeurs proposées pour les paramètres sont <span class="em">&#945; = 1 / 8</span> et <span class="em">&#946; = 1 / 4</span>. Cela permet à une implémentation TCP, implémentée dans le noyau, de calculer le <span class="em">rtt</span> en utilisant des opérations de décalage au lieu des opérations à virgule flottante plus coûteuses [Jacobson1988]. La figure ci-dessous illustre le calcul de <span class="em">rto</span> en cas de changements de <span class="em">rtt</span>.</p>
                    <figure>
                        <img src="../images/exemple_calcul_rto.png" alt="">
                        <figcaption>Figure 4.46 : Exemple de calcul de rto</figcaption>
                    </figure>
                    <h5>Statégies de retransmission avancées :</h5>
                    <p>La stratégie de retransmission par défaut <span class="em">Go-Back-N</span> a été définie dans le RFC 793. Lorsque le timer de retransmission expire, TCP retransmet le premier segment non acquitté (c'est-à-dire celio ayant un numéro de séquence <span class="em">snd.una</span>). Après chaque expiration du timeout de retransmission, le RFC 2988 recommande de doubler la valeur du timeout de retransmission. Cela s'appelle un "<span class="html">exponential backoff</span>". Ce doublement du timeout de retransmission après une retransmission a été inclus dans TCP pour faire face à des problèmes tels que la surcharge réseau/récepteur et les estimations initiales incorrectes du timeout de retransmission. Si le même segment est retransmis plusieurs fois jusqu'à ce qu'il atteigne un maximum configuré. Le RFC 2988 suggère un maximum de timeout de retransmission d'au moins 60 secondes. Une fois que le timeout de retransmission atteint ce maximum configuré, l'hôte distant est considéré comme inaccessible et la connexion TCP est fermée.</p>
                    <p>Cette stratégie de retransmission a été affinée en fonction de l'expérience de l'utilisation de TCP sur Internet. La première amélioration a été une clarification de la stratégie utilisée pour envoyer les accusés de réception. Comme TCP utilise le <span class="em">piggybacking</span>, la méthode la plus facile et la moins coûteuse pour envoyer les accusés de réception est de les placer dans les segments de données envoyés dans l'autre sens. Cependant, peu de protocoles de couche application échangent des données dans les deux sens en même temps, et donc cette méthode fonctionne rarement. Pour une application qui envoie des segments de données dans une direction seulement, l'entité TCP distante renvoie des segments TCP vides dont la seule information utile est leur numéro d'accusé de réception. Cela peut causer une grande surcharge dans le réseau étendu si un segment <span class="em">ACK</span> pur est envoyé en réponse à chaque segment de données reçu. La plupart des implémentations TCP utilisent une stratégie d'accusé de réception différé. Cette stratégie garantit que le <span class="em">piggybacking</span> est utilisé chaque fois que possible, sinon des segments <span class="em">ACK</span> purs sont envoyés pour chaque deuxième segment de données reçu lorsqu'il n'y a pas de perte.  Lorsqu'il y a des pertes ou des réorganisations, les segments <span class="em">ACK</span> sont plus importants pour l'expéditeur et ils sont envoyés immédiatement RFC 813 RFC 1122. Cette stratégie repose sur un nouveau timer avec un court délai (par exemple 50 millisecondes) et un drapeau  supplémentaire dans le <span class="em">TCB</span>. Elle peut être implémentée comme suit :</p>
<pre><code>reception of a data segment:
    if pkt.seq==rcv.nxt: # segment received in sequence
        if delayedack :
            send pure ack segment
            cancel acktimer
            delayedack=False
        else:
            delayedack=True
            start acktimer
    else: # out of sequence segment
        send pure ack segment
        if delayedack:
            delayedack=False
            cancel acktimer

transmission of a data segment: # piggyback ack
    if delayedack:
        delayedack=False
        cancel acktimer

acktimer expiration:
    send pure ack segment
    delayedack=False</code></pre>
                    <p>En raison de cette stratégie de reconnaissance de retard, lors d'un transfert en vrac, une implémentation TCP reconnaît généralement chaque deuxième segment TCP reçu.</p>
                    <p>La stratégie de retransmission par défaut de type "<span class="em">Go-Back-N</span>" utilisée par TCP a l'avantage d'être simple à implémenter, en particulier du côté du récepteur, mais lorsqu'il y a des perte, une stratégie "<span class="em">Selective Repeat</span>" fournit des performances inférieures à une stratégie "<span class="em">Go-Back-N</span>". Les développeurs de TCP ont conçu plusieurs extensions de TCP pour lui permettre d'utiliser une stratégie "<span class="em">Selective Repeat</span>" tout en maintenant une compatibilité descendante avec les anciennes implémentations de TCP/ Ces extensions de TCP supposent que le récepteur est capable de mettre en tampon les segments qu'il reçoit hors séquence. La première extension proposée est l'heuristique de retransmission rapide. Cette extension peut être implémentée sur les émetteurs TCP et ne nécessite donc aucun changement du protocole. Elle suppose simplement que le récepteur TCP est capable de mettre en tampon les segments hors séquence.</p>
                    <p>D'un point de vue de la performance, un problème avec le délai de retransmission de TCP est que lorsque des pertes de segments isolées se produisent, l'émetteur TCP reste souvent inactif en attendant l'expiration de ses délais de retransmission. De telles pertes isolées sont fréquentes dans l'Internet mondial [Paxson99]. Une heuristique pour traiter les pertes isolées sans attendre l'expiration du délai de retransmission a été incluse dans de nombreuses implémentations TCP depuis le début des années 1990. Pour comprendre cette heuristique, considérons la figure ci-dessous qui montre les segments échangés sur une connexion TCP lorsqu'un segment isolé est perdu.</p>
                    <figure>
                        <img src="../images/detection_pertes_segments_isoles.jpg" alt="">
                        <figcaption>Figure 4.47 : Détection de pertes de segments isolées</figcaption>
                    </figure>
                    <p>Comme indiqué ci-dessus, lorsqu'un seglent isolé est perdu, l'émetteur reçoit plusieurs acquittements en double car le récepteur TCP envoie immédiatement un acquittement pur lorsqu'il reçoit un segment hors séquence. Un acquittement en double est un acquittement qui contient le même numéro d'acquittement qu'un segment précédent. Un seul acquittement en double n'implique pas nécessairemnt qu'un segment a été perdu, car une simple réorganisation des segments peut également provoquer des acquittements en double. Des mesures [Paxson99] ont montré que la réorganisation de segments est fréquente dans l'Internet. Sur la base de ces observations, l'heuristique de retransmission rapide a été incluse dans la plupart des implémentations TCP. Elle peut être implémentée comme suit :</p>
<pre><code>ack arrival:
    if tcp.ack==snd.una: # duplicate acknowledgement
        dupacks++
        if dupacks==3:
            retransmit segment(snd.una)
    else:
        dupacks=0
        # process acknowledgment</code></pre>
                    <p>Cette heuristique nécessite une variable supplémentaire dans le TCB (<span class="html">dupacks</span>). La pluaprt des implémentations définissent le nombre par défaut d'acquittements dupliqués déclenchant une retransmission à 3. Elle fait désormais partie de la spécification standard de TCP RFC 2581. L'heuristique de <span class="em">retransmission rapide</span> améliore les performances de TCP à condition que des segments isolés soient perdus et que la fenêtre actuelle soit suffisamment grande pour permettre à l'émetteur d'envoyer trois acquittements dupliqués.</p>
                    <p>La figure ci-dessous illustre le fonctionneement de l'heuristique de <span class="em">retransmission rapide</span> de TCP.</p>
                    <figure>
                        <img src="../images/heuristiques_retransmission_rapide_TCP.jpg" alt="">
                        <figcaption>Figure 4.48 : Heuristiques de retransmission rapide de TCP</figcaption>
                    </figure>
                    <p>Lorsque les pertes ne sont pas isolées ou que les fenêtres sont petites, la performance de l'heuristique de <span class="em">retransmission rapide</span> diminuent. Dans de tels environnements, il est nécessaire de permettre à un émetteur TCP d'utiliser une stratégie "<span class="em">Selective Repeat</span>" au lieu de la stratégie par défaut "<span class="em">Go-Back-N</span>". L'implémentation de la <span class="em">Selective Repeat</span> nécessite une modification du protocole TCP, car le récepteur doit être capable d'informer l'émetteur des segments hors séquence qu'il a déjà reçus. Cela peut être fait en utilisant l'option <span class="html">Selective Acknowledgements (SACK)</span> définie dans la RFC 2018. Cette option TCP est négociée lors de l'établissement d'une connexion TCP. Si les deux hôtes TCP prennent en charge l'option, des blocs <span class="em">SACK</span> peuvent être attachés par le récepteur aux segments qu'il envoie. Les blocs <span class="em">SACK</span> permettent à un récepteur TCP d'indiquer les blocs de données qu'il a reçus correctement mais hors séquence. La figure ci-dessous illustre l'utilisation des blocs <span class="em">SACK</span>.</p>
                    <figure>
                        <img src="../images/accuses_reception_selectifs_TCP.jpg" alt="">
                        <figcaption>Figure 4.49 : Accusés de réception sélectifs TCP (SACK)</figcaption>
                    </figure>
                    <p>Une option <span class="em">SACK</span> contient un ou plusieurs blocs. Un bloc correspond à tous les numéros de séquence entre le <span class="html">left edge (trad. bord gauche)</span> et le <span class="html">right edge (trad. bord droit)</span> du bloc. Les deux bords du bloc sont encodés sous forme de nombres de 32 bits (de la même taille que le numéro de séquence TCP) dans une option <span class="em">SACK</span>. Comme l'option <span class="em">SACK</span> contient un octet pour coder son type et un octet pour sa longueur, une option <span class="em">SACK</span> contenant <span class="em">b</span> blocs est encodée sous forme d'une séquence de <span class="em">2 + 8 * b</span> octets. En pratique, la taille de l'option <span class="em">SACK</span> peut poser problème car l'extension d'en-tête TCP facultative ne peut pas dépasser 44 octets. Comme l'option <span class="em">SACK</span> est généralement combinée avec l'extension de marque temporelle RFC 1323, cela implique qu'un segment TCP ne peut généralement pas contenir plus de trois blocs <span class="em">SACK</span>. Cette limitation implique qu'un récepteur TCP ne peut pas toujours placer dans l'option <span class="em">SACK</span> qu'il envoie, des informations sur tous les blocs reçus.</p>
                    <p>Pour faire face à la taille limitée de l'option <span class="em">SACK</span>, un récepteur TCP ayant actuellement plus de 3 blocs dans son tampon de réception doit sélectionner les blocs à placer dans l'option <span class="em">SACK</span>. Une bonne heuristique consiste à placer dans l'option <span class="em">SACK</span> les blocs qui ont changé le plus récemment, car l'émetteur est susceptible d'être déjà au courant des blocs plus anciens.</p>
                    <p>Lorsqu'un émetteur reçoit une option <span class="em">SACK</span> indiquant un nouveau bloc et donc une nouvelle perte possible de segment, il ne réémet généralement pas immédiatement le ou les segments manquants. Pour traiter la réorganisation, un émetteur TCP peut utiliser une heuristique similaire à la retransmission rapide en retransmettant un écart uniquement une fois qu'il a reçu trois options <span class="em">SACK</span> indiquant cet écart. Il convient de noter que l'option <span class="em">SACK</span> ne remplace pas le numéro d'accusé de réception de l'en-tête TCP. Un émetteur TCP ne peut supprimer des données de son tampon d'envoi que lorsqu'elles ont été accusées de réception cumulatifs de TCP. Cette conception a été choisie pour deux raisons. Premièrement, elle eprmet au récepteur de supprimer des parties de son tampon de réception lorsqu'il manque de mémoire sans perdre de données. Deuxièmement, comme l'option <span class="em">SACK</span> n'est pas transmise de manière fiable, les accusés de réception cumulatifs sont toujours nécessaires pour traiter les pertes de segments <span class="em">ACK</span> transportant uniquement des informations <span class="em">SACK</span>. Ainsi, l'option <span class="em">SACK</span> ne sert que d'indice permettant à l'émetteur d'optimiser ses retransmissions.</p>
                    <h5>Contrôle de congestion TCP :</h5>
                    <p>Dans les sections précédentes, nous avons expliqué les mécanismes que TCP utilise pour traiter les erreurs de retransmission et les pertes de segments. Dans un réseau hétérgogène tel que l'Internet ou les réseaux IP d'entreprise, les systèmes terminaux sont des serveurs haut de gamme attachés à des liaisons de 10 Gbps, tandis que d'autres sont des dispositifs mobiles attachés à une liaison sans fil à très faible bande passante. Malgré ces énorms différences de performances, un dispositif mobile devrait être capable d'éffacer efficacement des segments avec un serveur haut de gamme.</p>
                    <p>Pour mieux comprendre ce problème, considérons le scénario illustré dans la figure ci-dessous, où un serveur (<span class="em">A</span>) attaché à une liaison de 10 Mbps envoie des segments TCP à un autre ordinateur (<span class="em">C</span>) via un chemin qui contient une liaison de 2 Mbps.</p>
                    <figure>
                        <img src="../images/TCP_liens_heterogenes.png" alt="">
                        <figcaption>Figure 4.50 : TCP sur des liens hétérogènes</figcaption>
                    </figure>
                    <p>Dans ce réseau, les sgments TCP envoyés par le serveur atteignent le routeur <span class="em">R1</span>. <span class="em">R1</span> transfère les segments vers le routeur <span class="em">R2</span>. Le routeur <span class="em">R2</span> peut potentiellement recevoir des segments à 10 Mbps, mais il ne peut les transférer qu'à 2 Mbps vers le routeur <span class="em">R2</span>, puis vers l'hôte <span class="em">C</span>. Le routeur <span class="em">R2</span> contient des tampons qui lui permettent de stocker les paquets qui ne peuvent pas être immédiatement transférés à leur destination. Pour comprendre le fonctionnement de TCP dans cet environnement, considérons un modèle simplifié de ce réseau où l'hôte <span class="em">A</span> est attach" à une liaison de 10 Mbps vers une file d'attente qui représente les tampons du routeur <span class="Em">R2</span>. Cette file d'attente est vidée à un débit de 2 Mbps.</p>
                    <p>Supposons que l'hôte <span class="em">A</span> utilise une fenêtre de trois segments. Il envoie ainsi trois segments consécutifs à 10 Mbps, puis attend un accusé de réception. L'hôte <span class="em">A</span> cesse d'envoyer des segments lorsque sa fenêtre est pleine. Ces segments atteignent les tampons du routeur <span class="em">R2</span>. Le premier segment stocké dans ce tampon est envoyé par le routeur <span class="em">R2</span> à un débit de 2 Mbps vers l'hôte de destination. À la réception de ce segment, la destination envoie un accusé de réception. Cet accusé de réception permet à l'hôte <span class="em">A</span> de transmettre un nouveau segment. Ce segment est stocké dans les tampons du routeur <span class="em">R2</span> pendnat qu'il transmet le deuxième segment envoyé par l'hôte <span class="em">A</span>... Ainsi, après la transmission de la première fenêtre de segments, TCP envoie un segment de données après la réception de chaque accusé de réception renvoyé par la destination. SI la destination utilise des accusés de réception différés, l'hôte émetteur envoie deux segments de données après chaque accusé de réception. En pratique, les accusés de réception envoyés par la destination servent de type d'horloge qui permet à l'hôte émetteur d'adapter son débit de transmission au débit auquel les segments sont reçus par la destination. Cette <span class="html">auto-horloge TCP</span> est le premier mécanisme qui permt à TCP de s'adapter aux réseaux hétérogènes [Jacobson1988]. Elle dépend de la disponibilité de tampons pour stocker les segments qui ont été envoyés par l'expéditeur mais n'ont pas encore été transmis à la destination.</p>
                    <figure>
                        <img src="../images/auto-calage_TCP.png" alt="">
                        <figcaption>Figure 4.51 : L'auto-clocking TCP</figcaption>
                    </figure>
                    <p>Cependant, TCP n'est pas toujours utilisé dans cet environnement. Dans l'Internet global, TCP est utilisée dans des réseaux ou un grand nombre d'hôtes envoient des segments à un grand nombre de destinataires. Par exemple, considérons le réseau représenté ci-dessous qui est celui discuté dans [Jacobson1988] et RFC 896. Dans ce réseau, nous supposons que les tampons du routeur sont infis pour garantir qu'aucun paquet ne soit perdu.</p>
                    <figure>
                        <img src="../images/probleme_effondrement_congestion.jpg" alt="">
                        <figcaption>Figure 4.42 : Le problème d'effondrement de la congestion</figcaption>
                    </figure>
                    <p>Si de nombreux émetteurs TCP sont connectés à la partie gauche du réseau ci-dessus, ils envoient tous une fenêtre pleine des segments. Ces segments sont stockés dans les tampons du routeur avant d'être transmis vers leur destination.Si de nombreux émetteurs sont présents sur la partie gauche du réseau, l'occupation des tampons augmente rapidement. Une conséquence de l'occupation du tampon est que le temps de trajet aller-retour, mesuré par TCP, entre l'émetteur et le récepteur augmente. Considérons un réseau où des segments de 10000 bits sont envoyés. Lorsque le tampon est vide, un tel segment nécessite 1 milliseconde pour être transmis sur le lien de 10 Mbps et 5 millisecondes pour être transmis sur le lien de 2 Mbps. Ainsi, le temps de trajet aller-retour mesuré par TCP est d'environ 6 millisecondes si l'on ignore le délai de propagation sur les liens. La plupart des routeurs gèrent leurs tampons sous forme de file d'attente FIFO. Nous discutons dans un autre chapitre d'autres organisations possibles de tampons du routeur. Si le tampon contient 100 segments, le temps de trajet aller-retour devient de <span class="em">1 + 100 * 5 + 5 millisecondes</span>, car les nouveaux segments ne sont transmis sur le lien de 2 Mbps qu'une fois que tous les segments précédents ont été transmis. Malheureusement, TCP utilise un timer de retransmission et effectue un <span class="em">Go-Back-N</span> pour récupérer les erreurs de transmission. Si l'occupation du tampon est élevée, TCP suppose que certains segments ont été perdus et retransmet une fenêtre complète de segments. Cela augmente l'occupation du tampon et le délai de transmission... De plus, le tampon peut stocker et envoyer sur les liens à faible bande passante plusieurs retransmissions du même segment. Ce problème est appelé <span class="html">effondrement de la congestion</span>. Il s'est produit plusieurs fois à la fin des années 1980. Par exemple, [Jacobson1988] note qu'en 1986, la bande passante utilisable d'un lien de 32 Kbits est tombée à 40 bits par seconde en raison de l'effondrement de la congestion ! À cette époque, les implémentations TCP suivaient principalement la RFC 793. Les estimations de temps de trajet aller-retour et les mécanismes de retransmission étaient très simples. TCP a été amélioré après la publication de [Jacobson1988].</p>
                    <p>Le <span class="html">collapsus de congestion</span> est un problème auquel sont confrontés tous les réseaux hétérogènes. Différents mécanismes ont été proposés dans la littérature scientifique pour éviter ou contrôler la congestion du réseau. Certains d'entre eux ont été implémentés et déployés dans des réseaux réels. Pour comprendre ce problème plus en détail, considérons d'abord un réseau simple avec deux hôtes atatchés à un lien haute bande passante qui envoient des segments à la destination <span class="em">C</span> attachée à un lien basse bande passante comme indiqué ci-dessous.</p>
                    <figure>
                        <img src="../images/probleme_congestion.png" alt="">
                        <figcaption>Figure 4.43 : Le problème de congestion</figcaption>
                    </figure>
                    <p>Pour éviter l'<span class="html">effondrement de la congestion</span>, les hôtes doivent réguler leur taux de retransmission en utilisant un mécanisme de contrôle de congestion. Dans cette section, nous nous concentrons sur les mécanismes de contrôle de congestion qui régulent le taux de transmission des hôtes. D'autres types de mécanismes ont été proposés dans la littérature. Par exemple, le contrôle de mécanismes ont été proposés dans la littérature. Par exemple, le contrôle de flux basé sur des crédits a été proposé pour éviter la congestion dans les réseaux ATM [KR1995]. Avec un mécanisme basé sur des crédits, les hôtes ne peuvent envoyer des envoyer de paquets que s'ils ont reçu des crédits des routeurs, et les crédits dépendent de l'occupation des tampons du routeur. Un tel mécanisme peut être implémenté dans la couche de transport ou dans la couche de réseau. Dans les réseaux TCP/IP, il est implémenté dans la couche de transport, mais d'autres technologies telles que l'<span class="html">Asynchronous Transfer Mode (ATM)</span> ou <span class="html">Frame Relay</span> incluent des mécanismes de contrôle de congestion dans des couches inférieures.</p>
                    <p>Commençons d'abord par le problème simple d'un ensemble de <span class="em">i</span> hôtes qui partagent un seul lien d'étranglement, comme indiqué dans l'exemple ci-dessus. Dans ce réseau, le schéma de contrôle de congestion doit atteindre les objectifs suivants [CJ1989] :</p>
                    <ol>
                        <li>
                            <p>Le schéma de contrôle de congestion doit éviter la congestion. En pratique, cela signifie que le lien le plus lent ne doit pas être surchargé. Si <span class="em">r<sub>i</sub>(t)</span> est le débit de transmission alloué à l'hôte <span class="em">i</span> au moment <span class="em">t</span> et que <span class="em">R</span> est la bande passante du lien le plus lent, alors le schéma de contrôle de congestion doit garantir que, en moyenne, <span class="em">&#8704; &#821;<sub>t</sub> r<sub>i</sub>(t) &lt;= R</span>.</sub></p>
                        </li>
                        <li>
                            <p>Le schéma de contrôle de congestion doit être efficace. Le lien de goulot d'étrangelement est généralement à la fois une ressource partagée et coûteuse. Habituellement, les liens de goulot d'étranglement sont des liens de grande distance qui sont beaucoup plus coûteux à mettre à niveua que les réseaux locaux. Le schéma de contrôle de congestion doit garantir que de tels liens sont utilisés efficacement. Mathématiquement, le schéma de contrôle doit garantir que <span class="em">&#8704; &#821;<sub>t</sub> r<sub>i</sub>(t) &#8776; R</span>.</p>
                        </li>
                        <li>
                            <p>Le mécanisme de contrôle de congestion doit être <span class="html">équitable</span>. La plupart des mécanismes de contrôle de congestion visent à atteindre une <span class="html">équité maximale-minimale</span>. Une allocation de taux de transmission aux sources est dite <span class="em">équitable maximale-minimale</span> si :</p>
                            <ul>
                                <li>
                                    <p>aucun liens dans le réseau n'est congestionné.</p>
                                </li>
                                <li>
                                    <p>Le taux alloué à la source <span class="em">j</span> ne peut pas être augmenté sans dimuner le taux alloué à une source <span class="em">i</span> dont l'allocation est inférieure au taux alloué à la source <span class="em">j</span>[Leboudec2008].</p>
                                </li>
                            </ul>
                        </li>
                    </ol>
                    <p>En fonction du réseau, une allocation d'<span class="em">équité max-min</span> n'existe pas toujours. En pratique, l'<span class="em">équité max-min</span> est un objectif idéal qui ne peut pas nécessairement être atteint. Lorsqu'il y a un seul goulot d'étranglement comme dans l'exple ci-dessus, l'<span class="em">équité max-min</span> implique que chaque source devrait se voir attribuer le même débit de transmission.</p>
                    <p>Pour visualiser les différentes allocations de taux, il est utile de considéré le graphe ci-dessous. Dans ce graphe, nous traçons sur l'<span class="em">axe des x</span> (resp. l'<span class="em">axe des y</span>) le taux alloué à l'hôte <span class="em">B</span> (resp. <span class="em">A</span>). Un point dans le graphe <span class="em">r<sub>B</sub>, r<sub>A</sub>)</span> correpsond à une allocation possible des taux de transmission. Comme il y a une liaison de 2 Mbps en goulot d'étranglement dans ce réseau, le graphe peut être divisé en deux régions. La partie inférieure gauche du graphe contient toutes les allocations <span class="em">(r<sub>B</sub>, r<sub>A</sub>)</span> telles que le goulot d'étranglement n'est pas congestionné (<span class="em">r<sub>A</sub> + r<sub>B</sub> &lt; 2</span>). La frontière droite de cette région est la <span class="html">ligne d'efficacité</span>, c'est-à-dire l'ensemble des allocations qui utilisent complètement le goulot d'étranglement (<span class="em">r<sub>A</sub> + r<sub>B</sub> = 2</span>). Enfin, la <span class="html">ligne d'équité</span> est l'ensemble des allocations équitables.</p>
                    <figure>
                        <img src="../images/taux_transmission_alloues_possibles.jpg" alt="">
                        <figcaption>Figure 4.54 : Taux de transmission alloués possibles.</figcaption>
                    </figure>
                    <p>Comme le montre le graphique ci-dessus, une allocation de débit peut être équitable mais pas efficace (par exemple, r<sub>A</sub> = 1, r<sub>B</sub> = 1) ou efficace mais pas équitable (par exemple, r<sub>A</sub> = 1.5, r<sub>B</sub> = 0.5). Idéalement, l'allocation devrait être à la fois équitable et efficace. Malheureusement, maintenir une telle allocation avec des fluctuations dans le nombre de flux qui utilisent le réseau est un problème difficile. De plus, il peut y avoir plusieurs milliers de connexions TCP ou plus qui passent par le même lien. Par exemple, les mesures effectuées sur le réseau <span class="em">Sprint</span> en 2004 ont signalé plus de 10000 connexions TCP actives sur un lien, voir <a href="https://research.sprintlabs.com/packstat/packetoverview.php" target="_blank">https://research.sprintlabs.com/packstat/packetoverview.php</a>. Des informations plus récentes sur les liens de backbone peuvent être obtenues à partir des mesures en temps réel de <span class="em">caida</span>, voir par exemple <a href="http://www.caida.org/data/realtime/passive/" target="_blank">http://www.caida.org/data/realtime/passive/</a>.</p>
                    <p>Pour faire face à ces fluctuations de la demande, qui entraînent des fluctuations de la bande passante disponible, les réseaux informatiques utilisent un schéma de contrôle de congestion. Ce schéma de contrôle de congestion devrait atteindre les trois objectifs énumérés ci-dessus. Certains schémas de contrôle de congestion reposent sur une étroite collaboration entre les hôtes de bout en bout et les routeurs, tandis que d'autres sont principalement implémentés sur les hôtes de bout en bout avec un soutien limité des routeurs.</p>
                    <p>Un schéma de contrôle de congestion peut être modifié comme un algorithme qui adapte le débit de transmission (<span class="em">r<sub>i</sub>(t)</span>) de l'hôte <span class="em">i</span> en fonction des commentaires reçus du réseau. Différents types de commentaires sont possibles. Le schéma de contrôle de congestion le plus simple est un feedback binaire [CJ1989] [Jacobson1988] où les hôtes apprennent simplement si le réseau est congestionné ou non. Certains schémas de contrôle de congestion permettent au réseau d'envoyer régulièrement à chaque hôte un débit de transmission alloué en Mbps [BF1995].</p>
                    <p>Concentrons-nous sur le schéma de feedbacks binaires qui est le plus largement utilisé aujourd'hui. De manière intuitive, le schéma de contrôle de congestion devrait réduire le débit de transmission d'un hôte lorsque la congestion a été détectée dans le réseau, afin d'éviter l'effondreemnt de la congestion. En outre, les hôtes devraient augmenter leur débit de transmission lorsque le réseau n'est pas congestionné. Sinon, les hôtes ne seraient pas en mesure d'utiliser efficacement le réseau. Le débit alloué à chaque hôte fluctue avec le temps, en fonction des commentaires reçus du réseau. La figure ci-dessous illustre l'évolution des débits de transmissions alloués à deux hôtes de nptre réseau simple. Initialement, deux hôtes ont une faible allocation, mais cela n'est pas efficace. Les allocations augmentent jusqu'à ce que le réseau devienne congestionné. À ce stade, les hôtes diminuent leur débit de transmission pour éviter l'effondrement de la congestion. Si le schéma de contrôle de congestion fonctionne bien, après un certain temps, les allocations devraient devenir à la fois équitables et efficaces.</p>
                    <figure>
                        <img src="../images/evolution_taux_transmission.jpg" alt="">
                        <figcaption>Figure 4.55 : Évolution des taux de transmission</figcaption>
                    </figure>
                    <p>Divers types d'algorithmes d'adaptation sont possibles. Dah Ming Chiu et Ray Jain ont analysé, dans [CJ1989], différents types d'algorithmes pouvant être utilisés par une source pour adapter son débit de transmission aux commentaires reçus du réseau. De manière intuitive, un tel algorithme d'adaptation de débit de transmision lorsque le réseau n'est pas congestionné (pour garantir que le réseau est utilisé efficacement) et diminue le débit de transmission lorque le réseau est congestionné (pour éviter l'effondrement de la congestion).</p>
                    <p>La forme la plus simple de commentaire que le réseau peut envoyer à une source est un feedback binaire (le réseau est congestionné ou non congestionné). Dans ce cas, un algorithme <span class="em">linéaire</span> d'adaptation de débit peut être exprimé comme suit :</p>
                    <ul>
                        <li>
                            <p><span class="em">rate(t + 1) = &#945;<sub>c</sub> + &#946;<sub>c</sub> * rate(t)</span> lorsque le réseau est congestionné.</p>
                        </li>
                        <li>
                            <p><span class="em">rate(t + 1) = &#946;<sub>N</sub> + B<sub>N</sub> * rate(t)</span> lorsque le réseau n'est pas congestionné.</p>
                        </li>
                    </ul>
                    <p>Avec un algorithme d'algorithme linéaire, <span class="em">&#946;<sub>C</sub></span>, <span class="em">&#945;<sub>N</sub></span>, <span class="em">&#946;<sub>C</sub></span> et <span class="em">&#946;<sub>N</sub></span> sont des constantes. L'analyse de [CJ1989] montre que pour être équitable et efficace, un tel mécanisme d'adaptation de taux binaire doit s'appuyer sur une <span class="html">augmentation additive</span> et une <span class="html">diminution multiplicative</span>. Lorsque le réseau n'est pas congestionné, les hôtes doivent augmenter lentement leur taux de transmission (&#946;<sub>N</sub> = 1 et &#945;<sub>N</sub> &gt; 0). Lorsque le réseau est congestionné, les hôtes doivent diminuer leur taux de transmission de manière multiplicative (&#946;<sub>C</sub> &lt; 1 et &#945;<sub>C</sub> = 0). Un tel algorithme d'adaptation de taux AIMD peut être implémenté en utilisant le pseudo-code ci-dessous :</p>
<pre><code># Additive Increase Multiplicative Decrease
if congestion :
    rate=rate*betaC # multiplicative decrease, betaC&lt;1
else :
    rate=rate+alphaN # additive increase, v0&gt;0</code></pre>
                    <hr>
                    <p>Note : Quel type de feedback binaire ?</p>
                    <p>Deux types de feedback binaire sont possibles dans les réseaux informatiques. Une première solution consiste à utiliser un feedback implicite. C'est la solution choisie pour TCP. Le schéma de contrôle de congestion de TCP [Jacobson1988] ne nécessite aucune coopération de la part du routeur. Il suppose simplement qu'ils utilisent des tampons et qu'ils rejettent les paquets en cas de congestion. Lorsqu'il n'y a pas de pertes, le réseau est considéré comme non congestionné. Cela implique que la congestion est la principale cause des pertes de paquets. C'est vrai dans les réseaux filaires, mais malheureusement pas toujours vrai dans les réseaux sans fil. Une autre solution consiste à utiliser un feedback explicite. C'est la solution proposée dans le schéma de contrôle de congestion DECBit [RJ1995] et utilisée dans les réseaux <span class="em">Frame Relay</span> et <span class="em">ATM</span>. Ce feedback explicite peut être implémenté de deux façons. Une première solution consisterait à définir un message spécial qui pourrait être envoyé par les routeurs aux hôtes lorsqu'ils sont congestionnés. Malheureusement, la génération de tels messages peut augmenter la quantité de congestion dans le réseau. Un tel paquet d'indication de congestion est donc découragé dans le RFC 1812. Une meilleure approche consiste à permettre aux routeurs intermédiaires d'indiquer, dans les paquets qu'ils transmettent, leur état de congestion actuel. Le feedback binaire peut être encodé en utilisant un bit dans l'en-tête du paquet. Avec un tel schéma, les routeurs congestionnés définissent un bit spécial dans les paquets qu'ils transmettent tandis que les routeurs non congestionnés laissent ce bit inchangé. L'hôte de destination renvoie l'état de congestion du réseau dans les accusés de réception qu'il envoie. Des détails sur une telle solution dans les réseaux IP peuvent être trouvés dans le RFC 3168. Malheureusement, à l'heure où ces lignes sont écrites, cette solution n'est toujours pas déployée malgré ses avantages potentiels.</p>
                    <hr>
                    <p>Le schéma de contrôle de congestion TCP a été initialement proposé par Van Jacobson dans [Jacobson1988]. La spécification actuelle peut êrte trouvée dans la RFC 5681. TCP s'appuie sur <span class="em">l'augmentation additive et la diminution multiplicative (AIMD pour <span lang="en">Additive Increase and Multiplicative Decrease</span>)</span>. Pour implémenter <span class="em">AIMD</span>, un hôte TCP doit être capable de contrôler son débit de transmission. Une première approche consisterait à utiliser des timers et à ajuster leurs temps d'expiration en fonction du taux imposé par AIMD. Malheureusement, la maintenance de tels timers pour un grand nombre de connexions TCP peut être difficile. À la place, Van Jacobson a remarqué que le taux de congestion de TCP peut être artificiellement contrôlé en limitant sa fenêtre d'envoi. Une connexion TCP ne peut pas envoyer de données plus rapidement que <span class="em">window / rtt</span> où <span class="em">window</span> est le maximum entre la fenêtre d'envoi de l'hôte et la fenêtre annoncée par le récepteur.</p>
                    <p>Le schéma de contrôle de congestion de TCP repose sur une <span class="html">fenêtre de congestion</span>. La valeur courante de la fenêtre de congestion (<span class="html">cwnd</span>) est stockée dans le TCB de chaque connexion TCP et la fenêtre qui peut être utilisée par l'émetteur est limitée par <span class="html">min(cwnd, rwin, swin)</span>, où <span class="html">swin</span> est la fenêtre d'envoi actuelle et <span class="html">rwin</span> la dernière fenêtre de réception reçue. La partie <span class="em">Additive Increase</span> du contrôle de congestion TCP augmente la fenêtre de congestion de <span class="em">MSS</span> octets à chaque temps aller-retour. Dans la littérature TCP, cette phase est souvent appelée phase <span class="html">d'évitement de congestion</span>. La partie <span class="em">Multiplicative Decrease</span> du contrôle de congestion TCP divise la valeur courante de la fenêtre de congestion une fois que la congestion a été détectée.</p>
                    <p>Lorsqu'une connexion TCP commence, l'hôte émetteur ne sait pas si la partie du réseau qu'il utilise pour atteindre la destination est congestionnée ou non. Pour éviter de causer trop de congestion, il doit commencer avec une petite fenêtre de congestion. [Jacobson1988] recommande une fenêtre initiale de MSS octets. Comme la partie d'augmentation additive du mécanisme de contrôle de congestion TCP incrémente la fenêtre de congestion de MSS octets à chaque temps aller-retour, la connexion TCP peut devoir attendre de nombreux temps aller-retour avant de pouvoir utiliser efficacement la bande passante disponible. C'est particulièrement important dans les environnements où le produit de la bande passante et du temps aller-retour est élevé. Pour éviter d'attendre trop de temps aller-retour avant d'atteindre une fenêtre de congestion suffisamment grande pour utiliser efficacement le réseau, le mécanisme de contrôle de congestion TCP inclut l'algorithme de <span class="html">démarrage lent (slow-start)</span>. L'objectif du <span class="em">démarrage lent</span> de TCP est d'atteindre rapidement une valeur acceptable pour la fenêtre de congestion. Pendant le <span class="em">démarrage lent</span>, la fenêtre de congestion est doublée à chaque temps aller-retour. L'algorithme de <span class="em">démarrage lent</span> utilise une variable supplémentaire dans le TCB : <span class="html" lang="en">shhtresh (slow-start threshold)</span>. Le <span class="html">seuil de démarrage lent</span> est une estimation de la dernière valeur de la fenêtre de congestion qui n'a pas causé de congestion. Il est initialisé à la fenêtre d'émission et est mis à jour après chaque événement de congestion.</p>
                    <p>En pratique, une implémentation TCP considère que le réseau est congestionné une fois qu'elle doit retransmettre un segment. Le mécanisme de contrôle de congestion TCP distingue deux types de congestion :</p>
                    <ul>
                        <li>
                            <p><span class="html">Une congestion légère</span> : TCP considère que le réseau est légèrement congestionné s'il reçoit trois accusés de réception dupliqués et effectue une retransmission rapide. Si la transmission rapide réussit, cela implique q'un seul segment a été perdu. Dans ce cas, TCP effectue une diminution multiplicative et la fenêtre de congestion est divisée par 3. Le seuil de démarrage lent est réglé sur la nouvelle valeur de la fenêtre de congestion.</p>
                        </li>
                        <li>
                            <p><span class="html">Une connexion sévère</span> : TCP considère que le réseau est fortement congestionné lorsque son timer de retransmission expire. Dans ce cas, TCP retransmet le premier segment, fixe le seuil de démarrage lent à 50% de la fenêtre de congestion. La fenêtre de congestion est réinitialisée à un segment. TCP effectue un démarrage lent.</p>
                        </li>
                    </ul>
                    <p>La figure ci-dessous illustre l'évolution de la fenêtre de congestion en cas de congestion sévère. Au début de la connexion, l'expéditeur effectue un démarrage lent jusqu'à ce que les premiers segments soient perdus et que le timer de retransmission expire. À ce moment-là, le seuil de démarrage lent est fixé à la moitié de la fenêtre de congestion actuelle et la fenêtre de congestion est réinitialisée à un segment. Les segments perdus sont retransmis lorsque l'expéditeur effectue à nouveau un démarrage lent jusqu'à ce que la fenêtre de congestion atteigne le seuil de démarrage lent. Il passe ensuite à l'évitement de congestion et la fenêtre de congestion augmente linéairement jusqu'à ce que les segments soient perdus et que le timer de retransmission expire...</p>
                    <figure>
                        <img src="../images/evolution_fenetre_congestion_TCP_congestion_severe.jpg" alt="">
                        <figcaption>Figure 4.56 : Évolution de la fenêtre de congestion TCP avec une congestion sévère</figcaption>
                    </figure>
                    <p>La figure ci-dessous illustre l'évolution de la fenêtre de congestion lorsque le réseau est légèrement congestionné et que tous les segments perdus peuvent être retransmis à l'aide d'une retransmission rapide. L'expéditeur commence avec un démarrage lent. Un segment est perdu mais est rapidement retransmis par une retransmission rapide. La fenêtre de congestion est divisée par 2 et l'expéditeur passe immédiatement en mode évitement de congestion, car il s'agissait d'une congestion légère.</p>
                    <figure>
                        <img src="../images/evolution_fenetre_congestion_TCP_reseau_legerement_congestionne.jpg" alt="">
                        <figcaption>Figure 4.57 : Évolution de la fenpetre de congestion de TCP lorsque le réseau est légèrement congestionné</figcaption>
                    </figure>
                    <p>La plupart des implémentations TCP mettent à jour la fenêtre de congestion lorsqu'elles reçoivent un accusé de réception. Si l'on suppose que le récepteur accusé réception de chaque segment reçu et que l'expéditeur ne transmet que des segments de taille MSS, le schéma de contrôle de congestion TCP peut être implémenté en utilisant le pseudo-code simplifié ci-dessous. Dans ce pseudo-code, nous supposons que TCP utilise des numéro de séquence et d'accusé de réception illimités. De plus, nous ne détaillons pas comment...</p>
<pre><code># Initialisation
cwnd = MSS;
ssthresh = swin;

# Ack arrival
if tcp.ack &gt; snd.una : # new ack, no congestion
    if cwnd &lt; ssthresh :
        # slow-start : increase quickly cwnd
        # double cwnd every rtt
        cwnd = cwnd + MSS
    else :
        # congestion avoidance : increase slowly cwnd
        # increase cwnd by one mss every rtt
        cwnd = cwnd + mss * (mss / cwnd)
else: # duplicate or old ack
    if tcp.ack == snd.una : # duplicate acknowledgement
    dupacks++
    if dupacks == 3 :
        retransmitsegment(snd.una)
        ssthresh=max(cwnd / 2, 2 * MSS)
        cwnd = ssthresh
    else:
        dupacks = 0
        # ack for old segment, ignored

Expiration of the retransmission timer :
    send(snd.una) # retransmit first lost segment
    sshtresh = max(cwnd / 2, 2 * MSS)
    cwnd = MSS</code></pre>
                    <p>De plus, lorsqu'une connexion TCP est restée inactive plus que son timer de retransmission actuel, elle doit réinitialiser sa fenêtre de congestion à la taille de la fenêtre de congestion qu'elle utilise lorsque la connexion débute, car elle ne connaît plus l'état de congestion actuel du réseau.</p>
                    <hr>
                    <p>Note : Fenêtre de congestion initiale :</p>
                    <p>Le mécanisme de contrôle de congestion TCP original proposé dans [Jacobson1988] recommandait que chaque connexion TCP commence en fixant <span class="em">cwnd = MSS</span>. Cependant, dans les réseaux à bande passante plus élevée d'aujourd'hui, l'utilisation d'une fenêtre de congestion initiale aussi petite affecte considérablement les performances pour les connexions TCP courtes, telles que celles utilisées par les serveurs web. Depuis la publication de la RFC 3390, les hôtes TCP sont autorisés à utiliser une fenêtre de congestion initiale d'environ 4 Ko, ce qui correspond à 3 segments dans de nombreux environnements.</p>
                    <hr>
                    <p>Grâce à son mécanisme de contrôle de congestion, TCCP adapte son taux de transmission aux pertes qui se produisent dans le réseau. Intuitivement, le taux de transmission TCP diminue lorsque le pourcentage de pertes augmente. Des chercheurs ont proposé des modèles détaillés qui permettent de prédire le débit d'une connexion TCP en cas de pertes [MSMO1997]. Pour avoir une certaine intuition sur les facteurs qui affectent les performances de TCP, considérons un modèle très simple. Ses hypothèses ne sont pas entièrement réalistes, mais il nous donne une bonne intuition sans nécessiter de mathématiques complexes.</p>
                    <p>Le <span class="em">cwnd</span> est ajusté après la retransmission du segment perdu par retransmission rapide. Des détails supplémentaires peuvent être trouvés dans la RFC 5681.</p>
                    <p>Ce modèle considère une connexion TCP hypothétique qui souffre de pertes de segments régulièrement espacées. Si <span class="em">p</span> est le taux de perte de segments, alors la connexion TCP transfère avec succès <span class="em">1/p - 1</span> segments et le segment suivant est perdu. Si nous ignorons le démarrage lent au début de la connexion, TCP dans cet environnement est toujours en évitement de congestion car n'y a que des pertes isolées qui peuvent être récupérées en utilisant une retransmission rapide. L'évolution de la fenêtre de congestion est donc comme indiquée dans la figure ci-dessous. Notez que l'axe des x représente le temps mesuré en unités d'un temps aller-retour, qui est supposé être constant dans le modèle, et l'axe des y représente la taille de la fenêtre de congestion mesurée en segments de taille MSS.</p>
                    <figure>
                        <img src="../images/evolution_fenetre_congestion_pertes_regulieres.jpg" alt="">
                        <figcaption>Figure 4.58 : Évolution de la fenêtre de congestion avec des pertes régulières</figcaption>
                    </figure>
                    <p>Comme les pertes sont régulièrement espacées, la fenêtre de congestion commence toujours à une certaine valeur (<span class="em">W/2</span>) et est incrémentée d'un MSS à chaque aller-retour jusqu'à ce qu'elle atteigne deux fois cette valeur (<span class="em">W</span>). À ce stade, un segment est retransmis et le cycle recommence. Si la fenêtre de congestion est mesurée en segments de taille MSS, un cycle dure <span class="em">W</span> aller-retour. La bande passante de la connexion TCP est le nombre d'octets qui ont été transmis pendant une période donnée. Pendant un cycle, le nombre de segments qui sont envoyés sur la connexion TCP est égal à l'aire du trapèze jaune dans la figure. Son aire est donc :</p>
<pre><code>aire = (w/2)<sup>2</sup> + 1/2 * (w/2)<sup>2</sup> = (3 * w<sup>2</sup>)/8</code></pre>
                    <p>Cependant, étant donné les pertes régulières que nous considérons, le nombre de segments qui sont envoyés entre deux pertes (c'est-à-dire pendant un cycle) est par définition égal à <span class="em">1/p</span>. Ainsi, <span class="em">W = &#8730;(8/3*p) = k/(&#830;(p))</span>. Le débit (en octets par seconde) de la connexion TCP est égal au nombre de segments transmis divisé par la durée du cycle :</p>
<pre><code>Throughput = (area * MSS) / time = ((3 * w<sup>2</sup>) / 8) / ((w/2) * rtt)
# où, après avoir éliminé W,
Throughput = &#8730;(3/2) * (MSS / (rtt * &#8730;(p)))</code></pre>
                    <p>Les modèles plus détaillés et l'analyse de simulations ont montré qu'un modèle de première ordre du débit TCP lorsque des pertes surviennent était <span class="em">Throughput &#8776; (k * MSS) / (rtt * &#8730;(p))</span>. Ceci est un résultat important que montre que :</p>
                    <ul>
                        <li>
                            <p>Les connexions TCP ayant un temps de trajet aller-retour (RTT) court peuvent atteindre un débit plus élevé que les connexions TCP ayant un RTT plus long lorsque des pertes surviennent. Cela implique que le mécanisme de contrôle de congestion de TCP n'est pas complètement équitable car il favorise les connexions ayant un RTT plus court.</p>
                        </li>
                        <li>
                            <p>Les connexions TCP utilisant une grande MSS peuvent atteindre un débit plus élevé que les connexions TCP utilisant une taille de MSS plus petite. Cela crée une autre source d'injustice entre les connexions TCP. Cependant, il convient de noter qu'aujourd'hui, la plupart des hôtes utilisent une MSS presque identique, d'environ 1460 octets.</p>
                        </li>
                    </ul>
                    <p>En général, le débit maximal qu'une connexion TCP peut atteindre dépend de sa taille maximale de fenêtre et du RTT s'il n'y a pas de pertes. S'il y a des pertes, cela dépend de la MSS, du RTT et du taux de perte.</p>
<pre><code>Throughput &lt; min(window / rtt, (k * MSS) / (rtt * &#8730;(p)))</code></pre>
                    <hr>
                    <p>Note : La ménagerie de contrôle de congestion TCP :</p>
                    <p>Le premier schéma de contrôle de congestion TCP a été proposé par Van Jacobson dans [Jacobson1988]. En plus d'avoir écrit l'article scientifique, Van Jacobson a également implémenté les mécanismes du démarrage lent et d'évitement de congestion dans la version 4.3 <span class="em">Tahoe</span> du système d'exploitation BSD Unix distribué par l'Université de Berkeley. Par la suite, il a amélioré le contrôle de congestion en ajoutant les mécanismes de retransmission rapide et de récupération rapide dans la version <span class="em">Reno</span> de 4.3 BSD Unix. Depuis lors, de nombreux chercheurs ont proposé, simulé et implémenté des modifcations du schéma de contrôle de congestion TCP. Certaines de ces modifications sont encore utilisées aujourd'hui, par exemple :</p>
                    <ul>
                        <li>
                            <p><span class="html">NewReno</span> (RFC 3782) qui a été proposé comme amélioration du mécanisme de récupération rapide dans l'implémentation <span class="em">Reno</span>.</p>
                        </li>
                        <li>
                            <p><span class="html">TCP Vegas</span>, qui utilise des changements dans le temps aller-retour pour estimer la congestion afin de l'éviter [BOP1994].</p>
                        </li>
                        <li>
                            <p><span class="html">CUBIC</span>, qui a été conçu pour les liens à large bande passante et est le schéma de contrôle de congestion par défaut dans le noyau Linux 2.6.19 [HRX2008].</p>
                        </li>
                        <li><span class="html">Compound TCP</span>, qui a été conçu pour les liens à haute bande passante est le schéma de contrôle de congestion par défaut dans plusieurs systèmes d'exploitation Microsoft [STBT2009].</li>
                    </ul>
                    <p>Une recherche dans la littérature scientifique (RFC 6077) révélera probablement plus de 100 variantes différentes du schéma de contrôle de congestion TCP. La plupart d'entre elles n'ont été évaluées que par des simulations. Cependant, l'implémentation TCP dans les noyaux Linux récents prend en charge plusieurs schémas de contrôle de congestion et de nouveaux peuvent être facilement ajoutés. Nous pouvons nous attendre à ce que de nouveaux schémas de contrôle de congestion TCP continuent toujours à apparaître.</p>
                    <hr>
                    <h3>4.4 Exercices :</h3>
                    <p>Cette section est divisée en deux parties. La première partie contient des exercices sur les principes des protocoles de transport, y compris TCP. La deuxième partie contient des défis de programmation pour les outils d'analyse de paquets afin d'observer le comportement des protocoles de transport.</p>
                    <h4>4.4.1 Principes :</h4>
                    <ol>
                        <li>
                            <p>Considérons le protocole de bit alterné (ABP) tel que décrit dans ce chapitre :</p>
                            <ul>
                                <li>
                                    <p>Comment le protocole récupère-t-il la perte d'un segment de données ?</p>
                                </li>
                                <li>
                                    <p>Comment le protocole récupère-t-il la perte d'un accusé de réception</p>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <p>Un étudiant a proposé d'optimisé le protocole d ebit alterné en ajoutant un accusé de réception négatif, c'est-à-dire que le récepteur envoie un segment de contrôle <span class="em">NAK</span> lorsqu'il reçoit un segment de données corrompu. Quel type d'informations devrait être placé dans ce segment de contrôle et comment l'émetteur devrait-il réagir lorsqu'il reçoit un tel <span class="em">NAK</span> ?</p>
                        </li>
                        <li>
                            <p>Les protocoles de transport s'appuient sur différents types de sommes de contrôel pour vérifier si les segments ont été affectés par des erreurs de transmission. Les sommes de contrôle les plus fréquemment utilisées sont :</p>
                            <ul>
                                <li>
                                    <p>La somme de contrôle Internet utilisée par UDP, TCP et d'autres protocoles Internet qui est définie dans la RFC 1071 et implémentée dans divers modules, par exemple <a href="http://ilab.cs.byu.edu/cs460/code/ftp/ichecksum.py" target="_blank">http://ilab.cs.byu.edu/cs460/code/ftp/ichecksum.py</a> pour une implémentation en Python.</p>
                                </li>
                                <li>
                                    <p>Les codes de véirification de redondance cyclique (CRC) de 16 bits ou de 32 bits qui sont souvent utilisés sur les disques, dans les archives zip et dans les protocoles de couche de liaison de donnés. Voir <a href="http://docs.python.org/library/binascii.html" target="_blank">http://docs.python.org/library/binascii.html</a> pour un module Python qui contient le CRC de 32 bits.</p>
                                </li>
                                <li>
                                    <p>La somme de contrôle Alder définie dans la RFC 2920 pour le protocole SCTP mais remplacée par un CRC dans la RFC 3309.</p>
                                </li>
                                <li>
                                    <p>La somme de contrôle Fletcher [Fletcher1982], voir <a href="http://drdobbs.com/database/184408761" target="_blank">http://drdobbs.com/database/184408761</a> pour les détails d'implémentation.</p>
                                </li>
                            </ul>
                            <p>En utilisant vos connaissances sur la somme de contrôle Internet, pouvez-vous trouver une erreur de transmission qui ne sera pas détectée par la somme de contrôle Internet</p>
                        </li>
                        <li>
                            <p>Les CRC sont des codes de détection d'erreurs efficaces qui sont capables de détecter :</p>
                            <ul>
                                <li>
                                    <p>Toutes les erreurs qui affectent un nombre impair de bits.</p>
                                </li>
                                <li>
                                    <p>Toutes les erreurs qui affectent une séquence de bits plus courte que la longueur du CRC.</p>
                                </li>
                            </ul>
                            <p>Effectuez des expériences avec une implémentation de CRC-32 pour vérifier que c'est effectivement le cas.</p>
                        </li>
                        <li>
                            <p>Les sommes de contrôle et les CRC ne doivent pas être confondus avec les fonctions de hachage sécurisées telles que MD5 définie dans la RFC 1321 ou SHA-1 décrite dans la RFC 4634. Les fonctions de hachage sécurisées sont utilisées pour s'assurer que les fichiers ou parfois les paquets/segments n'ont pas été modifiés. Les fonctions de hachage sécurisées visent à détecter les modifications malveillantes tandis que les sommes de contrôle et les CRC ne détectent que les erreurs de transmission aléatoires. Effectuez des expériences avec des fonctions de hachage telles que celles définies dans le module hashlib de Python à l'adresse <a href="http://docs.python.org/library/hashlib.html" target="_blank">http://docs.python.org/library/hashlib.html</a> pour vérifier que c'est effectivement le cas.</p>
                        </li>
                        <li>
                            <p>Une version du Alternating Bit Protocol prenant en charge des segments de longueur variable un en-tête qui contient les champs suivants :</p>
                            <ul>
                                <li>
                                    <p>un <span class="em">nombre</span> (0 ou 1)</p>
                                </li>
                                <li>
                                    <p>un champ <span class="em">length</span> qui indique la longueur des données</p>
                                </li>
                                <li>
                                    <p>un <span class="em">CRC</span></p>
                                </li>
                            </ul>
                            <p>Pour accélérer la transmission des segments, un étudiant propose de calculer le CRC sur la partie des données du segment mais pas sur l'en-tête. Que pensez-vous de cette optimisation</p>
                        </li>
                        <li>
                            <p>Sur les hôtes Unix, la commande <span class="html">ping(8)</span> peut être utilisée pour mesurer le temps de trajet aller-retour pour envoyer et recevoir des paquets depuis un hôte distant. Utilisez <span class="em">pring(8)</span> pour mesurer le temps de trajet aller-retour vers un hôte distant. Choisissez une destination lointaine par rapport à votre emplacement actuel, par exemple un petit serveur Web dans un pays éloigné. Il existe des implémentations de <span class="em">ping</span> dans divers langages, voir par exemple <a href="http://pypi.python.org/pypi/ping/0.2" target="_blank">http://pypi.python.org/pypi/ping/0.2</a> pour une implémantion de "<span class="em">ping</span>" en Python.</p>
                        </li>
                        <li>
                            <p>Comment définiriez-vous le timer de retransmission si vous implémentiez l'Alternating Bit Protocol pour échanger des fichiers avec un serveur tel que celui que vous avez mesuré ci-dessus ?</p>
                        </li>
                        <li>
                            <p>Quels sont les facteurs qui affectent les performances du Alternating Bit Protocol ?</p>
                        </li>
                        <li>
                            <p>Les liaisons sont souvent considérées comme symétriques, c'est-à-dire qu'elles offrent la même bande passante dans les deux directions. Les liaisons symétriques sont largement utilisées dans les réseaux locaux et dans le coeur d'Internet, mais il existe de nombreuses technologies de liaison asymétrique. L'exemple le plus courant est celui des divers types de technologies ADSL et CATV. Considérez une implémentation du Alternating Bit Protocol qui est utilisée entre deux hôtes qui sont directement connectés en utilisant une liaison asymétrique. Supposons qu'un hôte envoie des segments contenant 10 octets d'informations de contrôle et 90 octets de données et que les accusés de réception sont longs de 10 octets. Si le temps de trajet aller-retour est négligeable, quelle est la bande passante minimale requise sur le lien de retour pour s'assurer que la transmission des accusés de réception ne constitue pas un goulot d'étranglement ?</p>
                        </li>
                        <li>
                            <p>Dérivez une expression mathématique qui fournit le débit atteint par le protocole de bits alternés en supposant que :</p>
                            <ul>
                                <li>
                                    <p>Chaque segment contient <span class="em">D</span> octets de donénes et <span class="em">c</span> octets d'informations de contrôle.</p>
                                </li>
                                <li>
                                    <p>Chaque accusé de réception contient <span class="em">c</span> octets d'informations de contrôle.</p>
                                </li>
                                <li>
                                    <p>La bande passante des deux directions du lien est réglée sur <span class="em">B</span> bits par seconde.</p>
                                </li>
                                <li>
                                    <p>Le délai entre les deux hôtes est de <span class="em">s</span> secondes dans les deux sens.</p>
                                </li>
                            </ul>
                            <p>Le débit est défini comme la quantité d'unités de données de service (mesurée en octets) qui est transférée avec succès pendant une période de temps.</p>
                        </li>
                        <li>
                            <p>Considérez un Alternating Bit Protocol qui est utilisé sur une liaison qui souffre d'erreurs déterministes. Lorsque le taux d'erreur est fixé à <span class="em">1/p</span>, cela signifie que <span class="em">p - 1</span> bits sont transmis correctement et le <span class="em">p<sup>ème</sup></span> bit est corrompu. Discutez des facteurs qui affectent les performances du Alternating Bit Protocol sur une telle liaison.</p>
                        </li>
                        <li>
                            <p>Amazon propose le <span class="em">service de stockage S3</span> où les entreprises et les chercheurs peuvent stocker de grandes quantités d'informations et effectuer des calculs sur les informations stockées. Amazon permet aux utilisateurs d'envoyer des fichiers via Internet, mais également en envoyant des disques durs. Supposons q'un disque dur d'un téraoctet peut être livré dans les 24 heures à Amazon par service de messagerie. Quelle est la bande passante minimale requise pour égaler la bande passante de ce service de messagerie ?</p>
                        </li>
                        <li>
                            <p>Plusieurs grands opérateurs de centres de données (par exemple, Microsoft et Google) ont annoncé qu'ils installent des serveurs sous forme de conteneurs, chaque conteneur hébergeant jusqu'à 2000 serveurs. En supposant un conteneur avec 2000 serveurs et chaque serveur stockant 500 Go de données, combien de temps est nécessaire pour déplacer l'ensembles des données stockées dans un conteneur sur un lien de 10 Gbps ? Quelle est la bande passante d'un camion qui a besoin de 10 heures pour déplacer un conteneur d'un centre de données à un autre ?</p>
                        </li>
                        <li>
                            <p>Quelles sont les techniques utilisées par un émetteur Go-Back-N pour récupérer :</p>
                            <ul>
                                <li>
                                    <p>les erreurs de transmission</p>
                                </li>
                                <li>
                                    <p>les pertes de segments de données</p>
                                </li>
                                <li>
                                    <p>les pertes d'acquittements</p>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <p>Considérez une liaison de <span class="em">b</span> bits par seconde entre deux hôtes qui a un retard de propagation de <span class="em">t</span> secondes. Dérivez une formule qui calcule le temps écoulé entre la transmission du premier bit d'un segment de <span class="em">d</span> octets à partir d'un hôte émetteur et la réception du dernier bit de ce segment sur l'hôte récepteur.</p>
                        </li>
                        <li>
                            <p>Considérez un émetteur de type "Go-Back-N" et unrécepteur de type "Go-Back-N" qui sont directement connectés avec une liaison de 10 Mbps qui a un retard de propagation de 100 millisecondes. Supposons que le timer de retransmission est réglé sur trois secondes. Si la fenêtre a une longueur de 4 segments, dessinez un diagramme de séquence temporelle montrant la transmission de 10 segments (chaque segment contient 10000 bits) :</p>
                            <ul>
                                <li>
                                    <p>quand il n'y a pas de pertes</p>
                                </li>
                                <li>
                                    <p>lorsque les trosième et septième segments sont perdus</p>
                                </li>
                                <li>
                                    <p>lors les deuxième, quatrième, sixième, huitième,... acquittements sont perdus</p>
                                </li>
                                <li>
                                    <p>lorsque les troisième et quatrième segments de données sont réordonnés (c'est-à-dire que le quatrième arrive avant le troisième)</p>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <p>Même question lors de l'utilisation de "Selective Repeat" au lieu de "Go-Back-N". Notez que la réponse n'est pas nécessairelent la même.</p>
                        </li>
                        <li>
                            <p>Considérons deux serveurs haut de gamme connectés directement en utilisant une interface de 10 Gbps. Si le délai entre les deux serveurs est d'une milliseconde, quel est le débit qui peut être atteint par un protocole de transport qui utilise des segments de 10000 bits et une fenêtre de :</p>
                            <ul>
                                <li>
                                    <p>un segment</p>
                                </li>
                                <li>
                                    <p>dix segments</p>
                                </li>
                                <li>
                                    <p>cent segments</p>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <p>Considérons deux serveurs connectés directement en utilisant un lien de <span class="em">b</span> bits par seconde avec un temps aller-retour de <span class="em">r</span> secondes. Les deux serveurs utilisent un protocole de transport qui envoie des segments contenant <span class="em">s</span> octets et des acquittements composés de <span class="em">a</span> octets. Pouvez-vous dériver une formule qui calcule la plus petite fenêtre (mesurée en segments) nécessaire pour garantir que les serveurs pourront utiliser complètement le lien ?</p>
                        </li>
                        <li>
                            <p>Même question que ci-dessus si les deux serveurs sont connectés via une liaison asymétrique qui transmet <span class="em">bu</span> bits par seconde dans la direction utilisée pour envoyer les segments de données et <span class="em">bd</span> bits par seconde dans la direction utilisée pour envoyer les accusés de réception.</p>
                        </li>
                        <li>
                            <p>Le <span class="html">protocole de transfert de fichiers trivial (TFTP pour Trivial File Transfer Protocol)</span> est un protocole de transfert de fichiers très simple qui est souvent utilisé des hôtes sans disque lorsqu'ils démarrent à partir d'un serveur. Lire la spécification TFP dans la RFC 1350 et expliquer comment TFTP récupère des erreurs et des pertes de transmission.</p>
                        </li>
                        <li>
                            <p>Est-il possible pour un récepteur Go-Back-N d'interopérer avec un émetteur à "Selective repeat" ? Justifiez votre réponse.</p>
                        </li>
                        <li>
                            <p>Est-il possible pour un récepteur à "Selective repeat" d'interopérer avec un émetteur Go-Back-N ? Justifiez votre réponse.</p>
                        </li>
                        <li>
                            <p>Les mécanismes de Go-Back-N et de Selective repeat décrits dans le livre reposent exclusivement sur des accusés de réception cumulatifs. Cela implique qu'un récepteur retourne toujours à l'envoyeur des informations sur le dernier segment qui a été reçu dans l'ordre. En cas de pertes ou de réarrangements fréquents, un récepteur à Selective repeat pourrait retourner plusieurs fois le même accusé de réception cumulatif. Pouvez-vous penser à d'autres types d'accusés de réception qui pourraient être utilisés par un récepteur à Selective repeat pour fournir des informations supplémentaires sur les segments hors-séquence qu'il a reçus ? Concevez de tels accusés de réception et expliquez comment l'émetteur devrait réagir à la réception de ces informations.</p>
                        </li>
                        <li>
                            <p>Le <span class="em">goodput (débit utile)</span> atteint par un protocole de transport est généralement défini comme le nombre de bytes de la couche application échangés par unité de temps. Quels sont les facteurs qui peuvent influencer le goodput atteint par un protocole de transport donné ?</p>
                        </li>
                        <li>
                            <p>Lorsqu'il est utilisé avec IPv4, le protocole de contrôle de transmission (TCP) attache 40 octets d'informations de contrôle à chaque segment envoyé. En supposant une fenêtre infinie et aucune perte ni erreur de transmission, dérivez une formule qui calcule le débit TCP maximal en fonction de la taille des segments qui sont envoyés.</p>
                        </li>
                        <li>
                            <p>Un émetteur Go-Back-N utilise une taille de <span class="em">n</span> bits. Combien de segments peut-il envoyer sans recevoir d'accusé de réception</p>
                        </li>
                        <li>
                            <p>Considérez la situation suivante. Un récepteur Go-Back-N a envoyé une fenêtre complète de segments de données. Tous les segments ont été reçus correctement et dans l'ordre par le récepteur, mais tous les accusés de réception retournés ont été perdus. Montrez en utilisant un diagramme de séquence temporelle (par exemple, en considérant une fenêtre de quatre segments) ce qui se passe dans ce cas. Pouvez-vous corriger le problème sur l'émetteur Go-Back-N ?</p>
                        </li>
                        <li>
                            <p>Même question que ci-dessus, mais supposez maintenant que l'émetteur et le récepteur implémentent la Selective repeat. Notez que la réponse sera différente de la question précédente.</p>
                        </li>
                        <li>
                            <p>Considérez un transport qui prend en charge une fenêtre de cent segments de 1250 octets. Quelle est la bande passante maximale que ce protocole peut atteindre si le temps aller-retour est fixé à une seconde ? Que se passe-t-il sin au lieu d'annoncer une fenêtre de cent segments, le récepteur décide d'annoncer une fenêtre de 10 segments ?</p>
                        </li>
                        <li>
                            <p>Pour comprendre le fonctionnement du mécanisme de contrôle de congestion TCP, il est utile de dessiner des diagrammes de séquence temporelle. Considérons un scénario simple d'un client Web connecté à Internet qui souhaite récupérer une simple page Web à partir d'un serveur Web distant. Pour simplifier, nous supposerons que le délai entre le client et le serveur est de 0,5 seconde et que les temps de transmission de paquets sur le client et les serveurs sont négligeables (par exemple, ils sont tous deux connectés à un réseau de 1 Gbit/s). Nous supposerons égalemnt que le client et le serveur utilisent des segments de 1 Kbyte.</p>
                            <ol>
                                <li>
                                    <p>Calculez le temps nécessaire pour ouvrir une connexion TCP, envoyer une requête HTTP et récupérer une page Web de 16 KBytes. Cette taille de page est typique des résultats renvoyés par les moteurs de recherche tels que Google ou Bing. Un facteur important de ce délai est la taille initiale de la fenêtre de congestion TCP sur le serveur. Supposons d'abord que la fenêtre initiale soit définie à 1 segmpent selon la RFC 2001, à 4 KBytes (c'est-à-dire 4 segments dans ce cas) comme proposé dans la RFC 3390 ou à 16 KBytes comme proposé dans un article récent.</p>
                                </li>
                                <li>
                                    <p>Effectuez la même analyse avec une fenêtre initiale d'un segment si le troisième segment envoyé par le serveur est perdu et que le délai de retransmission est fixé et réglé à 2 secondes.</p>
                                </li>
                                <li>
                                    <p>Même question que ci-dessus, mais supposons maintenant que le sixième segment est perdu.</p>
                                </li>
                                <li>
                                    <p>Même question que ci-dessus, mais considérez maintenant la perte des deuxième et septième accusés de réception envoyés par le client.</p>
                                </li>
                                <li>
                                    <p>Est-ce que l'analyse ci-dessus change si la fenêtre initiale est définie à 16 KBytes au lieu d'un segment ?</p>
                                </li>
                            </ol>
                        </li>
                        <li>
                            <p>Plusieurs MBytes ont été envoyés sur une connexion TCP et elle reste inactive pendant plusieurs minutes. Discutez des valeurs qui devraient être utilisées pour la fenêtre de congestion, le seuil de démarrage lent et les timers de retransmission.</p>
                        </li>
                        <li>
                            <p>Pour fonctionner de manière fiable, un protocole de transport qui utilise Go-Back-N (resp. Selective repeat) ne peut pas utiliser une fenêtre plus grande que <span class="em">2<sup>n</sup> - 1</span>( resp. <span class="em">2<sup>n-1</sup></span>) segments. Cettr limitation affecte-t-elle TCP ? Expliquez votre réponse.</p>
                        </li>
                        <li>
                            <p>Considérons le réseau simple représenté dans la figure ci-dessous. Dans ce réseau, le routeur entre le client et le serveur ne peut stocker sur chaque interface de sortie qu'un seul paquet en plus du paquet qu'il est en train de transmettre. Il ignore tous les paquets qui arrivent lorsque son tampon est plein. En supposant que vous pouvez négliger le temps de transmission des accusés de réception et que le serveur utilise une fenêtre initiale d'un segment et a un timer de retransmission réglé sur 500 millisecondes, combien de temps est nécessaire pour transmettre 10 segments du client au serveur. Les performances augmentent-elles si le serveur utilise une fenêtre initiale de 16 segments à la place ?</p>
                            <figure>
                                <img src="../images/reseau_simple.jpg" alt="">
                                <figcaption>Figure 4.59 : Réseau simple</figcaption>
                            </figure>
                        </li>
                        <li>
                            <p>La figure ci-dessous décrit l'évolution de la fenêtre de congestion d'une connexion TCP. Pouvez-vous trouver les raisons des trois événements marqués sur la figure ?</p>
                            <figure>
                                <img src="../images/evolution_fenetre_congestion.jpg" alt="">
                                <figcaption>Figure 4.60 : Évolution de la fenêtre de congestion</figcaption>
                            </figure>
                        </li>
                        <li>
                            <p>La figure ci-dessous décrit l'évolution de la fenêtre de congestion d'une connexion TCP. Pouvez-vous trouver les raisons des trois événements marqués sur la figure ?</p>
                            <figure>
                                <img src="../images/evolution_fenetre_congestion_2.png" alt="">
                                <figcaption>Figure 4.61 : Évolution de la fenêtre de congestion</figcaption>
                            </figure>
                        </li>
                        <li>
                            <p>Un serveur web sert principalement des pages HTML qui tiennent dans 10 segments TCP. En supposant que le temps de transmission de chaque segment peut être négligé, calculez le temps total de transfert d'une telle page (en temps aller-retour) en supposant que :</p>
                            <ul>
                                <li>
                                    <p>la pile TCP utilise une taille de fenêtre initiale de 1 segment;</p>
                                </li>
                                <li>
                                    <p>la pile TCP utilise une taille de fenêtre initiale de trois segments.</p>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <p>La RFC 3168 définit des mécanismes qui permettent aux routeurs de marquer les paquets en définissant un bit dans l'en-tête du paquet lorsqu'ils sont congestionnés. Lorsqu'un destinataire TCP reçoit une telle marque dans un paquet, il renvoie la marque de congestion à la source qui réagit en réduisant de moitié sa fenêtre de congestion et en effectuant un évitement de congestion. Considérez une connexion où le quatrième segment de données rencontre une congestion. Comparez le délai de transmisison de 8 seglents dans un réseau où les routeurs rejettent les paquets pendant la congestion et un réseau où les routeurs marquent les paquets pendant la congestion.</p>
                        </li>
                    </ol>
                    <h3>4.5 Pratique :</h3>
                    <ol>
                        <li>
                            <p>L'interface de socket vous permet d'utiliser le protocole UDP sur un hôte Unix. UDP fournit un service non fiable sans connexion qui théoriquement vous permet d'envoyer des SDU allant jusqu'à 64 Ko.</p>
                            <ul>
                                <li>
                                    <p>Implémentez un petit client UDP et un petit serveur UDP (en python, vous pouvez partir de l'exemple fourni dans <a href="http://docs.python.org/library/socket.html" target="_blank">http://docs.python.org/library/socket.html</a> mais vous pouvez également utiliser C ou Java).</p>
                                </li>
                                <li>
                                    <p>Exécutez le client et les serveurs sur des postes de travail différents pour déterminer expérimentalement la plus grande SDU prise en charge par votre langage et votre OS. Si possible, utilisez des langages et des systèmes d'exploitation différents dans chaque groupe.</p>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <p>En utilisant l'interface de socket, implémentez, sur le service non fiable sans connexion fourni par UDP, un client simple qui envoie le message suivant illustré dans la figure ci-dessous.</p>
                            <figure>
                                <img src="../images/format_SDU_simple.png" alt="">
                                <figcaption>Figure 4.62 : format de SDU simple</figcaption>
                            </figure>
                            <p>Dans ce message, les indicateurs de bits doivent être réglés sur <span class="em">01010011b</span>, la valeur du champ de 16 bits doit être la racine carrée de la valeur contenue dans le champ de 32 bits, la chaîne de caractères doit être une représentation ASCII (sans aucun octet de remplissage) du nombre contenu dans le champ de caractères de 32 bits. Les 16 derniers bits du message contiennent une somme de contrôle Internet qui a été calculée sur l'ensemble du message.</p>
                            <p>À la réception d'un message, le serveur vérifie que :</p>
                            <ul>
                                <li>
                                    <p>le drapeau a la valeur correcte</p>
                                </li>
                                <li>
                                    <p>l'entier de 32 bits est le carré de l'entier de 16 bits</p>
                                </li>
                                <li>
                                    <p>la chaîne de caractères est une représentation ASCII de l'entier de 32 bits</p>
                                </li>
                                <li>
                                    <p>la somme de contrôle Internet est correcte</p>
                                </li>
                            </ul>
                            <p>Si la vérification réussit, le serveur renvoie une SDU contenant <span class="em">11111111b</span>. Sinon, il renvoie <span class="em">01010101b</span>.</p>
                            <p>Votre implémentation doit pouvoir fonctionner sur des machines low endian et big endian. Si vous avez accès à différents types de machines (par exemple, des ordinateurs portables x86 et des serveurs SPARC), essayez de faire fonctionner votre implémentation sur les deux types de machines.</p>
                        </li>
                        <li>
                            <p>La bibliothèque de sockets est également utilisée pour développer des applications au-dessus du service de flux de bytes fiable fourni par TCP. Nous avons installé sur le serveur <span class="em">cnp3.info.ucl.ac.be</span> un serveur simple qui fournit un service client-serveur simple. Le service fonctionne comme suit :</p>
                            <ul>
                                <li>
                                    <p>le serveur écoute sur le port <span class="em">62141</span> pour une connexion TCP</p>
                                </li>
                                <li>
                                    <p>lors de l'établissement d'une connexion TCP, le serveur envoie un entier en utilisant le format TLV suivant :</p>
                                    <ul>
                                        <li>
                                            <p>les deux premiers bits indiquent le type d'information (01 pour ASCII, 10 pour booléen)</p>
                                        </li>
                                        <li>
                                            <p>les six prochains bits indiquent la longueur de l'information (en octets)</p>
                                        </li>
                                        <li>
                                            <p>Un TLV ASCII a une longueur variable et les octets suivants contiennent un caractère ASCII par octet. Un TLV booléen a une longueur d'un octet. L'octet est défini sur <span class="em">00000000b</span> et <span class="em">00000001b</span> pour faux.</p>
                                        </li>
                                    </ul>
                                </li>
                                <li>
                                    <p>le serveur renvoie un TLV contenant vrai si l'entier était correct et un TLC contenant faux sinon et ferme la connexion TCP</p>
                                </li>
                            </ul>
                            <p>Implémentez un client pour interagir avec ce serveur en C, Java ou Python.</p>
                        </li>
                        <li>
                            <p>Il est maintenant temps de mettre en place un petit protocole de transport. Le protocole utilise une fenêtre coulissante pour transmettre plusieurs segments sans être contraint d'attendre un accusé de réception. Votre implémentation doit prendre en charge une fenêtre coulissante de taille variable, car l'autre extrémité du flux peut envoyer sa taille de fenêtre maximale. La taille de la fenêtre est encodée en tant qu'entier non signé sur trois bits.</p>
                            <p>Le protocole identifie les segments de données en utilisant des numéros de séquence. Le numéro de séquence du premier segment doit être 0. Il est incrémenté de un pour chaque nouveau segment. Le récepteur doit reconnaître les segments livrés en envoyant un segment ACK. Le champ <span class="em">numéro de séquence</span> du prochain segment attendu dans la séquence par le récepteur. Le flux de données est unidirectionnel, ce qui signifie que l'émetteur ne transmet que des segments ACK.</p>
                            <p>Pour traiter avec les pertes de segments, le protocole doit implémenter une technique de récupération telle que Go-Back-N ou Selective repeat et utiliser des timers de retransmission. Vous pouvez sélectionner la technique qui convient le mieux à vos besoins et partir d'une technique simple que vous améliorez par la suite.</p>
                        </li>
                    </ol>
                    <figure>
                        <img src="../images/format_segment.png" alt="">
                        <figcaption>Figure 4.62 : format de segment</figcaption>
                    </figure>
                    <p>Ce format de segment contient les champs suivants :</p>
                    <ul>
                        <li>
                            <p><span class="html">Type</span> : type de segment :</p>
                            <ul>
                                <li>
                                    <p>0x1 segment DATA.</p>
                                </li>
                                <li>
                                    <p>0x2 segment ACK.</p>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <p><span class="html">WIN</span> : la taille de la fenêtre actuelle (un entier encodé sur 3 bits). Dans les segments DATA, ce champ indique la taille de la fenêtre d'envoi de l'émetteur. Dans les segments ACK, ce champ indique la valeur actuelle de la fenêtre de réception.</p>
                        </li>
                        <li>
                            <p><span class="html">Sequence</span> : Numéro de séquence (entier non signé sur 8 bits), commence à 0. Le numéro de séquence est incrémenté de 1 pour chaque nouveau segment DATA envoyé par l'émetteur. Dans un segment ACK, le champ de séquence contient le numéro de séquence du prochain segment attendu dans la séquence par le récepteur.</p>
                        </li>
                        <li>
                            <p><span class="html">Length</span> : longueur du payload en multiples d'un octet. Tous les segments DATA contiennent une charge utile avec 512 octets de données, sauf le dernier segment DATA qui peut être plus court. La réception d'un segment DATA dont la longueur est différente de 512 indique la fin du transit de données.</p>
                        </li>
                        <li>
                            <p><span class="html">Payload</span> : les données à envoyer.</p>
                        </li>
                    </ul>
                    <p>Le client et le serveur échangent des datagrammes UDP qui contiennent les segments DATA et ACK. Ils doivent fournir une interface en ligne de commande qui permet de transmettre un fichier binaire et prendre en charge les paramètres suivants :</p>
<pre><code>sender &lt;destination_DNS_name&gt; &lt;destination_port_number&gt; &lt;window_size&gt; &lt;input_file&gt;
receiver &lt;listening_port_number&gt; &lt;window_size&gt; &lt;output_file&gt;</code></pre>
                    <p>Pour tester les réactions de votre protocole contre les erreurs et les pertes, vous pouvez utiliser un générateur de nombres aléatoires pour abandonner probabilistiquement les segments reçus et introduire des délais aléatoires à l'arrivée d'un segment.</p>
                    <h4>4.5.1 Analyse de la trace de paquets :</h4>
                    <p>Lors du débogage de problèmes de réseau ou pour analyser des problèmes de performance, il est parfois utile de capturer les segments échangés entre deux hôtes et de les analyser.</p>
                    <p>Plusieurs outils d'analyse de trace de paquets sont disponibles, soit comme des outils commerciaux ou open source. Ces outils sont capables de capturer tous les paquets échangés sur une liaison. Bien sûr, la capture de paquets nécessite des privilèges administrateur. Ils peuvent également analyser le contenu des paquets capturés et afficher des informations à leur sujet. Les paquets capturés peuvent être stockés dans un fichier pour une analyse hors ligne.</p>
                    <p><span class="html">tcpdump</span> est probablement l'un des logiciels de capture de paquets les plus connus. Il peut à la fois capturer des paquets et afficher leur contenu. <span class="em">tcpdump</span> est un outil basé sur du texte qui peut afficher la valeur des champs les plus importants des paquets capturés. Des informations supplémentaires sur <span class="em">tcpdump</span> peuvent être trouvées dans <span class="em">tcpdump(1)</span>. Le texte ci-dessous est un exemple de sortie de <span class="em">tcpdump</span> pour les premiers segments TCP échangés lors d'un transfert scp entre deux hôtes.</p>
<pre><code>21:05:56.230737 IP 192.168.1.101.54150 &gt; 130.104.78.8.22: S 1385328972:1385328972(0) win 65535 &lt;mss
21:05:56.251468 IP 130.104.78.8.22 &gt; 192.168.1.101.54150: S 3627767479:3627767479(0) ack 1385328973
21:05:56.251560 IP 192.168.1.101.54150 &gt; 130.104.78.8.22: . ack 1 win 65535 &lt;nop,nop,timestamp 274527749&gt;
21:05:56.279137 IP 130.104.78.8.22 &gt; 192.168.1.101.54150: P 1:21(20) ack 1 win 49248 &lt;nop,nop,timestamp 274527749&gt;
21:05:56.279241 IP 192.168.1.101.54150 &gt; 130.104.78.8.22: . ack 21 win 65535 &lt;nop,nop,timestamp 274527749&gt;
21:05:56.279534 IP 192.168.1.101.54150 &gt; 130.104.78.8.22: P 1:22(21) ack 21 win 65535 &lt;nop,nop,timestamp 274527749&gt;
21:05:56.303527 IP 130.104.78.8.22 &gt; 192.168.1.101.54150: . ack 22 win 49248 &lt;nop,nop,timestamp 1212093357&gt;
21:05:56.303623 IP 192.168.1.101.54150 &gt; 130.104.78.8.22: P 22:814(792) ack 21 win 65535 &lt;nop,nop,&gt;</code></pre>
                    <p>Vous pouvez facilement reconnaître dans la sortie ci-dessus le segment <span class="em">SYN</span> contenant les options <span class="html">MSS</span>, <span class="html">window scale</span>, <span class="html">timestamp</span> et <span class="html">sackOK</span>, le segment <span class="em">SYN+ACK</span> dont l'option <span class="html">wscale</span> utilise le scaling de fenêtre pour cette connexion, puis les premiers segments échangés sur la connexion.</p>
                    <p><span class="html">Wireshark</span> est plus récent que <span class="em">tcpdump</span>. Il a évolué à partir du logiciel d'analyse de trace de paquets <span class="html">Ethereal</span>. Il peut être utilisé comme outil texte comme <span class="em">tcpdump</span>. Pour une connexion TCP, <span class="em">wireshark</span> fournirait presque la même sortie que <span class="em">tcpdump</span>. Le principal avantage de <span class="em">wireshark</span> est qu'inclut également une interface utilisateur graphique qui permet d'effectuer différents types d'analyses sur une trace de paquets.</p>
                    <p>La fenêtre de wireshark est divisée en trois parties. La partie supérieure de la fenêtre est un résumé des premiers paquets de la trace. En cliquant sur l'une des lignes, vous pouvez afficher le contenu détaillé de ce paquet dans la partie centrale de la fenêtre. Le milieu de la fenêtre vous permet d'inspecter tous les champs du paquet capturé. La partie inférieure de la fenêtre est la représentation hexadécimale du paquet, avec le champ sélectionné dans la fenêtre du milieu qui est mis en évidence.</p>
                    <p>Wireshark est très bon pour afficher des paquets, mais il contient également plusieurs outils d'analyse qui peuvent être très utiles. Le premier outil est <span class="html">Follow TCP stream</span>. Il fait partie du menu "<span class="em">Analyse</span>" et permet de réassembler d'afficher tous les payloads échangés lors d'une connexion TCP. Cet outil peut être utile si vous avez besoin d'analyser, par exemple, les commandes échangées lors d'une session SMTP.</p>
                    <figure>
                        <img src="../images/wireshark_fenetre_defaut.jpg" alt="">
                        <figcaption>Figure 4.64 : Wireshark : fenêtre par défaut</figcaption>
                    </figure>
                    <p>Le deuxième outil est le graphique de flux qui fait partie du menu "<span class="em">Statistiques</span>". Il fournit un diagramme de séquence temporelle des paquets échangés avec des commentaires sur le contenu des paquets. Voir ci-dessous pour un exemple.</p>
                    <figure>
                        <img src="../images/wireshark_graphe_flux.png" alt="">
                        <figcaption>Figure 4.65 : Wireshark : graphique de flux</figcaption>
                    </figure>
                    <p>Le troisième ensemble d'outils est constitué des outils de graphique de flux TCP qui font partie du menu "<span class="em">Statistiques</span>". Ces outils permettent de tracer différents types d'informations extraites des segments échangés lors d'une connexion TCP. Un premier graphique intéressant est le <span class="html">graphique de numéro de séquence</span> qui montre l'évolution du champ de numéro de séquence des segments capturés dans le temps. Ce graphique peut être utilisé pour détecter graphiquement des retransmissions.</p>
                    <figure>
                        <img src="../images/wireshark_graphe_numero_sequence.png" alt="">
                        <figcaption>Figure 4.66 : graphique de numéro de séquence</figcaption>
                    </figure>
                    <p>Un deuxième graphique intéressant est le graphique du temps aller-retour qui montre l'évolution du temps aller-retour en fonction du temps. Ce graphique peut être utilisé pour vérifier si le temps aller-retour reste stable ou non. Notez que depuis une trace de paquets, Wireshark peut tracer deux graphiques de temps aller-retour, un pour le flux du client vers le serveur et l'autre pour le sens inverse. Wireshark affichera le graphique de temps aller-retour qui correspond au paquet sélectionné dans la fenêtre supérieure de Wireshark.</p>
                    <figure>
                        <img src="../images/wireshark_graphe_round-trip-time.png" alt="">
                        <figcaption>Figure 4.67 : Wireshark : graphe du temps aller-retour</figcaption>
                    </figure>
                    <h4>Émulation de reséau avec Netkit :</h4>
                    <p><span class="html">Netkit</span> est un émulateur de réseau basé sur User Mode Linux. Il permet de mettre en place facilement un réseau émulé de machines Linux, qui peuvent agir comme des hôtes finaux ou des routeurs.</p>
                    <hr>
                    <p>Note : Où puis-je trouver Netkit ?</p>
                    <p>Netkit est disponible sur <a href="http://www.netkit.org/" target="_blank">http://www.netkit.org/</a>. Les fichiers peuvent être téléchargés depuis <a href="http://wiki.netkit.org/index.php/Download_Official" target="_blank">http://wiki.netkit.org/index.php/Download_Official</a>, et les instructions d'installation sont disponibles ici : <a href="http://wiki.netkit.org/download/netkit/INSTALL" target="_blank">http://wiki.netkit.org/download/netkit/INSTALL</a>.</p>
                    <p>Il y a deux façons d'utiliser Netkit : la méthode manuelle et l'utilisation de laboratoires pré-configurés. Dans le premier cas, vous démarrez et contrôlez chaque machine individuellement, en utilisant les commandes commençant par un "<span class="em">v</span>" (pour machine virtuelle). Dans le second cas, vous pouvez démarrer tout un réseau en une seule opération. Les commandes de contrôle du laboratoire commencent par un "<span class="em">l</span>". Les pages de manuel de ces commandes sont disponibles sur <a href="http://wiki.netkit.org/man/man7/netkit.7.html" target="_blank">http://wiki.netkit.org/man/man7/netkit.7.html</a>.</p>
                    <p>Vous devez être attentif à ne pas oublier d'arrêter vos machines virtuelles et laboratoires, en utilisant soit <span class="html">vhalt</span>, soit <span class="html">lhalt</span>.</p>
                    <hr>
                    <p>Un laboratoire Netkit est simplement un répertoire contenant au moins un fichier de configuration appelé <span class="em">lab.conf</span>, et un répertoire pour chaque machine virtuelle. Dans le cas du laboratoire disponible sur iCampus, le réseau est composé de deux ordinateurs, <span class="em">pc1</span> et <span class="em">pc2</span>, tous deux connectés à un routeur <span class="em">r1</span>. Le fichier lab.conf contient les lignes suivantes :</p>
<pre><code>pc1[0]=A
pc2[0]=B
r1[0]=A
r1[1]=B</code></pre>
                    <p>Cela signifie que <span class="em">pc1</span> et <span class="em">r1</span> sont connectés à un "LAN virtuel" nommé <span class="em">A</span> via leur interface <span class="em">eth0</span>, tandis que <span class="em">pc2</span> et <span class="em">r1</span> sont connectés au "LAN virtuel" <span class="em">B</span> via respectivement leurs interfaces <span class="Em">eth0</span> et <span class="em">eth1</span>.</p>
                    <p>Le répertoire du laboratoire peut contenir des fichiers facultatifs. Dans le laboratoire qui vous est fourni, le fichier "pc1.startup" contient les instructions de shell à exécuter au démarrage de la machine virtuelle. Dans ce cas particulier, le script configure l'interface <span class="em">eth0</span> pour permettre les échanges de trafic entre <span class="em">pc1</span> et <span class="em">r1</span>, ainsi que l'entrée de la table de routage pour rejoindre <span class="em">pc2</span>.</p>
                    <p>Le démarrage d'un laboratoire consiste donc simplement à décompresser l'archive fournie, à entrer dans le répertoire du laboratoire et à taper <span class="html">Istart</span> pour démarrer le réseau.</p>
                    <hr>
                    <p>Note : Partage de fichiers entre les machines virtuelles et l'hôte :</p>
                    <p>Les machines virtuelles peuvent accéder au répertoire du laboratoire auquel elles appartiennent. Ce répertoire est monté dans leur système de fichiers sous le chemin <span class="em">/hostlab</span>.</p>
                    <hr>
                    <p>Dans le laboratoire netkit (<span class="em">exercises/netkit/netkit_lab_2hosts_1tr_ipv4.tar.tar.gz</span>), vous pouvez trouver une application client/serveur Python simple qui établit des connexions TCP. N'hésitez pas à réutiliser ce code pour effectuer vos analyses.</p>
                    <hr>
                    <p>Note : outils Netkit :</p>
                    <p>Comme les machines virtuelles exécutent Linux, des outils de réseau standard tels que <span class="html">hping</span>, <span class="html">tcpdump</span>, <span class="html">netstat</span>, etc. sont disponibles comme d'habitude.</p>
                    <p>Notez que la capture de traces réseau peut être facilitée en utilisant l'extension <span class="html">uml_dump</span> disponible sur <a href="http://kartoch.msi.unilim.fr/blog/?p=19" target="_blank">http://kartoch.msi.unilim.fr/blog/?p=19</a>. Cette extension est déjà installée dans l'installation Netkit sur le laboratoire étudiant. Pour capturer le trafic échangé sur un "virtual LAN" donné, vous devez simpelement saisir la commande <span class="html">vdump &lt;nom LAN&gt;</span> sur l'hôte. Si vous voulez rediriger la trace vers Wireshark, vous pouvez utiliser <span class="html">vdump A | wireshark -i --k</span>.</p>
                    <hr>
                    <ol>
                        <li>
                            <p>Une pile TCP/IP reçoit un segment SYN avec le numéro de séquence fixé à 1234. Quelle sera la valeur du numéro d'accusé de réception dans le segment SYN+ACK retourné ?</p>
                        </li>
                        <li>
                            <p>Est-il possible pour une pile TCP/IP de renvoyer un segment SYN+ACK avec le numéro d'accusé de réception fixé à 0 ? Si non, expliquez pourquoi. SI oui, quel était le contenu du segment SYN reçu.</p>
                        </li>
                        <li>
                            <p>Ouvrez la trace de paquets <span class="html">tcpdump</span> <span class="em">exercices/traces/trace.5connections_opening_closing.pcap</span> et identifez le nombre de connexions TCP différentes établies et fermées. Pour chaque connexion, expliquez par quel mécanisme elles sont fermées. Analysez les numéros de séquence initiaux utilisés dans les segments SYN et SYN+ACK. COmment évoluent ces numéros de séquenc einitiaux ? Sont-ils augmentés toutes les 4 microsecondes ?</p>
                        </li>
                        <li>
                            <p>La trace de paquets <span class="html">tcpdump</span> <span class="em">exercices/traces/trace.5connections.pcap</span> contient plusieurs tentatives de connexion. Pouvez-vous expliquer ce qui se passe avec ces tentatives de connexion ?</p>
                        </li>
                        <li>
                            <p>La trace de paquets <span class="html">tcpdump</span> <span class="em">exercises/traces/trace.ipv6.google.com.pcap</span> a été collectée à partir d'un site Web populaire accessible en utilisant IPv6. Expliquez les options TCP prises en charge par le client et le serveur.</p>
                        </li>
                        <li>
                            <p>La trace de paquets <span class="html">tcpdump</span> <span class="em">exercises/traces/trace/sirius.info.ucl.ac.be.pcpap</span> a été collectée sur le serveur départemental. QUelles sont les options TCP prises en charge par ce serveur ?</p>
                        </li>
                        <li>
                            <p>Une implémentation TCP maintient un bloc de contrôle de transmission (TCB) pour chaque connexion TCP. Ce TCB est une structure de données qui contient l'ensemble de l'"état" de chaque connexion TCP. Le TCB est décrit dans la RFC 793. Il contient d'abord l'identification de la connexion TCP :</p>
                            <ul>
                                <li>
                                    <p><span class="html">localip</span> : l'adresse IP de l'hôte local.</p>
                                </li>
                                <li>
                                    <p><span class="html">remoteip</span> : l'adresse IP de l'hôte distant.</p>
                                </li>
                                <li>
                                    <p><span class="html">remoteport</span> : le port TCP utilisé pour cette connexion sur l'hôte distant.</p>
                                </li>
                                <li>
                                    <p><span class="html">localport</span> : le port TCP utilisé pour cette connexion sur l'hôte local. Notez que lorsqu'un client ouvre une connexion TCP, le port local sera souvent choisi dans la plage de ports éphémères (49152 &lt;= localport &lt;= 65535).</p>
                                </li>
                                <li>
                                    <p><span class="html">sndnxt</span> : le numéro de séquence du prochain octet dans le flux d'octets (le premier octet d'un nouveau segment de données que vous envoyez utilisera ce numéro de séquence).</p>
                                </li>
                                <li>
                                    <p><span class="html">snduna</span> : le numéro de séquence du prochain octet que votre implémentation s'attend à recevoir de l'hôte distant. Pour cet exercice, vous n'avez pas besoin de maintenir un tampon de réception et votre implémentation peut ignorer les segments hors séquence qu'elle reçoit.</p>
                                </li>
                                <li>
                                    <p><span class="html">sndwnd</span> : la fenêtre d'envoi actuelle.</p>
                                </li>
                                <li>
                                    <p><span class="html">rcwnd</span> : la fenpêtre actuellement annoncée par le récepteur.</p>
                                </li>
                            </ul>
                            <p>En utilisant la trace de paquets <span class="em">exerses/traces/trace.sirius.info.ucl.ac.be.pcap</span>, quel est le TCB de la connexion sur l'hôte 130.104.78.8 lorsqu'il envoie le troisième segment de la trace ?</p>
                        </li>
                        <li>
                            <p>La tace de paquets <span class="html">tcpdump</span> <span class="em">exersises/traces/trace.maps.google.com</span> a été collectée en accédant à un site web populaire qui fournit des informations de cartographie. Combien de connexions TCP ont été utilisées pour récupérer les informations de ce serveur</p>
                        </li>
                        <li>
                            <p>Certains outils de serveillance réseau tels que <span class="html">ntop</span> collectent tous les segments TCP envoyés et reçus par un hôte ou un groupe d'hôtes et fournissent des statistiques intéressantes que le nombre de connexions TCP, le nombre d'octets échangés sur chaque connexion TCP, etc. En supposant que vous puissiez capturer tous les segments TCP envoyés par un hôte, proposez le pseudo-code d'une application qui répertorie toutes les connexions TCP établies et acceptées par cet hôte et le nombre d'octets échangés sur chaque connexion. Doit-on compter le nombre d'octets contenus dans chaque segment pour rapporter le nombre d'octets échangés sur chaque connexion TCP ?</p>
                        </li>
                        <li>
                            <p>Il existe deux types de pare-feux : des dispositifs spéciaux placés à la frontière des réseaux d'entreprise ou de campus et des logiciels qui s'éxécutent sur des hôtes finaux. Un pare-feu est un dispositif matériel ou logiciel qui analyse les paquets TCP/IP et décide, en fonction d'un ensemble de règles, d'accepter ou de rejeter les paquets reçus ou envoyés. Les règles utilisées par un pare-feu dépendent généralement de la valeur de certains champs des paquets (par exemple, le type de protocole de transport, les ports, ...). Nous discuterons plus en détail du fonctionnement des pare-feux dans le chapitre sur la couche réseau. Les logiciels de pare-feux analysent généralement tous les paquets reçus par un hôte et décident, en fonction de l'en-tête du contenu du paquet, s'il peut être traité par la pile réseau de l'hôte ou s'il doit être rejeté. Les administrateurs système configurent souvent des pare-feux sur les ordinateurs portables ou les machines étudiantes pour empêcher les étudiants d'installer des serveurs sur leurs machines. Comment concevriez-vous une pare-feu simple qui bloque toutes les connexions TCP entrantes mais qui permet néanmoins à l'hôte d'établir des connexions TCP avec n'importe quel serveur distant ?</p>
                        </li>
                        <li>
                            <p>En utilisant le laboratoire Netkit expliqué ci-dessus, effectuez quelques tests en utilisant <span class="html">hping3(8)</span>. <span class="em">hping3(8)</span> est un outil en ligne de commande qui permet à quiconque (ayant des privilèges d'administrateur système) d'envoyer des paquets IP spéciaux et des segments TCP, <span class="em">hping3(8)</span> peut être utilis& pour vérifier la configuration des pare-feux ou diagnostiquer des problèmes. Nous l'utiliserons pour tester le fonctionnement de la pile TCP Linux en cours d'exécution dans Netkit.</p>
                            <ul>
                                <li>
                                    <p>Sur l'hôte serveur, lancez <span class="html">tcpdump(1)</span> avec <span class="html">-vv</span> en paramètre pour collecter tous les paquets reçus du client et les afficher. En utilisant <span class="em">hping3(8)</span> sur l'hôte client, envoyez un segment SYN valide vers un port inutilisé sur l'hôte serveur (par exemple 12345). Quels sont les contenus du segment renvoyé par le serveur ?</p>
                                </li>
                                <li>
                                    <p>Effectuez la même expérience, mais envoyez maintenant un segment SYN vers le port 7. Ce port est le port par défaut pour le service de discard (voir <span class="html">services(5)</span>) lancé par <span class="html">xinetd(8)</span>. Quel segment le serveur envoie-t-il en réponse ? Que se passe-t-il à la réception de ce segment ? Expliquez votre réponse.</p>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <p>Le stack TCP/IP de Linux peut être facilement configuré en utilisant <span class="html">sysctl(8)</span> pour modifier les variables de configuration du kernel. Consultez <a href="http://fasterdata.es.net/TCP-tuning/ip-sysctl-2.6.txt" target="_blank">http://fasterdata.es.net/TCP-tuning/ip-sysctl-2.6.txt</a> pour une liste récente des variables <span class="em">sysctl</span> sur le stack TCP/IP de linux. Essayez de désactiver les accusés de réception sélectifs et les options de timestamp RFC1323 et de fenêtre large, et ouvrez une connexion TCP sur le port 7 sur le serveur en utilisant <span class="html">telnet(1)</span>. Vérifiez en utilisant <span class="html">tcpdump(1)</span> l'effet de ces variables du kernel sur les segments envoyés par le stack Linux dans Netkit.</p>
                        </li>
                        <li>
                            <p>Les administrateurs réseau doivent parfois vérifier quels démons de réseau sont actifs sur un serveur. Lorsqu'ils sont connectés au serveur, plusieurs outils peuvent être utilisés pour vérifier cela. Une première solution consiste à utiliser la commande <span class="html">netstat(8)</span>. Cette commande permet d'extraire diverses statistiques de la pile réseau sur le noyau Linux. Pour TCP, <span class="html">netstat</span> peut lister toutes les connexions TCP actives avec l'état de leur FSM. <span class="em">netstat</span> prend en charge les options suivantes qui pourraient être utiles pendant ces exercices :</p>
                            <ul>
                                <li>
                                    <p><span class="html">-t</span> demande des informations sur les connexions TCP.</p>
                                </li>
                                <li>
                                    <p><span class="html">-n</span> demande une sortie numérique (par défaut, <span class="em">netstat</span> envoie des requêtes DNS pour résoudre les adresses IP en noms d'hôtes et utilise <span class="em">/etc/services</span> pour convertir le numéro de port en noms de service, <span class="em">-n</span> est recommandé sur les machines Netkit).</p>
                                </li>
                                <li>
                                    <p><span class="html">-e</span> fournit plus d'informations sur l'état des connexions TCP.</p>
                                </li>
                                <li>
                                    <p><span class="html">-o</span> fournit des informations sur les timers.</p>
                                </li>
                                <li>
                                    <p><span class="html">-a</span> fournit des informations sur toutes les connexions TCP, pas seulement celles dans l'état "<span class="em">Established</span>".</p>
                                </li>
                            </ul>
                            <p>Sur le laboratoire Netkit, lancez un démon et démarrez une connexion TCP en utilisant <span class="html">telnet(1)</span> et utilisez <span class="html">netstat(8)</span> pour vérifier l'état de ces connexions.</p>
                            <p>Une deuxième solution pour déterminer quels démons réseau sont en cours d'exécution sur un serveur consiste à utiliser un outil comme <span class="html">nmap(1)</span>. <span class="em">nmap(1)</span> peut être exécuté à distance et peut être exécuté à distance et peut donc fournir des informations sur un hôte sur lequel l'administrateur système ne peut pas se connecter. Utilisez <span class="em">tcpdump(1)</span> pour collecter les seglents envoyés par <span class="em">nmap(1)</span> s'exécutant sur le client et expliquez comment <span class="em">nmap(1)</span> fonctionne.</p>
                        </li>
                        <li>
                            <p>Les connexions TCP à long terme sont susceptibles de subir des <span class="html">attaques dites RST</span>. Essayez de trouver des informations supplémentaires sur cette attaque et expliquez comment un stack TCP pourrait atténuer de telles attaques.</p>
                        </li>
                        <li>
                            <p>Pour les exercices ci-dessous, nous avons effectué des mesures dans un réseau émulé similaire à celui représenté ci-dessous. Avec un réseau émulé, il est plus difficile d'obtenir des résultats quantitatifs qu'avec un réseau réel car toutes les machines émulées doivent partager le même CPU et la même mémoire. Cela crée des interactions entre les différentes machines émulées qui ne se produisent pas dans le monde réel. Cependant, étant donnée que l'objectif de cet exercice est simplement de permettre aux étudiants de comprendre le comportement du mécanisme de contrôle de congestion TCP, ce n'est pas un problème grave.</p>
                            <figure>
                                <img src="../images/reseau_emule.jpg" alt="">
                                <figcaption>Figure 4.68 : Réseau émulé</figcaption>
                            </figure>
                            <p>Le réseau émulé est composé de trois machines UML : un client, un serveur et un routeur. Pour plus d'informations sur les schémas de contrôle de congestion TCP implémentés dans le noyau Linux, voir <a href="http://linuxgazette.net/135/pfeiffer.html" target="_blank">http://linuxgazette.net/135/pfeiffer.html</a> et <a href="http://www.cs.helsinki.fi/research/iwtcp/papers/linuxtcp.pdf" target="_blank">http://www.cs.helsinki.fi/research/iwtcp/papers/linuxtcp.pdf</a> ou le code source d'un noyau Linux récent. Une description de certains des variables <span class="html">sysctl</span> qui permettent de régler l'implémentation TCP dans le noyau LInux peut être trouvée à l'adresse <a href="http://fasterdata.es.net/TCP-tuning/linux.html" target="_blank">http://fasterdata.es.net/TCP-tuning/linux.html</a>. Pour cet exercice, nous avons configuré le noyau Linux pour utiliser le schéma NewReno RFC 3782 qui est très proche de la norme officielle définie dans le RFC 5681. Le client et le serveur sont connectés via le routeur. Le client envoie des données au serveur. Le lien entre le routeur et le client est contrôlé en utilisant le module du noyau Linux "netem". Ce module nous permet d'insérer des délais supplémentaores, de réduire la bande passante du lien et d'insérer des pertes de paquets aléatoires.</p>
                            <p>Nous avons utilisé <span class="html">netem</span>pour collecter plusieurs traces :</p>
                            <ul>
                                <li>
                                    <p class="em">/transport/exercises/traces/trace0.pcap</p>
                                </li>
                                <li>
                                    <p class="em">/transport/exercises/traces/trace1.pcap</p>
                                </li>
                                <li>
                                    <p class="em">/transport/exercises/traces/trace2.pcap</p>
                                </li>
                                <li>
                                    <p class="em">/transport/exercises/traces/trace2.pcap</p>
                                </li>
                            </ul>
                            <p>En utilisant Wireshark ou tcpdump, effectuez les analyses suivantes :</p>
                            <ol>
                                <li>
                                    <p>Identifier les options TCP utilisées sur la connexion TCP en utilisant Wireshark ou tcpdump.</p>
                                </li>
                                <li>
                                    <p>Analyser l'évolution du temps aller-retour (RTT) sur chacune des connexions TCP en utilisant le graphique de temps aller-retour de Wireshark. Attention à l'estimation du RTT car certaines versions de Wireshark peuvent être défectueuses.</p>
                                </li>
                                <li>
                                    <p>Vérifier si l'implémentation TCP utilisée a implémenté des <span class="html">accusés de réception différés</span>.</p>
                                </li>
                                <li>
                                    <p>Dans chaque trace de paquets, trouvez :</p>
                                    <ol>
                                        <li>
                                            <p>un segment qui a été retransmis en utilisant une <span class="em">retransmission rapide (fast retransmit)</span>. Expliquer cette retransmission en détail.</p>
                                        </li>
                                        <li>
                                            <p>un segment qui a été retransmis grâce à l'expiration du délai de retransmission TCP. Expliquer pourquoi ce segment n'aurait pas pu être retransmis en utilisant une <span class="em">retransmission rapide</span>.</p>
                                        </li>
                                    </ol>
                                </li>
                                <li>
                                    <p>Wireshark contient deux graphiques utiles : le graphique du <span class="em">temps aller-retour</span> et le graphique de <span class="em">séquence temporelle</span>. Expliquez comment vous calculeriez le même graphique à partir d'une telle trace.</p>
                                </li>
                                <li>
                                    <p>Lors de l'affichage des segments TCP, les versions récentes de Wireshark contiennent des heuristiques d'<span class="em">analyse experte</span> qui indiquent si le segment a été retransmis, s'il s'agit d'un ack en double ou si le délai de retransmission a expiré. Expliquez comment vous implémenteriez les même heuristiques que Wireshark.</p>
                                </li>
                                <li>
                                    <p>Pouvez-vous trouver quel fichier a été échangé lors du transfert ?</p>
                                </li>
                            </ol>
                        </li>
                        <li>
                            <p>Vous avez été embauché en tant qu'expert en réseaux par une entreprise. Dans cette entreprise, les utilisateurs d'une application en réseau se plaignent que le réseau est très lent. Les développeurs de l'application affirment que tout retard est causé par des pertes de paquets et un réseau défectueux. L'administrateur réseau sontient que le réseau fonctionne parfaitement et que les retards perçus par les utilisateurs sont causés par les applications ou les serveurs sur lesquels l'application est exécutée. Pour résoudre le cas et déterminer si le problème est dû au réseau a collecté une trace de paquets représentative que vous pouvez télécharger depuis <span class="em">exercises/traces/trace9.pcao</span>. En regardant la trace, pouvez-voud résoudre ce cas et indiquer si le réseau ou l'application est le coupable</p>
                        </li>
                    </ol>
                </article>
                <article>
                    <h2 id="couche_reseau">Partie 5 : La couche réseau :</h2>
                    <h3>5.1 La couche réseau :</h3>
                    <p>La couche transport permet aux applications d'échanger des données de manière efficace et fiable. Les entités de la couche transport s'attendent à pouvoir envoyer des segments vers n'importe quelle destination sans avoir à comprendre quoi que ce soit sur les technologies de sous-réseau sous-jacentes. Il esiste de nombreuses technologies de sous-réseau, la plupart différant dans des détails subtils (taille de trame, adressage, etc.). La couche réseau est la liaison entre ces sous-réseaux et la couche de transport. Elle cache à la couche transport toute la complexité des sous-réseaux sous-jacents et garantit que des informations peuvent être échangées entre des hôtes connectés à différents types de sous-réseaux.</p>
                    <p>Dans ce chapitre, nous expliquons d'abord les principes de la couche réseau. Ces principes comprennent les modes datagramme et circuit virtuel, la séparation entre le plan de données et le plan de contrôle et les algorithmes utilisés par les protocoles de routage. Ensuite, nous expliquons plus en détail la couche réseau dans Internet, en commençant par IPv4 et IPv6, puis en passant aux protocoles de routage (RIP, OSPF et BGP).</p>
                    <h4>5.1.1 Principes :</h4>
                    <p>L'objectif principal de la couche réseau est de permettre aux systèmes finaux, connectés à différents réseaux, d'échanger des informations via des systèmes intermédiaires appelés <span class="html">routeurs</span>. L'unité d'information dans la couche réseau est appelée un <span class="html">paquet</span>.</p>
                    <figure>
                        <img src="../images/couche_reseau_modele_reference.png" alt="">
                        <figcaption>Figure 5.1 : La couche réseau dans le modèle de référence</figcaption>
                    </figure>
                    <p>Avant d'expliquer en détail la couche réseau, il est utile de commencer par analyser le service fourni par la couche liaison de données. Il existe de nombreuses variantes de la couche liaison de données. Certaines fournissent un service orienté connexion, tandis que d'autres fournissent un service sans connexion. Dans cette section, nous nous concentrons sur les services de la couche liaison de données sans connexion, car ils sont les plus largement utilisés. L'utilisation d'une couche liaison de données orientée connexion cause certains problèmes qui dépassent le cadre de ce chapitre. Voir la RFC 3819 pour une discussion sur ce sujet.</p>
                    <p>Il existe trois principaux types de couches liaison de données. La couche de liasion de données la plus simple est utilisée lorsque seuls deux systèmes communicants sont directement connectés via la couche physique. Une telle couche liaison de données est utilisée lorsqu'il existe une liaison point-à-point entre les deux systèmes communicants. Les deux systèmes peuvent être des systèmes terminaux ou des routeurs. PPP (Point-to-Point Protocol), défini dans le RFC 1661, est un exemple d'une telle couche liaison de données point-à-point. Les couches liaison de données échangent des trames et une trame de liaison de données envoyée par une entité de la couche liaison de données à gauche est transmise via la couche physique, de sorte qu'elle peut atteindre l'entité de la couche liaison de données à droite. Les couches liaison de données point-à-point peuvent fournir un service non fiable (les trames peuvent être corrompues ou perdues) ou un service fiable (dans ce cas, la couche liaison de données inclut des mécanismes de retransmission similaires à ceux utilisés dans la couche transport). Le service non fiable est fréquemment utilisé au-dessus de couches physiques (par exemple, la fibre optique, les paires torsadées) ayant un faible taux d'erreur de bits, tandis que les mécanismes de fiabilité sont souvent utilisés dans les réseaux sans fil pour récupérer localement des erreurs de transmission.</p>
                    <figure>
                        <img src="../images/couche_liaison_donnees_point_a_point.png" alt="">
                        <figcaption>Figure 5.2 : La couche liaison de données point-à-point</figcaption>
                    </figure>
                    <p>Le deuxième type de couche liaison de données est celui utilisé dans les réseaux locaux (LAN). Conceptuellement, un LAN est un ensemble de dispositifs communicants tels que deux dispositifs quelconques peuvent échanger directement des trames via la couche liaison de données. Des systèmes terminaux et des routeurs peuvent être connectés à un LAN. Certains LAN ne connectent que quelques dispositifs, mais il existe des LAN qui peuvent connecter des centaines, voire des miliers de dispositifs.</p>
                    <figure>
                        <img src="../images/LAN.png" alt="">
                        <figcaption>Figure 5.3 : Un réseau local (LAN)</figcaption>
                    </figure>
                    <p>Dans le prochain chapitre, nous décrivons l'organisation et le fonctionnement des réseaux locaux. Une différence importante entre les couches liaison de données point à point et les couches liaison de données utilisées dans les LAN est que dans un LAN, chaque dispositif communicant est identifié par une adresse unique de couche liaison de données. Cette adresse est généralement intégrée dans le matériel du dispositif et différents types de LAN utilisent différents types d'adresses de couche liaison de données. Un dispositif communicant attaché à un LAN peut envoyer une trame de liaison de données à n'importe quel autre dispositif communicant attaché au même LAN. La plupart des LAN prennent également en charge des adresses de couche liaison de données spéciales de diffusion et de multidiffusion. Une trame envoyée à l'adresse de diffusion du LAN est distribuée à tous les dispositifs communicants qui sont attachés au LAN. Les adresses de multidiffusion sont utilisées pour identifier des groupes de dispositifs communicants. Lorsqu'une trame est envoyée vers une adresse de couche liaison de données de multidiffusion, elle est distribuée par le LAN à tous les dispositifs communicants qui appartiennent au groupe correspondant.</p>
                    <p>Le troisième type de couches de liaison de données sont utilisés dans les réseaux NBMA (Non-Broadcast Multi Access). Ces réseaux sont utilisés pour interconnecter des dispositifs comme un LAN. Tous les dispositifs attachés à un réseau NBMA sont identifés par un réseau NBMA sont identifiés par une adresse de couche de liaison de données unique. Cependant, et c'est la principale différence entre un réseau NBMA et un LAN traditionnel, le service NBMA ne prend en charge que l'unicast. Le service de couche de liaison de données fourni par un réseau NBMA ne prend en charge ni la diffusion ni la multidiffusion.</p>
                    <p>Malheureusement, aucune couche de liaison de données ne peut envoyer des trames de taille illimitée. Chaque couche de liaison de données est caractérisée par une taille maximale de trame. Il existe plus d'une douzaine de couches de liaison de données différentes et malheureusement la plupart d'entre elles utilisent une taille maximale de trame différente. La couche réseau doit faire face à l'hétérogénéité de la couche de liaison de données.</p>
                    <p>La couche réseau elle-même repose sur les principes suivants :</p>
                    <ol>
                        <li>
                            <p>Chaque entité de la couche réseau est identifiée par une <span class="em">adresse de couche réseau</span>. Cette adresse est indépendante des adresses de la couche liaison de données qu'elle peut utiliser.</p>
                        </li>
                        <li>
                            <p>Le service fourni par la couche réseau ne dépend pas du service ou de l'organisation interne des couches liaison de données sous-jacentes.</p>
                        </li>
                        <li>
                            <p>La couche réseau est conceptuellement divisée en deux plans : le <span class="html">plan de données</span> et le <span class="html">plan de contrôle</span>. Le <span class="em">plan de données </span> contient les protocoles et mécanismes qui permettent aux hôtes et aux routeurs d'échanger de paquets portant des données utilisateur. Le <span class="em">plan de contrôle</span> contient les protocoles et mécanismes qui permettent aux routeurs d'apprendre efficacement comment transférer les paquets vers leur destination finale.</p>
                        </li>
                    </ol>
                    <p>L'indépendance de la couche réseau vis-à-vis de la couche liaison de données sous-jacente est un principe clé de la couche réseau. Cela garantit que la couche réseau peut être utilisée pour permettre aux hôtes attachés à différents types de couches liaison de données d'échanger des paquets via des routeurs intermédiaires. De plus, cela permet aux couches liaison de données et à la couche réseau d'évoluer indépendamment les unes des autres. Cela permet à la couche réseau de s'adapter facilment une nouvelle couche liaison de données chaque fois qu'une nouvelle couche liaison de données est inventée.</p>
                    <p>Il existe deux types de service pouvant être fournis par la couche réseau :</p>
                    <ul>
                        <li>
                            <p>un service <span class="em">non fiable et sans connexion</span>.</p>
                        </li>
                        <li>
                            <p>un service <span class="em">orienté connexion</span>, fiable ou non fiable.</p>
                        </li>
                    </ul>
                    <p>Les services orientés connexion ont été populaires avec des technologies telles que <span class="em">X.25</span> et <span class="em">ATM</span> ou <span class="em">frame-relay</span>, mais de nos jours, la plupart des réseaux utilisent un service non fiable et sans connexion. C'est notre principal sujet de discussion dans ce chapitre.</p>
                    <h5>Organisation de la couche réseau :</h5>
                    <p>Il existe deux organisations internes posibles de la couche réseau :</p>
                    <ul>
                        <li>
                            <p>datagramme</p>
                        </li>
                        <li>
                            <p>circuits virtuels</p>
                        </li>
                    </ul>
                    <p>L'organisation interne du réseau est indépendante du service qu'il fournit, mais la plupart du temps une organisation en datagramme est utilisée pour fournir un service sans connexion, tandis qu'une organisation en circuits virtuels est utilisée dans les réseaux qui fournissent un service orienté connexion.</p>
                    <h5>L'organisation en datagramme :</h5>
                    <p>La première et la plus populaire organisation de la couche réseau est l'organisation en datagramme. Cette organisation est inspirée de l'organisation du service postal. Chaque hôte est inspirée de l'organisation du service postal. Chaque hôte est identifié par une adresse de couche réseau. Pour envoyer des informations à un hôte distant, un hôte crée un paquet qui contient :</p>
                    <ul>
                        <li>
                            <p>l'adresse de couche réseau de l'hôte de destination.</p>
                        </li>
                        <li>
                            <p>sa propre adresse de couche réseau.</p>
                        </li>
                        <li>
                            <p>les informations à envoyer.</p>
                        </li>
                    </ul>
                    <p>La couche réseau limite la taille maximale du paquet. Ainsi, les informations doivent avoir été divisées en paquets par la couche  transport avant d'être transmises à la couche réseau.</p>
                    <p>Pour comprendre l'organisation en datagramme, considérons la figure ci-dessous. Une adresse de couche réseau, représentée par une lettre, a été attribuée à chaque hôte et routeur. Pour envoyer des informations à l'hôte <span class="em">J</span>, l'hôte <span class="em">A</span> crée un paquet contenant sa propre adresse, l'adresse de destination et les informations à échanger.</p>
                    <p>Avec l'organisation en datagramme, les routeurs utilsent une <span class="html">transmission hop-by-hop</span>. Cela signifie que lorsqu'un routeur reçoit un paquet qui n'est pas destiné à lui-même, il recherche l'adresse de destination du paquet dans sa <span class="html">table de routage</span>. Une <span class="em">table de routage</span> est une structure de données qui associe chaque adresse de destination (ou ensemble d'adresses de destination) à l'interface de sortie sur laquelle un paquet destiné à cette adresse doit être transmis pour atteindre sa destination finale.</p>
                    <p>La principale contrainte imposée aux tables de routage est qu'elles doivent permettre à n'importe quel hôte du réseau d'atteindre n'importe quel autre hôte. Cela implique que chaque routeur doit connaître une route vers chaque destination, mais aussi que les chemins composés des informations stockées dans les tables de routage ne doivent pas contenir de boucles. Sinon, certaines destinations serianet inaccessibles.</p>
                    <figure>
                        <img src="../images/inter-reseau_simple.PNG" alt="">
                        <figcaption>Figure 5.4 : Un inter-réseau simple</figcaption>
                    </figure>
                    <p>Dans l'exemple ci-dessus, l'hôte <span class="em">A</span> envoie son paquet au routeur <span class="em">R1</span>. <span class="em">R1</span> consulte sa table de routage et transfère le paquet vers <span class="em">R2</span>. Sur la base de sa propre table de routage, <span class="em">R2</span> décide de transférer le paquet vers <span class="em">R5</span> qui peut le livrer à sa destination.</p>
                    <p>Pour permettre aux hôtes d'échanger des paquets, un réseau repose sur deux types différents de protocoles et de mécanismes.</p>
                    <p>Pour permettre aux hôtes d'échanger des paquets, un réseau repose sur deux types différents de protocoles et des mécanismes. Tout d'abord, il doit avoir une définition précise du format des paquets qui sont envoyés par les hôtes et traités par les routeurs. Deuxièmement, l'algorithme utilisé par les routeurs pour transférer ces paquets doit être défini. Ce protocole et cet algorithme font partie du plan de données de la couche réseau. Le plan de données contient tous les protocoles et algorithmes utilisés par les hôtes et les routeurs pour créer et traiter les paquets qui contiennent des données utilisateur.</p>
                    <p>Le <span class="em">plan de données</span>, et en particulier l'algorithme de transfert utilisé par les routeurs, dépend des tables de routage maintenues sur chaque routeur. Ces tables de routage peuvent être maintenues en utilisant diverses techniques (configuration manuelle, protocoles distribués, calcul centralisé, etc.). Ces techniques font partie du plan de contrôle de la couche réseau. Le <span class="em">plan de contrôle</span> contient tous les protocoles et mécanismes utilisés pour calculer et installer les tables de routage sur les routeurs.</p>
                    <p>L'organisation de datagramme a été très populaire dans les réseaux informatiques. Les couches réseau basées sur les datagrammes incluent IPv4 et IPv6 dans l'Internet global, CLNP défini par l'ISO, IPX défini par Noveil ou XNS défini par Xerox [Perlman2000].</p>
                    <h5>L'organisation en circuits virtuels :</h5>
                    <p>Le principal avantage de l'organisation en datagramme est sa simplicité. Les principes de cette organisation peuvent être facilement compris. De plus, cela permet à un hôte d'envoyer facilement un paquet vers n'importe quelle destination à tout moment. Cependant, comme chaque paquet est transmi indépendamment par des routeurs intermédiaires, les paquets envoyés par un hôte peuvent ne pas suivre le même chemin pour atteindre une destination donnée. Cela peut causer un réarrangement de paquets, ce qui peut être gênant pour les protocoles de transport. De plus, comme un routeur utilisant une transmission de saut en saut envoie toujours des paquets envoyés vers la même destination sur la même destination sur la même interface sortante, cela peut causer une congestion sur certains liens.</p>
                    <p>La deuxième organisation de la couche réseau, appelée <span class="html">circuits virtuels</span>, a été inspirée par l'organisation des réseaux téléphoniques. Les réseaux téléphoniques ont été conçus pour transporter des appels téléphoniques qui durent généralemnt quelques minutes. Chaque téléphone est identifié par un numéro de téléphone et est connecté à un commutateur téléphonique. Pour initier un appel téléphonique, un téléphone doit d'abord envoyer le numéro de téléphone de destination à son commutateur local. Le commutateur coopère avec les autres commutatuers du réseau pour créer un canal bidirectionnel entre les deux téléphones à travers le réseau. Ce canal sera utilisé par les deux téléphones pendant la durée de l'appel et sera libéré à la fin de l'appel. Jusqu'aux années 1960, la plupart de ces canaux étaient créés manuellement, par des opérateurs téléphoniques, sur demande de l'appelant. Les réseaux téléphoniques d'aujourd'hui utilisent des commutateurs automatisés et permettent à plusieurs canaux d'être transportés sur le même lien physique, mais les principes restent globalement les mêmes.</p>
                    <p>Dans un réseau utilisant des circuits virtuels, tous les hôtes sont identifiés par une adresse de couche réseau. Cependant, un hôte doit explicitement demander l'établissement d'un circuit virtuel ayant de pouvoir envoyer des paquets à un hôte de destination. La demande d'établissement d'un circuit virtuel est traitée par le plan de contrôle, qui installe un état pour créer le circuit virtuel entre la source et la destination à travers les routeurs intermédiaires. Tous les paquets qui sont envoyés sur le circuit virtuel contiennent un identificateur de circuit virtuel qui permet aux routeurs de déterminer à quel circuit virtuel chaque paquet appartient. Cela est illustré dans la figure ci-dessous avec un circuit virtuel entre l'hôte <span class="em">A</span> et l'hôte <span class="em">I</span> et un autre entre l'hôte <span class="em">A</span> et l'hôte <span class="em">J</span>.</p>
                    <figure>
                        <img src="../images/inter-reseau_simple_utilisation_circuits_virtuels.png" alt="">
                        <figcaption>Figure 5.5 : Un inter-réseau simple utilisant des circuits virtuels</figcaption>
                    </figure>
                    <p>L'établissement d'un circuit virtuel est effectué à l'aide d'un protocole de signalisation dans le <span class="em">plan de contrôle</span>. Généralement, l'hôte source envoie un message de signalisation pour indiquer à son routeur l'adresse de destination et éventuellement certaines caractéristiques de performance du circuit virtuel à établir. Le premier routeur peut traiter le message de signalisation de deux manières différentes.</p>
                    <p>Une première solution consiste pour le routeur à consulter sa table de routage, à se souvenir des caractéristiques du circuit virtuel demandé et à la transmettre sur son interface de sortie vers la destination. Le message designalisation est ainsi transmis de saut en saut jusqu'à ce qu'il atteigne la destination et que le circuit virtuel soit ouvert le long du chemin suivi par le message de signalisation. Ceci est illustré avec le circuit virtuel rouge dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/etablissement_circuit_virtuel.png" alt="">
                        <figcaption>Figure 5.6 : Établissement de circuits virtuels</figcaption>
                    </figure>
                    <p>Une deuxième solution peut être utilisée si les routeurs connaissent l'ensemble de la topologie du réseau. Dans ce cas, le premier routeur peut utiliser une technique appelée <span class="html">routage source</span>. À la réception du message de signalisation, le premier routeur choisit le chemin du circuit virtuel dans le réseau. Ce chemin est codé sous forme de liste des adresses de tous les routeurs intermédiaires pour atteindre la destination. Il est inclus dans le message de signalisation et les routeurs intermédiaires peuvent retirer leur adresse du message de signalisation avant de le transférer. Cette technique permet aux routeurs de mieux répartir les circuits virtuels dans l'ensemble du réseau. Si les routeurs connaissent la charge des liens distants, ils peuvent également sélectionner le chemin le moins chargé lors de l'établissement d'un circuit virtuel. Cette solution est illustrée avec le circuit bleu dans la figure ci-dessus.</p>
                    <p>Le dernier point à discuter concernant l'organisation du circuit virtuel est son <span class="em">plan de données</span>. Le <span class="em">plan de données</span> définit principalement le format des paquets de données et l'algorithme utilisé par les routeurs pour transmettre les paquets. Les paquets de données contiennent un identifiant de circuit virtuel, encodé sous forme d'un nombre fixe de bits. Ces identifiants de circuits virtuels sont généralement appelés des <span class="html">étiquettes</span>.</p>
                    <p>Chaque hôte maintient une table de flux qui associe une étiquette à chaque circuit qu'il a établi. Lorsqu'un routeur reçoit un paquet  contenant une étiquette, il extrait l'étiquette et consulte sa <span class="html">table de transfert d'étiquettes</span>. Cette table est une structure de données qui associe chaque couple <span class="em">(interface d'entrée, étiquette)</span> à l'interface de sortie à utiliser pour transmettre le paquet ainsi que l'étiquette qui doit être placée dans les paquets de sortie. En pratique, la table de transfert d'étiquettes peut être implémentée sous forme de vecteur et le couple <span class="em">(interface d'entrée, étiquette)</span> est l'indice de l'entrée dans le vecteur qui contient l'interface de sortie et l'étiquette de sortie. Ainsi, une seule lecture de mémoire est nécessaire pour consulter la table de transfert d'étiquettes. L'utilisation de la table de transfert d'étiquettes est illustrée dans la figure ci-desous.</p>
                    <figure>
                        <img src="../images/tables_transfert_label_reseau_utilisation_circuits_virtuels.png" alt="">
                        <figcaption>Figure 5.7 : Tables de transfert d'étiquettes dans un réseau utilisant des circuits virtuels</figcaption>
                    </figure>
                    <p>La structure du circuit virtuel a été principalement utilisée dans les réseaux publics, à partir de X.25, puis dans Frame Relay et le réseau de <span class="html">mode de transfert asynchrone (ATM)</span>.</p>
                    <p>Les deux structures, le datagramme et le circuit virtuel, ont des avantages et des inconvénients. Le principal avantage de l'organisation en datagramme est que les hôtes peuvent facilement envoyer des paquets à un nombre quelconque de destinations, tandis que l'organisation en circuit virtuel nécessite l'établissement d'un circuit virtuel avant la transmission d'un paquet de données. Cette solution peut être coûteuse pour les hôtes qui échangent de petites quantités de données. En revanche, le principal avantage de l'organisation en circuit virtuel est que l'algorithme de transfert utilisé par les routeurs est plus simple que lors de l'utilisation de l'organisation en datagramme. De plus, l'utilisation de circuits virtuels peut permettre de mieux répartir la charge  à travers le réseau grâce à l'utilisation de plusieurs circuits virtuels. La technique <span class="html">MultiProtocol Label Switching (MPLS)</span> que nous discuterons dans une autre révision de ce livre peut être considérée comme un bon compromis entre le datagramme et les circuits vituels. MPLS utilise des ciruits virtuels entre les routeurs, mais ne les étend pas aux hôtes finaux. Des informations supplémentaires sur MPLS peuvent être trouvées dans [ML2011].</p>
                    <h5>Le plan de contrôle :</h5>
                    <p>L'un des objectifs du <span class="em">plan de contrôle</span> dans la couche réseau est de maintenir les tables de routage utilisées sur tous les routeurs. Comme indiqué précédemment, une table de routage est une structure de données qui contient, pour chaque adresse de destination (ou bloc d'adresses) connue par le routeur, l'interface de sortie par laquelle le routeur doit faire passer un paquet destiné à cette adresse. La table de routage peut également contenir des informations supplémentaires telles que l'adresse du prochain routeur sur le chemin vers la destination ou une estimation du coût de ce chemin.</p>
                    <p>Dans cette section, nous discutons des trois principales techniques pouvant être utilisées pour maintenir les tables de routage dans un réseau.</p>
                    <h5>Le routage statique :</h5>
                    <p>La solution la plus simple est de pré-calculer toutes les tables de routage de tous les routeurs et de les installer sur chaque routeur. Plusieurs algorithmes peuvent être calculer ces tables.</p>
                    <p>Une solution simple consiste à utiliser le routage de chemin le plus court et à minimiser le nombre de routeurs intermédiaires pour atteindre chaque destination. Des algorithmes plus complexes peuvent prendre en compte la charge attendue sur les liens pour garantir qu'il n'y a pas de congestion pour une demande de trafic donnée. Ces algorithles doivent tous garantir que :</p>
                    <ul>
                        <li>
                            <p>tous les routeurs sont configurés avec une route pour atteindre chaque destination.</p>
                        </li>
                        <li>
                            <p>aucun des chemins composés des entrées trouvées dans les tables de routage ne contient de cycle. Un tel cycle entraînerait une boucle de transfert.</p>
                        </li>
                    </ul>
                    <p>La figure ci-dessous montre des tables de routage d'exemple dans un réseau de cinq routeurs.</p>
                    <figure>
                        <img src="../images/tables_routage_reseau_simple.png" alt="">
                        <figcaption>Figure 5.8 : Tables de routage dans un réseau simple</figcaption>
                    </figure>
                    <p>Le principal inconvénient du routage statique est qu'il ne s'adapte pas à l'évolution du réseau. Lorsqu'un nouveau routeur ou lien est ajouté, toutes les tables de routage doivent être recalculées. De plus, lorsqu'un lien ou un routeur tombe en panne, les tables de routage doivent également être mises à jour.</p>
                    <h5>Le routage par vecteur de distance :</h5>
                    <p>Le routage par vecteur de distance est un protocole de routage distribué simple. Le routage par vecteur de distance permet aux routeurs de découvrir automatiquement les destinations atteignables à l'intérieur du réseau ainsi que le chemin le plus court pour atteindre chacune de ces destinations. Le chemin le plus court est calculé sur la base de <span class="em">métriques</span> ou de <span class="em">coûts</span> qui sont associés à chaque lien. Nous utilisons <span class="em">l.cost</span> pour représenter la métrique qui a été configurée par le lien <span class="em">l</span> sur un routeur.</p>
                    <p>Chaque routeur maintient une table de routage. La table de routage <span class="em">R</span> peut être modélisée comme une structure de données qui stocke, pour chaque adresse de destination connue <span class="em">d</span>, les attributs suivants :</p>
                    <ul>
                        <li>
                            <p><span class="em">R[d].link</span> est le lien de sortie que le routeur utilise pour acheminer les paquets vers la destination <span class="em">d</span>.</p>
                        </li>
                        <li>
                            <p><span class="em">R[d].cost</span> est la somme des métriques des liens qui composent le plus court chemin pour atteindre la destination <span class="em">d</span>.</p>
                        </li>
                        <li>
                            <p><span class="em">R[d].time</span> est l'horodatage du dernier vecteur de distance contenant la destination <span class="em">d</span>.</p>
                        </li>
                    </ul>
                    <p>Un routeur qui utilise le routage par vecteur de distance envoie régulièrement son vecteur de distance sur toutes ses interfaces. Le vecteur de distance est un résumé de la table de routage du routeur qui indique la distance vers chaque destination connue. Ce vecteur de distance peut être calculé à partir de la table de routage en utilisant le pseudo-code ci-dessous.</p>
<pre><code>Every N seconds:
    v=Vector()
    for d in R[]:
        # add destination d to vector
        v.add(Pair(d,R[d].cost))
    for i in interfaces
        # send vector v on this interface
        send(v,interface)</code></pre>
                    <p>Lorsqu'un routeur démarre, il ne connaît aucune destination dans le réseau et sa table de routage ne contient que lui-même. Il envoie donc à tous ses voisins un vecteur de distance qui ne contient que son adresse à une distance de <span class="em">0</span>. Lorsqu'un routeur reçoit un vecteur de distance sur la liaison <span class="em">l</span>, il le traite comme suit.</p>
<pre><code># V : received Vector
# l : link over which vector is received
def received(V,l):
    # received vector from link l
    for d in V[]
        if not (d in R[]) :
            # new route
            R[d].cost=V[d].cost+l.cost
            R[d].link=l
            R[d].time=now
        else :
            # existing route, is the new better ?
            if ( ((V[d].cost+l.cost) &lt; R[d].cost) or ( R[d].link == l) ) :
                # Better route or change to current route
                R[d].cost=V[d].cost+l.cost
                R[d].link=l
                R[d].time=now</code></pre>
                    <p>Le routeur itère sur toutes les adresses incluses dans le vecteur de distnace. Si le vecteur de distance contient une adresse que le routeur ne connaît pas, il insère la destination dans sa table de routage via la liaison <span class="em">l</span> et à une distance qui est la somme entre la distance indiquée dans le vecteur de distance et le coût associé à la liaison <span class="em">l</span>. Si la destination était déjà connue du routeur, il met à jour l'entrée correspondante dans sa table de routage si soit :</p>
                    <ul>
                        <li>
                            <p>le coût de la nouvelle route est inférieur au coût de la route déjà connue (<span class="em">(V[d].cost+l.cost) &lt; R[d].cost</span></p>
                        </li>
                        <li>
                            <p>la nouvelle route a été apprise via la même liaison que la meilleure route courante vers cette destination (<span class="em">R[d].link == l</span>).</p>
                        </li>
                    </ul>
                    <p>La première condition permet au routeur de découvrir le chemin le plus court vers chaque destination. La deuxième condition est utilisée pour prendre en compte les changements de routes qui peuvent survenir après une défaillance de liaison ou une modification de la métrique associée à une liaison.</p>
                    <p>Pour comprendre le fonctionnement d'un protocole de vecteur de distance, prenons en compte le réseau de cinq routeurs présenté ci-dessous.</p>
                    <figure>
                        <img src="../images/fonctionnement_routage_vecteur_distance_reseau_simple.png" alt="">
                        <figcaption>Figure 5.9 : Fonctionnement du routage de vecteur de distance dans un réseau simple</figcaption>
                    </figure>
                    <p>D'après l'hypothèse que <span class="em">A</span> envoie en premier son vecteur de distance <span class="em">[A=0]</span> :</p>
                    <ul>
                        <li>
                            <p><span class="em">B</span> et <span class="em">D</span> traitent le vecteur de distance reçu et mettent à jour leur table de routage avec une route vers <span class="em">A</span>.</p>
                        </li>
                        <li>
                            <p><span class="em">D</span> envoie son vecteur de distance <span class="em">[D=0,A=1]</span> à <span class="em">A</span> et <span class="em">E</span>. <span class="em">E</span> peut maintenant atteindre <span class="em">A</span> et <span class="em">D</span>.</p>
                        </li>
                        <li>
                            <p><span class="em">C</span> envoie son vecteur de distance <span class="Em">[C=0]</span> à <span class="em">B</span> et <span class="em">E</span>.</p>
                        </li>
                        <li>
                            <p><span class="em">E</span> envoie son vecteur de distance <span class="em">[E=0,D=1,A=2,C=2]</span> à <span class="em">D</span>, <span class="em">B</span> et <span class="em">C</span>. <span class="em">B</span> peut maintenant atteindre <span class="em">A</span>, <span class="em">C</span>, <span class="em">D</span> et <span class="em">E</span>.</p>
                        </li>
                        <li>
                            <p><span class="em">B</span> envoie son vecteur de distance <span class="em">[B=0,A=1,C=1,D=2,E=1]</span> à <span class="em">A</span>, <span class="em">C</span> et <span class="em">E</span>. <span class="em">A</span>, <span class="em">B</span>, <span class="em">C</span> et <span class="em">E</span> peuvent maintenant atteindre toutes les destinations.</p>
                        </li>
                        <li>
                            <p><span class="em">A</span> envoie son vecteur de distance <span class="em">[A=0,B=1,C=2,D=1,E=2]</span> à <span class="em">B</span> et <span class="em">D</span>.</p>
                        </li>
                    </ul>
                    <p>À ce stade, tous les routeurs peuvent atteindre tous les autres routeurs du réseau grâce aux tables de routage affichées dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/tables_routage_calculees_vecteur_distance_reseau_simple.png" alt="">
                        <figcaption>Figure 5.10 : Tables de routage calculées par vecteur de distance dans un réseau simple</figcaption>
                    </figure>
                    <p>Pour faire face aux pannes de liens et de routeurs, les routeurs utilisent le timestamp stocké dans leur table de routage. Comme tous les routeurs envoient leur vecteur de distance toutes les <span class="em">N</span> secondes, le timestamp de chaque route devrait être régulièrement actualisé. Ainsi, aucune route ne doit avoir un timestamp plus ancien que <span class="em">N</span> secondes, sauf si la route n'est pas atteignable. En pratique, pour faire face à la perte possible d'un vecteur de distance due à des erreurs de transmission, les routeurs vérifient le timestamp des routes stockées dans leur table de routage toutes les <span class="em">N</span> secondes et suppriment les routes qui ont plus de <span class="em">3 * N </span> secondes. Lorsqu'un routeur remarque qu'une route vers une direction a expiré, il doit d'abord associer un coût <span class="em">&#8734;</span> à cette route et envoyer son vecteur de distance à ses voisins pour les informer. La route peut ensuite être supprimée de la table de routage après un certain temps (par exemple, <span class="em">3 * N</span> secondes), pour s'assurer que les routeurs voisins ont reçu les mauvaises nouvelles, même si certains vecteurs de distance ne les atteignent pas en raison d'erreurs de transmission.</p>
                    <p>Considérons l'exemple ci-dessus et supposons que le lien entre les routeurs <span class="em">A</span> et <span class="em">B</span> échoue. Avant la défaillance, <span class="em">A</span> utilisait <span class="em">B</span> pour atteindre les destinations <span class="em">B</span>, <span class="em">C</span> et <span class="Em">E</span>, tandis que <span class="em">B</span> n'utilisait que le lien <span class="em">A-B</span> pour atteindre <span class="em">A</span>. Les entrées affectées expirent sur les routeurs <span class="em">A</span> et <span class="em">B</span> et ils envoient tous deux leur vecteur de distance.</p>
                    <ul>
                        <li>
                            <p><span class="em">A</span> envoie son vecteur de distance <span class="em">[A = 0, D = &#8734;, C = &#8734;, D = 1, E = &#8734;]</span>. <span class="em">D</span> sait qu'il ne peut plus atteindre <span class="em">B</span> via <span class="em">A</span>.</p>
                        </li>
                        <li>
                            <p><span class="em">D</span> envoie son vecteur de distance <span class="em">[D = 0, B = &#8734;, A = 1, C = 2, E = 1]</span> à <span class="em">A</span> et <span class="em">E</span>. <span class="em">A</span> récupère les routes vers <span class="em">C</span> et <span class="em">E</span> via <span class="em">D</span>.</p>
                        </li>
                        <li>
                            <p><span class="em">B</span> envoie son vecteur de distance <span class="em">[B = 0, B = &#8734;, C = 1, D = 2, E = 1]</span> à <span class="em">E</span> et <span class="em">C</span>. <span class="em">D</span> apprend qu'il n'y a plus de route pour atteindre <span class="em">A</span> via <span class="em">B</span>.</p>
                        </li>
                        <li>
                            <p><span class="em">E</span> envoie son vecteur de distance <span class="em">[E = 0, A = 2, C = 1, D = 1, B = 1]</span> à <span class="em">D</span>, <span class="em">B</span> et <span class="em">C</span>. <span class="em">D</span> apprend une route vers <span class="em">B</span>. <span class="em">C</span> et <span class="em">B</span> apprennent une route vers <span class="em">A</span>.</p>
                        </li>
                    </ul>
                    <p>À ce stade, tous les routeurs ont une table de routage leur permettant d'atteindre tous les autres routeurs, sauf le routeur <span class="em">A</span>, qui ne peut pas encore atteindre le routeur <span class="em">B</span>. <span class="em">A</span> récupère la route vers <span class="em">B</span> une fois que le routeur <span class="em">D</span> envoie son vecteur de distance mis à jour <span class="em">[A = 1, B = 2, C = 2, D = 1, E = 1]</span>. Cette dernière étape est illustrée dans la figure "<span class="em">Tables de routage calculées par vecteur de distance après une défaillance</span>", qui montre les tables de routage sur tous les routeurs.</p>
                    <figure>
                        <img src="../images/tables_routage_calculees_vecteur_distance_apres_echec.png" alt="">
                        <figcaption>Figure 5.11 : Tables de routage calculées par vecteur de distance après une défaillance</figcaption>
                    </figure>
                    <p>Considérons maintenant que le lien entre <span class="em">D</span> et <span class="em">E</span> échoue. Le réseau est maintenant partionné en deux parties disjointes : <span class="em">(A, D)</span> et <span class="em">(B, E, C)</span>. Les routes vers <span class="em">B</span>, <span class="em">C</span> et <span class="em">E</span> expirent en premier sur le routeur <span class="em">D</span>. À ce moment, le routeur <span class="em">D</span> met à jour sa table de routage.</p>
                    <p>Si <span class="em">D</span> envoie <span class="em">[D = 0, A = 1, B = &#8734;, C = &#834;, E = &#8734;]</span>, <span class="em">A</span> apprend que <span class="em">B</span>, <span class="em">C</span> et <span class="em">E</span> sont injoignables et met à jour sa table de routage.</p>
                    <p>Malheureusement, si le vecteur de distance envoyé à <span class="em">A</span> est perdu ou si <span class="em">A</span> envoie son propre vecteur de distance (<span class="em">[A = 0, D = 1, B = 3, C = 3, E = 2]</span>) en même temps que <span class="em">D</span>envoie son vecteur de distance, <span class="em">D</span> met à jour sa tables de routage pour utiliser les routes plus courtes annoncées par <span class="em">A</span> vers <span class="em">B</span>, <span class="em">C</span> et <span class="em">E</span>. Après un certain temps, <span class="em">D</span> envoie un nouveau vecteur de distance : <span class="em">[D = 0, A = 1, E = 3, C = 4, B = 4]</span>. <span class="em">A</span> met à jour sa table de routage et après un certain temps envoie son propre vecteur de distance <span class="em">[A = 0, D = 1, B = 5, C = 5, E = 4]</span>, etc. Ce problème est connu sous le nom de <span class="html">problème de comptage à l'infini</span> dans la littérature sur les réseaux. Les routeurs <span class="em">A</span> et <span class="em">D</span> échangent des vecteurs de distance avec des coûts croissants jusqu'à ce que ces coûts atteignent <span class="em">&#8734;</span>. Ce problème peut se produire dans d'autres scénarios que celui représenté dans la figure ci-dessus. En fait, le routage vectoriel peut souffrir de problèmes de comptage à l'infini dès qu'il y a un cycle dans le réseau. Les cycles sont nécessaires pour avoir suffisamment de redondance pour faire face aux pannes de liens et de routeurs. Pour atténuer l'impact du comptage à l'infini, certains protocoles de routage vectoriel considèrent que <span class="em">16 = &#8734;</span>. Malheureusement, cela limite les métriques que les opératuers de réseau peuvent utiliser et le diamètre des réseaux utilisant des vecteurs de distance.</p>
                    <p>Ce problème de comptage à l'infini se produit parce que le routeur <span class="em">A</span> annonce au routeur <span class="em">D</span> une route qu'il a apprise via le routeur <span class="em">D</span>. Une solution possible pour éviter ce problème pourrait être de changer la façon dont la façon dont un routeur crée son vecteur de distance. Au lieu de calculer un vecteur de distance spécifique à chaque voisin et ne contenant que les routes qui n'ont pas été apprises via ce voision. Ceci pourrait être implémenté avec le pseudo-code suivant.</p>
<pre><code>Every N seconds:
    # one vector for each interface
    for l in interfaces:
        v=Vector()
        for d in R[]:
            if (R[d].link != i) :
                v=v+Pair(d,R[d.cost])
        send(v)
        # end for d in R[]
    #end for l in interface</code></pre>
                    <p>La technique utilisée s'appelle <span class="html">split-horizon</span>. Avec cette technique, le problème du comptage à l'infini ne se serait pas produit dans le scénario ci-dessus, car le routeur <span class="em">A</span> aurait annoncé <span class="em">[A = 0]</span>, car il a appris toutes ses autres routes via le routeur <span class="em">D</span>. Une autre variante appelée <span class="html">split-horizon avec poison reverse</span> est également possible. Les routeurs utilisant cette variante annoncent un coût de <span class="em">&#8734;</span> pour les destinations qu'ils atteignent via le routeur auquel ils envoient le vecteur de distance. Cela peut être implémenté en utilisant le pseudo-code ci-dessous.</p>
<pre><code>Every N seconds:
    for l in interfaces:
        # one vector for each interface
        v=Vector()
        for d in R[]:
            if (R[d].link != i) :
                v=v+Pair(d,R[d.cost])
            else:
                v=v+Pair(d,infinity);
        send(v)
        # end for d in R[]
    #end for l in interface</code></pre>
                    <p>Malheureusement, la technique du "split-horizon" n'est pas suffisante pour éviter tous les problèmes de comptage à l'infini avec le routage par vecteur de distance. Considérez la défaillance du lien <span class="em">A-B</span> dans le réseau de quatre routeurs ci-dessous.</p>
                    <figure>
                        <img src="../images/probleme_comptage_infini.png" alt="">
                        <figcaption>Figure 5.12 : Problème du comptage à l'infini</figcaption>
                    </figure>
                    <p>Après avoir détecté la défaillance, le routeur <span class="em">A</span> envoie ses vecteurs de distance :</p>
                    <ul>
                        <li>
                            <p><span class="em">[A = &#8734;, B = 0, C = &#8734;, E = 1]</span> au routeur <span class="em">C</span>.</p>
                        </li>
                        <li>
                            <p><span class="em">[A = &#8734;, B = 0, C = 1, E = &#8734;]</span> au routeur <span class="em">E</span>.</p>
                        </li>
                    </ul>
                    <p>Si, malheureusement, le vecteur de distance envoyé au routeur <span class="em">C</span> est perdu en raison d'une erreur de transmission ou parce que le routeur <span class="em">C</span> est surchargé, un nouveau problème de comptage à l'infini peut survenir. Si le routeur <span class="em">C</span> envoie son vecteur de distance <span class="em">[A = 2, B = 1, C = 0, E = &#8734;]</span> au routeur <span class="em">E</span>, ce routeur envoie ses vecteurs de distance <span class="em">[A = 3, B = &#8734;, C = 1, E = 1]</span> au routeur <span class="em">B</span> et <span class="em">[A = &#8734;, B = 1, C = &#8734;, E = 0]</span> au routeur <span class="em">C</span>. Ce vecteur de distance permet à <span class="em">B</span> de récupérer une route de distance <span class="em">4</span> pour atteindre <span class="em">A</span>.</p>
                    <h5>Le routage par état de lien :</h5>
                    <p>Le routage par état de lien est la seconde famille de protocoles de routage. Alors que les routeurs à vecteurs de distance utilisent un algorithme distribué pour calculer leurs tables de routage, les routeurs à état de lien échangent des messages pour permettre à chaque routeur d'apprendre la topologie complète du réseau. En se basant sur cette topologie apprise, chaque routeur est ensuite en mesure de calculer sa table de routage en utilisant un calcul de chemin le plus court [Dijkstra1959].</p>
                    <p>Pour le routage par état de lien, un réseau est modélisé comme un <span class="em">graphe pondéré orienté</span>. Chaque routeur est un noeud, et les liens entre les routeurs sont les arrêtes du graphe. Un poids positif est associé à chaque arrête dirigée et les routeurs utilisent le chemin le plus court pour atteindre chaque destination. En pratique, différents types de poids peuvent être associés à chaque arrête dirigée :</p>
                    <ul>
                        <li>
                            <p>poids unitaire. Si tous les liens ont un poids unitaire, le routage par chemin le plus court préfère les chemins avec le moins de routeurs intermédiaires.</p>
                        </li>
                        <li>
                            <p>poids proportionnel au délai de propagation sur le lien. Si tous les poids de lien sont configurés de cette manière, le routage par chemin le plus court utilise les chemins avec le plus petit délai de propagation.</p>
                        </li>
                        <li>
                            <p><span class="em">weight = C/bandwidth</span> où <span class="em">C</span> est une constante supérieure à la bande passante de lien la plus élevée dans le réseau. Si tous les poids de lien sont configurés de cette manière, le routage par chemin le plus court préfère les chemins à bande passante plus élevée par rapport aux chemins à bande passante plus faible.</p>
                        </li>
                    </ul>
                    <p>Généralement, le même poids est asocié aux deux arêtes dirigées qui correspondent à un lien physique (c'est-à-dire <span class="em">R1 &#8594; R2</span> et <span class="em">R2 &#8594; R1</span>). Cependant, rien dans les protocoles d'état de lien ne l'exige. Par exemple, si le poids est défini en fonction de la bande passante de lien, alors une liaison ADSL asymétrique pourrait avoir un poids différent pour les directions amont et aval. D'autres variantes sont possibles. Certains réseaux utilisent des algorithmes d'optimisation pour trouver le meilleur ensemble de poids pour minimiser la congestion à l'intérieur du réseau pour une demande de trafic donnée [FRT2002].</p>
                    <p>Lorsqu'un routeur à état de liaison démarre, il doit d'abord découvrir à quels autres routeurs il est directement connecté. Pour cela, chaque routeur envoie un message HELLO toutes les <span class="em">N</span> secondes sur toutes ses interfaces. Ce message contient l'adresse du routeur. Chaque routeur a une adresse unique. Comme ses routeurs voisins envoient également des messages HELLO, le routeur découvre automatiquement à quels voisins il est connecté. Ces messages HELLO ne sont envoyés qu'aux voisins qui sont directement connectés à un routeur, et un routeur ne transmet jamais les messages HELLO qu'il reçoit. Les messages HELLO sont également utilisés pour détecter les pannes de lien et de routeur. Un lien est considéré comme défaillant si aucun message HELLO n'a été reçu du routeur voisin pendant une période de <span class="em">k * N</span> secondes.</p>
                    <figure>
                        <img src="../images/echange_messages_HELLO.png" alt="">
                        <figcaption>Figure 5.13 : L'échange de messages HELLO</figcaption>
                    </figure>
                    <p>Une fois qu'un routeur a découvert ses voisins, il doit distribuer de manière fiable ses liens locaux à tous les routeurs du réseau pour leur permettre de calculer leur vue locale de la topologie du réseau. Pour cela, chaque routeur construit un <span class="html">paquet d'état de liaison (LSP pour Link-State Packet)</span> contenant les informations suivantes :</p>
                    <ul>
                        <li>
                            <p><span class="html">LSP.Router</span> : identification (adresse) de l'émetteur du LSP</p>
                        </li>
                        <li>
                            <p><span class="html">LSP.age</span> : âge ou durée de vie restante du LSP</p>
                        </li>
                        <li>
                            <p><span class="html">LSP.seq</span> : numéro de séquence du LSP</p>
                        </li>
                        <li>
                            <p><span class="html">LSP.Links[]</span> : liens annoncés dans le LSP. Chaque lien dirigé est représenté avec les informations suivantes :</p>
                            <ul>
                                <li>
                                    <p><span class="html">LSP.Links[i].Id</span> : identification du voisin.</p>
                                </li>
                                <li>
                                    <p><span class="html">LSP.Links[i].cost</span> : cout du lien.</p>
                                </li>
                            </ul>
                        </li>
                    </ul>
                    <p>Ces LSP doivent être distribués de manière fiable à l'intérieur du réseau sans utiliser la table de routage du routeur, car ces tables ne peuvent être calculées qu'une fois que les LSP ont été reçus. L'algorithme <span class="html">Flooding</span> est utilisé pour distribuer efficacement les LSP de tous les routeurs. Chaque routeur qui implémente le flooding maintient une <span class="html">base de données d'état de lien (LSDB pour Link-State DataBase)</span> contenant le LSP le plus récent envoyé par chaque routeur. Lorsqu'un routeur reçoit un LSP, il vérfie d'abord si ce LSP est déjà stocké dans sa LSDB. Si tel est le cas, le routeur a déjà distribué le LSP plus tôt et il n'a pas besoin de le transmettre. Sinon, le routeur a déjà distribué le LSP plus tôt et il n'a pas besoin de le transmettre. Sinon, le routeur transmet le LSP sur tous les liens sauf le lien par lequel le LSP a été reçu. Le flooding fiable peut être implémenté en utilisant le pseudo-code suivant.</p>
<pre><code># links is the set of all links on the router
# Router R's LSP arrival on link l
if newer(LSP, LSDB(LSP.Router)) :
    LSDB.add(LSP)
    for i in links :
        if i!=l :
            send(LSP,i)
else:
    # LSP has already been flooded</code></pre>
                    <p>Dans ce pseudo-code, <span class="em">LSDB(r)</span> renvoie le LSP le plus récent provenant du routeur <span class="em">r</span> qui est stocké dans la base de données LSDB. <span class="em">newer(lsp1,lsp2)</span> renvoie vrai si <span class="em">lsp1</span> est plus récent que <span class="em">lsp2</span>. Voir la note ci-dessous pour une discussion sur la façon dont <span class="html">newer</span> peut être implémenté.</p>
                    <hr>
                    <p>Note : Quel est le LSP le plus récent ?</p>
                    <p>Un routeur qui implémente le flooding doit être capable de détecter si un LSP reçu est plus récent que le LSP stocké. Cela nécessite une comparaison entre le numéro de séquence du LSP reçu et le numéro de séquence du LSP stocké dans la base de données d'état de liaison. Le protocole de routage ARPANET [MRR1979] utilisait un numéro de séquence de 6 bits et implémentait la comparaison comme suit RFC 789.</p>
<pre><code>def newer( lsp1, lsp2 ):
    return ( ( ( lsp1.seq &gt; lsp2.seq) and ( (lsp1.seq - lsp2.seq) &lt;= 32) ) or ( ( lsp1.seq &lt; lsp2.seq) and ( (lsp2.seq - lsp1.seq) &gt; 32) ) )</code></pre>
                    <p>Cette comparaison prend en compte l'arithmétique modulo 2<sup>6</sup> utilisée pour incrémenter les numéros de séquence. De manière intuitive, la comparaison divise le cercle de tous les numéros de séquence en deux moitiés. Habituellement, le numéro de séquence de LSP reçu est égal au numéro de séquence de LSP stocké incrémenté de un, mais parfois les numéros de séquence de deux LSP successifs peuvent différer, par exemple si un routeur a été déconnecté du réseau pendant un certain temps. La comparaison ci-dessus a bien fonctionné jusqu'au 27 octobre 1980. Ce jour-là, l'ARPANET s'est complètement écrasé. L'accident était complexe et impliquait plusieurs routeurs. À un moment donné, les LSP <span class="em">40</span> et <span class="em">44</span> d'un des routeurs ont été stockés dans la LSDB de certains routeurs de l'ARPANET. Comme LSP <span class="em">44</span> était le plus récent, il aurait dû remplacer <span class="em">LSP 40</span> sur tous les routeurs. Malheureusement, l'un des routeurs de l'ARPANET souffrait d'un problème de mémoire et le numéro de séquence <span class="em">40</span> (<span class="em">101000</span> en binaire) a été remplacé par <span class="em">8</span> (<span class="em">001000</span> en binaire) dans le routeur défecteux et diffusé. Trois LSP étaient présents dans le réseau et <span class="em">44</span> était plus récent que <span class="em">40</span> qui était plus récent que <span class="em">8</span>, mais malheureusement, <span class="em">8</span> était considéré comme plus récent que <span class="em">44</span>... Tous les routeurs ont commencé à échanger ces trois paquets d'état de lien pour toujours et la seule solution pour récupérer de ce problème était de fermer tout le réseau. (RFC 789)</p>
                    <p>Les protocoles de routage d'état de lien actuels utilisent généralement des numéros de séquence de 32 bits et incluent un mécanisme spécial dans le cas peu probable où un numéro de séquence atteint la valeur maximale (l'utilisation d'un espace de numéros de séquence de 32 bits prend 136 ans si un paquet d'état de lien est généré chaque seconde).</p>
                    <p>Pour traiter le problème de corruption de mémoire, les paquets d'état de lien contiennent un code de contrôle de redondance cyclique (CRC). Ce CRC est calculé par le routeur qui génère le paquet d'état de lien. Chaque routeur doit vérifier le CRC lorsqu'il reçoit ou propage un paquet d'état de lien. De plus, chaque routeur doit vérifier périodiquement les CRC des paquets d'état de lien stockés dans sa base de données d'état de lien.</p>
                    <hr>
                    <p>La technique de Flooding est illustrée dans la figure ci-dessous. En échangeant des messages HELLO, chaque routeur apprend les routeurs voisins avec lesquels il est directement connecté. Par exemple, le routeur <span class="em">E</span> qu'il apprend qu'il est directement connecté aux routeurs <span class="em">D</span>, <span class="em">B</span> et <span class="em">C</span>. Son premier LSP a un numéro de séquence <span class="em">0</span> et contient les liens <span class="em">E&#8592;D</span>, <span class="em">E&#8592;B</span> et <span class="em">E&#8592;C</span>. Le routeur <span class="em">E</span> envoie son LSP sur toutes ses liaisons et les routeurs <span class="em">D</span>, <span class="em">B</span> et <span class="em">C</span> insèrent le LSP dans leur LSDB et le transmettent sur leurs autres liens.</p>
                    <figure>
                        <img src="../images/exemple_flooding.png" alt="">
                        <figcaption>Figure 5.14 : Flooding : exemple</figcaption>
                    </figure>
                    <p>Le flooding permet aux LSP d'être distribuées à tous les routeurs à l'intérieur du réseau sans se fier aux tables de routage. Dans l'exemple ci-dessus, il est probable que la LSP envoyée par le routeur <span class="em">E</span> soit envoyée deux fois sur certains liens du réseau. Par exemple, les routeurs <span class="em">B</span> et <span class="em">C</span> reçoivent la LSP de <span class="em">E</span> presque en même temps et la transmettent sur le lien <span class="em">B-C</span>. Pour éviter d'envoyer deux fois la même LSP sur chaque lien, une solution possible consiste à modifier légèrement le pseudo-code ci-dessus pour qu'un routeur attende un temps aléatoire avant de transférer une LSP sur chaque lien. L'inconvénient de cette solution est que le délai pour diffuser une LSP à tous les routeurs du réseau augmente. En pratique, les routeurs innondent immédiatement les LSP qui contiennent de nouvelles informations (par exemple, l'ajout ou la suppression d'un lien) et retardent l'inondation des LSP de rafraîchissement (c'est-à-dire les LSP qui contiennent exactement les mêmes informations que la LSP précédente provenant de ce routeur) [FFEB2005].</p>
                    <p>Afin de garantir que tous les routeurs reçoivent tous les LSP, même lorsqu'il y a des erreurs de transmission, les protocoles de routage à état de lien utilisent le <span class="html">flooding fiable</span>. Avec le flooding fiable, les routeurs utilisent des accusés de réception et, si nécessaire, des retransmissions pour s'assurer que tous les paquets d'état de lien sont transférés avec succès à tous les routeurs voisins. Grâce au flooding fiable, tous les routeurs stockent dans leur base de données d'&tat de lien le LSP le plus récent envoyé par chaque routeur du réseau. En combinant les LSP reçus avec son propre LSP, chaque routeur peut calculer la topologie de l'ensemble du réseau.</p>
                    <figure>
                        <img src="../images/db_etat_liaison_recues_tous_routeurs.png" alt="">
                        <figcaption>Figure 5.15 : Base de données d'état de liaison reçues par tous les routeurs.</figcaption>
                    </figure>
                    <hr>
                    <p>Note : Métriques de lien statiques ou dynamiques ?</p>
                    <p>Comme les paquets d'état de liaison sont régulièrement diffusés, les routeurs sont caoables de mesurer la qualité (par exemple, le délai ou la charge) de leurs liens et d'ajuster la métrique de chaque lien en fonction de sa qualité actuelle. De tels ajustements dynamiques ont été inclus dans le protocole de roitage ARPANET [MRR1979]. Cependant, l'expérience a montré qu'il était difficle d'ajuster les ajustements dynamiques et de garantir qu'aucune boucle de renvoi ne se produit dans le réseau [K71989]. Les protocoles de routage d'état de liaison actuels utilisent des métriques qui sont configurées manuellement sur les routeurs et ne sont modifiées que par les opérateurs de réseau ou les outils de gestion de réseau [FRT2002].</p>
                    <hr>
                    <p>Lorsqu'un lien échoue, les deux routeurs attachés au lien détectent la panne par l'absence de messages HELLO reçus dans les dernières <span class="em">k * N</span> secondes. Une fois qu'un routeur a détecté une défaillance de lien locale, il génère et diffuse un nouveau LSP qui ne contient plus le lien défaillant et le nouveau LSP remplace le LSP précédent dans le réseau. Comme les deux routeurs attachés à un lien ne détectent pas cette panne exactement en même temps, certains liens peuvent être annoncés dans une seule direction. Cela est illustré dans la figure ci-dessous. Le routeur <span class="em">E</span> a détecté les pannes du lien <span class="em">E-B</span> et a diffusé un nouveau LSP, mais le routeur <span class="em">B</span> n'a pas encore détecté la panne.</p>
                    <figure>
                        <img src="../images/verification_connectivite_bidirectionnelle.png" alt="">
                        <figcaption>Figure 5.16 : La vérification de la connectivité bidirectionnelle</figcaption>
                    </figure>
                    <p>Lorsqu'un lien est signalé dans le LSP d'un seul des routeurs connectés, les routeurs considèrent que le lien a échoué et le suppriment du graphe orienté qu'ils calculent à partir de leur LSDB. Cela s'appelle la <span class="html">vérification de connectivité à deux voies</span>. Cette vérification permet de diffuser rapidement les pannes de liaison car un seul LSP est suffisant pour annoncer une telle mauvaise nouvelle. Cependant, lorsqu'un lien se rétablit, il ne peut être utilisé que lorsque les deux routeurs connectés ont envoyé leurs LSP. La <span class="em">vérification de connectivité à deux voies</span> permet également de faire face aux pannes de routeur. Lorsqu'un routeur tombe en panne, tous ses liens échouent par définition. Malheureusement, il ne peut pas, bien sûr, envoyer un nouvel LSP pour annoncer sa défaillance. La <span class="em">vérification de connectivité à deux voies</span> garantit que le routeur en panne est supprimé du graphe.</p>
                    <p>Lorqu'un routeur a échoué, son LSP doit être supprimé de la LSDB de tous les routeurs. Il convient de noter que le routage à état de lien suppose que tous les routeurs du réseau disposent de suffisamment de mémoire pour stocker l'ensemble de la LSDB. Les routeurs qui n'ont pas suffisamment de mémoire pour stocker l'ensemble de la LSDB ne peuvent pas participer au routage à état de lien. Certains protocoles à état de lien permettent aux routeurs de signaler qu'ils n'ont pas suffisamment de mémoire et doivent être supprimés du graphe par les autres routeurs du réseau. Cela peut être fait en utilisant le champ <span class="em">age</span> qui est inclus dans chaque LSP. Le champ <span class="em">age</span> est utilisé pour limiter la durée de vie maximale d'un paquet d'état de lien dans le réseau. Lorsqu'un routeur génère un LSP, il définit sa durée de vie (généralement mesurée en secondes) dans le champ <span class="em">age</span>. Tous les routeurs décrémentent régulièrement l'<span class="em">age</span> des LSP de leur LSDB et un LSP est supprimé une fois que son <span class="em">age</span> atteint <span class="em">0</span>. Grâce au champ <span class="em">age</span>, le LSP d'un routeur défaillant ne reste pas indéfiniment dans les LSDB.</p>
                    <p>Pour calculer sa table de routage, chaque routeur calcule l'arbre de recouvrement enraciné en lui-même en utilisant l'algorithme du plus court chemin de Dijkstra [Dijkstra1959]. La table de routage peut être dérivée automatiquement de l'arbre de recouvrement, comme illustré dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/calcul_table_routage.png" alt="">
                        <figcaption>Figure 5.17 : Calcul de la table de routage</figcaption>
                    </figure>
                    <h4>5.1.2 Internet Protocol (IP) :</h4>
                    <p>Le protocole Internet (IP) est le protocole de couche réseau de la suite de protocoles TCP/IP. IP permet aux applications fonctionannt au-dessus de la couche de transport (UDP/TCP) d'utiliser une large gamme de couches de liaison de données hétérogènes. IP a été conçu à une époque où la plupart des liaisons point à point étaient des lignes téléphoniques avec des modems. Depuis, IP a été capable d'utiliser des réseaux locaux (Ethernet, Token Ring, FDDI, ...), de nouvelles technologies de couche de liaison de données étendues (X.25, ATM, Frame Relay, ...) et plus récemment des réseaux sans fil (802.11, 802.15, UMTS, GPRS, ...). La flexibilité d'IP et sa capacité à utiliser divers types de technologies de couche de liaison de données sous-jacentes constituent l'un de ses principaux avantages.</p>
                    <figure>
                        <img src="../images/IP_modele_reference.png" alt="">
                        <figcaption>Figure 5.18 : IP et le modèle de référence</figcaption>
                    </figure>
                    <p>La version actuelle d'IP est la version 4 spécifiée dans la RFC 791. Nous décrivons d'abord cette version et expliquons ensuite la version 6 d'IP, qui devrait remplacer la version 4 d'IP dans un avenir proche.</p>
                    <p>La version 4 d'IP est le protocole du plan de données de la couche réseau de la suite de protocoles TCP/IP. La conception de la version IPv4 était basée sur les hypothèses suivantes :</p>
                    <ul>
                        <li>
                            <p>IP doit fournir un service de connexion sans fiablité (TCP fournit la fiabilité lorsque l'application en a besoin).</p>
                        </li>
                        <li>
                            <p>IP fonctionne avec le mode de transmission de datagrammes.</p>
                        </li>
                        <li>
                            <p>les adresses IP ont une taille fixe de 32 bits.</p>
                        </li>
                        <li>
                            <p>IP doit être utilisable au-dessus de différents types de couches de liaison de données.</p>
                        </li>
                        <li>
                            <p>Les hôtes IP échangent des paquets de longueur variable.</p>
                        </li>
                    </ul>
                    <p>Les adresses sont une partie importante de tout protocole de couche réseau. À la fin des années 1970, les développeurs d'IPv4 ont conçu IPv4 pour un réseau de recherche qui interconnecterait certains laboratoires de recherche et universités. Pour cette utilisation, les adresses de 32 bits étaient beaucoup plus grandes que le nombre attendu d'hôtes sur le réseau. De plus, 32 bits étaient une belle taille d'adresse pour les routeurs basés sur un logiciel. Aucun des développeurs d'IPv4 ne s'attendait à ce qu'IPv4 devienne aussi largement utilisé qu'il ne l'est aujourd'hui.</p>
                    <p>Les adresses IPv4 sont codées sous forme d'un champ de 32 bits. Elles sont souvent représentées sous forme <span class="html">décimale pointée</span>, c'est-à-dire une séquence de quatre nombres entiers séparés par des points. Le premier nombre entier représente la vaeur décimale du byte le plus significatif de l'adresse IPv4 de 32 bits, ... Par exemple :</p>
                    <ul>
                        <li>
                            <p>1.2.3.4 correspond à 00000001000000100000001100000100.</p>
                        </li>
                        <li>
                            <p>127.0.0.1 correspond à 01111111000000000000000000000001.</p>
                        </li>
                        <li>
                            <p>255.255.255.255 correspond à 11111111111111111111111111111111.</p>
                        </li>
                    </ul>
                    <p>Une adresse IPv4 est utilisée pour identifier une interface sur un routeur ou un hôte. Un routeur possède donc autant d'adresses IPv4 que le nombre d'interfaces qu'il possède dans la couche de liaison de données. La plupart des hôtes ont une seule interface de liaison de données et donc une seule adresse IPv4. Cependant, avec la croissance des réseaux sans fil, de plus en plus d'hôtes ont plusieurs interfaces de liaison de données (par exemple, une interface Ethernet et une interface WiFi). Ces hôtes sont dits <span class="html">"multi-homed" (ou "multi-attaché")</span>. Un hôte multi-attaché avec deux interfaces a donc deux adresses IPv4.</p>
                    <p>Un point important à définir dans un protocole de couche réseau est l'allocation des adresses de couche réseau. Un schéma d'allocation naïf serait de fournir une adresse IPv4 à chaque hôte lorsqu'il est connecté à Internet sur une base de premier arrivé, premier servi. Avec cette solution, un hôte en Belgique pourrait avoir l'adresse 2.3.4.5 tandis qu'un autre hôte situé en Afrique utiliserait l'adresse 2.3.4.6. Malheureusement, cela obligerait tous les routeurs à maintenir une route spécifique vers chaque hôte. La figure ci-dessous montre un réseau d'entreprise simple avec deux routeurs et trois hôtes et les tables de routage associées si de telles adresses isolées étaient utilisées.</p>
                    <figure>
                        <img src="../images/problemes_evolutivite_utilisation_IP_isolees.png" alt="">
                        <figcaption>Figure 5.19 : Problèmes de mise à l'échelle lors de l'utilisation d'adresses IP isolées.</figcaption>
                    </figure>
                    <p>Pour préserver la scalabilité du système de routage, il est important de minimiser le nombre de routes stockées sur chaque routeur. Un routeur ne peut pas stocker et maintenir une route pour chacun des presque 1 milliard d'hôtes connectés à l'Internet d'aujourd'hui. Les routeurs ne doivent stocker que des routes vers des blocs d'adresses et non vers des hôtes individuels. Pour cela, les hôtes sont regroupés en sous-réseaux en fonction de leur emplacement dans le réseau. Un sous-réseau typique regroupe tous les hôtes faisant partie de la même entreprise. Un réseau d'entreprise en généralement composé de plusieurs LAN interconnectés par des routeurs. Un petit bloc d'adresses de l'entreprise est généralement assigné à chaque LAN. Une adresse IPv4 est composé de deux parties : un <span class="html">identifiant de sous-réseau</span> et un <span class="html">identifant d'hôte</span>. L'identifiant de sous-réseau est composé des bits de poids fort de l'adresse et l'identifiant d'hôte est encodé dans les bits de poids faible de l'adresse. Ceci est illustré dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/identifiants_sous-reseau_hote_IPv4.png" alt="">
                        <figcaption>Figure 5.20 : Les identifiants de sous-réseau et d'hôte à l'intérieur d'une adresse IPv4</figcaption>
                    </figure>
                    <p>Lorsqu'un routeur doit transférer un paquet, il doit connaître le sous-réseau de l'adresse de destination pour pouvoir consulter sa table de routage et transférer le paquet. La RFC 791 a proposé d'utiliser les bits d'ordre supérieur de l'adresse pour encoder la longueur de l'identifiant de sous-réseau. Cela a conduit à la définition de trois <span class="html">classes</span> unicast. En plus des classes <span class="em">A</span>, <span class="em">B</span> et <span class="em">C</span>, la RFC 791 a également défini les classes <span class="em">D</span> et <span class="em">E</span> d'adresses IPv4. Les adresses de classe <span class="em">D</span> (resp. <span class="em">E</span>) sont celles dont les bits d'ordre supérieur sont définis à <span class="em">1110</span> (resp. <span class="em">1111</span>). Les adresses de classe <span class="em">D</span> sont utilisées par IP multicast et seront expliquées plus tard. Les adresses de classe <span class="em">E</span> sont actuellement inutilisées, mais il existe des discussions sur des utilisations futures possibles [WMH2008] [FLM2008].</p>
                    <table class="tableBalises" role="presentation">
                        <thead>
                            <tr>
                                <th>Classe</th>
                                <th>Bits de poids fort</th>
                                <th>Longueur de l'identifiant de sous-réseau</th>
                                <th>Nombre de réseaux</th>
                                <th>Adresses par réseau</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Classe A</td>
                                <td class="em">0</td>
                                <td>8 bits</td>
                                <td>128</td>
                                <td>16777216 (2<sup>24</sup>)</td>
                            </tr>
                            <tr>
                                <td>Classe B</td>
                                <td class="em">10</td>
                                <td>16 bits</td>
                                <td>16384</td>
                                <td>65536 (2<sup>16</sup>)</td>
                            </tr>
                            <tr>
                                <td>Classe C</td>
                                <td class="em">110</td>
                                <td>24 bits</td>
                                <td>2097152</td>
                                <td>256 (2<sup>8</sup>)</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>Cependant, ces trois classes d'adresses n'étaient pas assez flexibles. Un sous-réseau de classe <span class="em">A</span> était trop grand pour la plupart des organisations et un sous-réseau de classe <span class="em">C</span> était trop petit. La flexibilité a été ajoutée par l'introduction de <span class="html">sous-réseaux de longueurs variables </span> dans la RFC 1519. Avec les sous-réseaux de longueurs variables, l'identifiant de sous-réseau peut avoir une taille quelconque, de <span class="em">1</span> à <span class="em">31</span> bits. Les sous-réseaux de longueurs variables permettent aux opérateurs de réseau d'utiliser un sous-réseau qui correspond mieux au nombre d'hôtes placés à l'intérieur du sous-réseau. Un identifiant de sous-réseau ou préfixe IPv4 est généralement représenté sous la forme <span class="em">A.B.C.D/p</span> où <span class="em">A.B.C.D</span> est l'adresse de réseau obtenue en concaténant l'identifiant de sous-réseau avec un identifiant d'hôtes contenant uniquement des <span class="em">0</span> et <span class="em">p</span> est la longueur de l'identifant de sous-réseau en bits. Une autre façon de représenter les sous-réseaux IP est d'utiliser des masques de réseau. Un masque de réseau est un champ de 32 bits dont les <span class="em">p</span> bits de poids fort sont mis à <span class="em">1</span> et les bits de poids faible sont mis à <span class="em">0</span>. Le nombre de bits de poids fort mis à <span class="em">1</span> indique la longueur de l'identifiant de sous-réseau. Les masques de réseau sont généralment représentés dans le même format décimal pointé que les adresses IPv4. Par exemple, <span class="em">10.0.0.0/8</span> serait représenté comme <span class="em">10.0.0.0 255.0.0.0</span> alors que <span class="em">192.168.1.0/24</span> serait représenté comme <span class="em">192.168.1.0 255.255.255.0</span>. Dans certains cas, le masque de réseau peut être représenté en hexadécimal. Le tableau ci-dessous fournit des exemples de sous-réseaux IP.</p>
                    <table class="tableBalises" role="presentation">
                        <thead>
                            <tr>
                                <th>Sous-réseau</th>
                                <th>Nombre d'adresses</th>
                                <th>Adresse la plus petite</th>
                                <th>Adresse la plus élevée</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>10.0.0.0/8</td>
                                <td>16777216</td>
                                <td>10.0.0.0</td>
                                <td>10.255.255.255</td>
                            </tr>
                            <tr>
                                <td>192.168.0.0</td>
                                <td>65536</td>
                                <td>192.168.0.0</td>
                                <td>192.168.255.255</td>
                            </tr>
                            <tr>
                                <td>198.18.0.0/15</td>
                                <td>131072</td>
                                <td>198.18.0.0</td>
                                <td>198.19.255.255</td>
                            </tr>
                            <tr>
                                <td>192.0.2.0/24</td>
                                <td>256</td>
                                <td>192.0.2.0</td>
                                <td>192.0.2.255</td>
                            </tr>
                            <tr>
                                <td>10.0.0.0/30</td>
                                <td>4</td>
                                <td>10.0.0.0</td>
                                <td>10.0.0.3</td>
                            </tr>
                            <tr>
                                <td>10.0.0.0/31</td>
                                <td>2</td>
                                <td>10.0.0.0</td>
                                <td>10.0.0.1</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>La figure ci-dessous fournit un exemple simple de l'utilisation des sous-réseaux IPv4 dans un réseau d'entreprise. La longueur de l'identifiant de sous-réseau attribué à un LAN dépend généralement du nombre attendu d'hôtes connectés au LAN. Pour les liaisons point à point, de nombreuses installations ont utilisé des préfixes <span class="em">/30</span>, mais les routeurs récents utilsient maintenant des sous-réseaux <span class="em">/31</span> sur des liaisons point à point RFC 3021 ou n'utilisent même pas d'adresses IPv4 sur de telles liaisons. Une liaison point à point à laquelle aucune adresse IPv4 n'a été attribuée est appelée liaison non numérotée. Voir la section 2.2.7 de la RFC 1812 pour une discussion de ces liaisons non numérotées.</p>
                    <figure>
                        <img src="../images/sous-reseaux_IP_reseau_entreprise_simple.png" alt="">
                        <figcaption>Figure 5.21 ; Sous-réseaux IP dans un réseau d'entreprise simple</figcaption>
                    </figure>
                    <p>Un deuxième problème concernant les adresses de la couche réseau est le schéma d'allocation utilisé pour attribuer des blocs d'adresses aux organisations. Le premier schéma d'allocation était basé sur les différentes classes d'adresses. La réserve d'adresses IPv4 était gérée par un secrétariat qui attribuait des blocs d'adresses selon le principe du premier arrivé, premier servu. De grandes organisations telles qye IBM, BBN, ainsi que Standford ou le MIT ont pu obtenir un bloc d'adresses de classe <span class="em">A</span>. La plupart des organisations ont demandé un bloc d'adresses de classe <span class="em">B</span> contenant 65536 adresses, ce qui convenait à la plupart des entreprises et des universités. Le tableau ci-dessous fournit des exemples de certains blocs d'adresses IPv4 dans l'espace de classe <span class="em">B</span>.</p>
                    <table class="tableBalises" role="presentation">
                        <thead>
                            <tr>
                                <th>Sous-réseau</th>
                                <th>Organisation</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>130.100.0.0/16</td>
                                <td>Ericsson, Sweden</td>
                            </tr>
                            <tr>
                                <td>130.101.0.0/16</td>
                                <td>University of Akron, USA</td>
                            </tr>
                            <tr>
                                <td>130.102.0.0/16</td>
                                <td>The University of Queensland, Australia</td>
                            </tr>
                            <tr>
                                <td>130.103.0.0/16</td>
                                <td>Lotus Development, USA</td>
                            </tr>
                            <tr>
                                <td>130.104.0.0/16</td>
                                <td>Uniserte catholique de Louvain, Belgium</td>
                            </tr>
                            <tr>
                                <td>130.105.0.0/16</td>
                                <td>Open Software Foundation, USA</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>Cependant, l'Internet a été victime de son propre succès à la fin des années 1980, de nombreuses organisations demandaient des blocs d'adresses IPv4 et commençaient à se connecter à l'Internet. La plupart de ces organisations ont demandé des blocs d'adresses de classe <span class="em">B</span>, car les blocs d'adresses de classe <span class="em">A</span> étaient trop grands et en quantité limitée, tandis que les blocs d'adresses de classe <span class="em">C</span> étaient considérés comme trop petits. Malheureusement, il n'y avait que 16384 blocs d'adresses de classe <span class="em">B</span> différents et cet espace d'adressage était rapidement consommé. Par conséquent, les tables de routage maintenues par les routeurs augmentaient rapidement et certains routeurs avaient des difficultés à maintenir toutes ces routes dans leur mémoire limitée. Des exemples de routeurs de cette période incluent les Cisco AGS <a href="http://www.knossos.net.nz/don/wn1.html" target="_blank">http://www.knossos.net.nz/don/wn1.html</a> et AGS+ <a href="http://www.ciscopress.com/articles/article.asp?p=25296" target="_blank">http://www.ciscopress.com/articles/article.asp?p=25296</a>.</p>
                    <figure>
                        <img src="../images/evolution_taille_tables_routage_internet.png" alt="">
                        <figcaption>Figure 5.22 : Évolution de la taille des tables de routage sur Internet (juillet 1988 - décembre 1992 - source : RFC 1518)</figcaption>
                    </figure>
                    <p>Face à ces deux problèmes, l'Internet Engineering Task Force a décidé de développer l'architecture CIDR (Classless Interdomain Routing) RFC 1518. Cette architecture vise à permettre au routage IP de mieux s'échelonner que l'architecture basée sur les classes. CIDR contient trois modifications importantes par rapport à RFC 791.</p>
                    <ul>
                        <li>
                            <p>Les classes d'adresses IP sont obsolètes. Tout équipement IP doit utiliser et prendre en charge des sous-réseaux de longueur variable.</p>
                        </li>
                        <li>
                            <p>Les blocs d'adresses IP ne sont plus alloués selon le principe du premier arrivé, premier servi. Au lieu de cela, CIDR introduit un schéla d'allocation d'adresses hiérarchique.</p>
                        </li>
                        <li>
                            <p>Les routeurs IP doivent utiliser la correspondance de préfixe le plus long lorsqu'ils recherchent une adresse de destination dans leur table de transfert.</p>
                        </li>
                    </ul>
                    <p>Les deux dernières modifications ont été introduites pour améliorer la scalabilité du système de routage IP. Le principal inconvénient du schéma d'allocation de blocs d'adresses premier arrivé, premier servi était que des blocs d'adresses voisins étaient alloués à des organisations très différentes et inversement, des blocs d'adresses très différents étaient alloués à des organisations similaires. Avec CIDR, les blocs d'adresses sont alloués par les Registres IP régionaux (RIR) de manière agréable. Un RIR est responsable d'un grand bloc d'adresses et d'une région. Par exemple, RIPE est le RIR responsable de l'Europe. Un RIR alloue de plus petits blocs d'adresses à partir de son grand bloc à des fournisseurs de services Internet RFC 2050. Les fournisseurs de services Internet allouent ensuite de plus petits blocs d'adresses à leurs clients. Lorsqu'une organisation demande un bloc d'adresses, elle doit prouver qu'elle possède déjà ou qu'elle s'attend à avoir dans un avenir proche un nombre d'hôtes ou de clients équivalent à la taille du bloc d'adresses demandé.</p>
                    <p>Le principal avantage de ce schéma hiérarchique d'allocation de blocs d'adresses est qu'il permet aux routeurs de maintenir moins de routes. Par exemple, considérons les blocs d'adresses qui ont été alloués à certaines universités belges comme indiqué dans le tableau ci-dessous.</p>
                    <table class="tableBalises" role="presentation">
                        <thead>
                            <tr>
                                <th>Bloc d'adresses</th>
                                <th>Organisation</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>130.103.0.0/16</td>
                                <td>Universite catholique de Louvain</td>
                            </tr>
                            <tr>
                                <td>134.58.0.0/16</td>
                                <td>Katholiek Universiteit Leuven</td>
                            </tr>
                            <tr>
                                <td>138.48.0.0/16</td>
                                <td>Facultes universitaires Notre-Dame de la Paix</td>
                            </tr>
                            <tr>
                                <td>139.165.0.0/16</td>
                                <td>Univerte de Liege</td>
                            </tr>
                            <tr>
                                <td>164.15.0.0/16</td>
                                <td>Universite Libre de Bruxelles</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>Ces universités sont toutes connectées à Internet exclusivement via <span class="em">Belnet</span>. Comme chaque univerté a été attribuée à un bloc d'adresses différent, les routeurs de Belnet doivent annoncer une route pour chaque université et tous les routeurs sur Internet doivent maintenir une route vers chaque univerté. En revanche, considérons toutes les hautes écoles et les institutions gouvernementales qui sont connectées à Internet via Belnet. Un bloc d'adresses a été attribué à ces institutions après l'introduction de CIDR dans le bloc d'adresses <span class="em">193.190.0.0/15</span> appartenant à Belnet. Avec CIDR, Belnet peut annoncer une seule route vers <span class="em">193.190.0.0/15</span> qui couvre toutes ces hautes écoles.</p>
                    <p>Cependant, il y a une difficulté avec les sous-réseaux à longueur variable agrégeables utilisés par CIDR. Prenons par exemple <span class="em">FEDICT</span>, une institution gouvernementale qui utilise le bloc d'adresses <span class="em">193.191.244.0/23</span>. Supposons que, en plus d'être connecté à Internet via Belnet, FEDICT souhaite également être connecté à un autre fournisseur d'accès Internet. Le réseau FEDICT est alors dit "multi-homed". Ceci est montré dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/multihoming_CIDR.jpg" alt="">
                        <figcaption>Figure 5.23 : Multi-attachement et CIDR</figcaption>
                    </figure>
                    <p>Avec un réseau multi-homed tel que celui-ci, les routeurs span.em{R1} et <span class="em">R2</span> auraient deux routes vers l'adresse IPv4 <span class="em">193.191.245.88</span> : une route via Belnet (<span class="em">193.190.0.0/15</span>) et une route directe (<span class="em">193.191.245.88</span>). Les deux routes correspondent à l'adresse IPv4 <span class="em">193.191.145.88</span>. Depuis la RFC 1519, lorsqu'un routeur connaît plus routes vers la même adresse de destination, il doit tansférer les paquets le long de la route ayant la plus longue longueur de préfixe. Dans le cas de <span class="em">193.191.235.88</span>, il s'agit de la route <span class="em">193.181.244.0/23</span> qui est utilisée pour acheminer le paquet. Cette règle de transfert est appelée <span class="html">correspondance de préfixe le plus long</span> ou <span class="html">correspondance la plus spécifique</span>. Tous les routeurs IPv4 implémentent cette règle de transfert.</p>
                    <p>Pour comprendre transfert de la <span class="em">correspondance de préfixe le plus long</span>, considérons la figure ci-dessous. Avec cette règle, la route <span class="em">0.0.0.0/0</span> joue un rôle particulier. Comme cette route a une longueur de préfixe de <span class="em">0</span> bits, elle correspond à toutes les adresses de destination. Cette route est souvent appelée la route <span class="em">par défaut</span>.</p>
                    <ul>
                        <li>
                            <p>Un paquet avec une destination <span class="em">192.168.1.1</span> reçu par le routeur <span class="em">R</span> est destiné au routeur lui-même. Il est livré au protocole de transport approprié.</p>
                        </li>
                        <li>
                            <p>Un paquet avec une destination <span class="em">11.2.3.4</span> corrspond à deux routes : <span class="em">11.0.0.0/8</span> et <span class="em">0.0.0.0/0</span>. Le paquet est transféré sur l'interface <span class="em">Ouest (West en anglais)</span></p>
                        </li>
                        <li>
                            <p>Un paquet avec une destination <span class="em">120.4.3.4</span> corrspond à une route : <span class="em">0.0.0.0/0</span>. Le paquet est transféré sur l'interface <span class="em">Nord (North en anglais)</span>.</p>
                        </li>
                        <li>
                            <p>Un paquet avec une destination <span class="em">4.4.5.6</span> correspond à deux routes : <span class="Em">4.0.0.0/8</span> et <span class="em">0.0.0.0/0</span>. Le paquet est transféré sur l'interface <span class="em">Ouest (West en anglais)</span>.</p>
                        </li>
                        <li>
                            <p>Un paquet avec une destination <span class="em">4.10.11.254</span> correspond à trois routes : <span class="em">4.0.0.0/8</span>, <span class="em">4.10.11.0/24</span> et <span class="em">0.0.0.0/0</span>. Le paquet est transféré sur l'interface <span class="em">Sud (South en anglais)</span>.</p>
                        </li>
                    </ul>
                    <figure>
                        <img src="../images/exmple_correspondance_prefixe_plus_long.png" alt="">
                        <figcaption>Figure 5.24 : Exemple de la correspondance de préfixe le plus long</figcaption>
                    </figure>
                    <p>La correspondance de préfixe le plus long peut être implémentée en utilisant différentes structures de données. Une possibilité est d'utiliser un arbre. La figure ci-dessous montre un arbre qui code six routes ayant différentes interfaces sortantes.</p>
                    <figure>
                        <img src="../images/arbre_representant_table_routage.png" alt="">
                        <figcaption>Figure 5.25 : Un arbre représentant une table de routage</figcaption>
                    </figure>
                    <hr>
                    <p>Note : Adresses IPv4 spéciales :</p>
                    <p>La plupart des adresses IPv4 unicast peuvent apparaître en tant qu'adresses source et de destination dans les paquets sur l'Internet mondial. Cependant, il convient de noter que certains blocs d'adresses IPv4 ont une utilisation spéciale, comme décrit dans la RFC 5735.</p>
                    <p>Celles-ci comprennent :</p>
                    <ul>
                        <li>
                            <p><span class="html">0.0.0.0/8</span>, qui est réservé à l'auto-identification. Une adresse courante dans ce bloc est <span class="em">0.0.0.0</span>, qui est parfois utilisée lorsqu'un hôte démarre et ne connaît pas encore son adresse IPv4.</p>
                        </li>
                        <li>
                            <p><span class="html">127.0.0.0/8</span>, qui est réservé aux adresses de boucle locale (loopback en anglais). Chaque hôte implémentant IPv4 doit avoir une interface de bouclage (qui n'est pas attachée à une couche de liaison de données). Par convention, l'adresse IPv4 <span class="em">127.0.0.1</span> est assignée à cette interface. Cela permet aux processus s'exécutant sur un hôte d'utiliser TCP/IP pour contacter d'autres propcessus s'exécutant sur le même hôte. Cela peut être très utilse à des fins de test.</p>
                        </li>
                        <li>
                            <p><span class="html">10.0.0.0/8</span>, <span class="html">172.16.0.0/12</span> et <span class="html">192.168.0.0/16</span> sont réservés pour les réseaux privés qui ne sont pas directement attachés à l'Internet. Ces adresses sont souvent appelées adresses privées ou adresses RFC 1918.</p>
                        </li>
                        <li>
                            <p><span class="html">169.254.0.0/24</span> est utilisé pour les adresses link-local RFC 3927. Certains hôtes utilisent une adresse dans ce bloc lorsqu'ils sont connectés à un réseau qui n'alloue pas d'adresses comme prévu.</p>
                        </li>
                    </ul>
                    <hr>
                    <h5>Paquets IPv4 :</h5>
                    <p>Maintenant que nous avons clarifié l'allocation des adresses IPv4 et l'utilisation de la correspondance de préfixes la plus longue pour faire suivre les paquets IPv4, nous pouvons examiner plus en détail IPv4 en commençant par le format des paquets IPv4. Le format des paquets IPv4 a été défini dans la RC 791. Mis à part quelques carifications et quelques changements compatibles en arrière, le format des paquets IPv4 n'a pas beaucoup changé depuis la publication de la RFC 791. Tous les paquets IPv4 utilisent l'en-tête de 20 octets montré ci-dessous. Certains paquets IPv4 contiennent une extension d'en-tête facultative qui est décrite plus tard.</p>
                    <figure>
                        <img src="../images/entete_IPv4.png" alt="">
                        <figcaption>Figure 5.26 : L'en-tête IPv4</figcaption>
                    </figure>
                    <p>Les principaux champs de l'en-tête IPv4 sont :</p>
                    <ul>
                        <li>
                            <p>Un champ <span class="html">version</span> de 4 bits qui indique la version d'IP utilisée pour construire l'en-tête. L'utilisation d'un champ de version dans l'en-tête permet au protocole de couche réseau d'évoluer.</p>
                        </li>
                        <li>
                            <p>Un champ <span class="html">longueur d'en-tête IP (IHL pour IP Header Length)</span> de 4 bits qui indique la longueur de l'en-tête IP en mots de 32 bits. Ce champ permet à IPv4 d'utiliser des options si nécessaire, mais comme il est encodé sur 4 bits, l'en-tête IPv4 ne peut pas dépasser 64 octets.</p>
                        </li>
                        <li>
                            <p>Un champ <span class="html">DS</span> de 8 bits qui est utilis& pour la qualité de service et dont l'utilisation est décrite plus tard.</p>
                        </li>
                        <li>
                            <p>Un champ <span class="html">Protocole</span> de 8 bits qui indique le prototocole de couche de transport qui doit traiter le payload du paquet à la destination. Les valeurs courantes pour ce champ sont <span class="em">6</span> pour TCP et <span class="em">17</span> pour UDP. Voir <a href="http://www.iana.org/assignments/protocol-numbers/" target="_blank">http://www.iana.org/assignments/protocol-numbers/</a> pour la liste de tous les numéros de protocole assignés.</p>
                        </li>
                        <li>
                            <p>Un champ <span class="html">Length</span> de 16 bits qui indique la longueur totale de l'ensemble du paquet IPv4 (en-tête et payload) en octets. Cela implique qu'un paquet IPv4 ne peut pas dépasser 65535 octets.</p>
                        </li>
                        <li>
                            <p>Un champ d'<span class="html">adresse source</span> de 32 bits qui contient l'adresse IPv4 de l'hôte source.</p>
                        </li>
                        <li>
                            <p>Un champ d'<span class="html">adresse de destination</span> de 32 bits qui contient l'adresse IPv4 de l'hôte de destination.</p>
                        </li>
                        <li>
                            <p>Un <span class="html">checksum</span> de 16 bits qui ne protège que l'en-tête IPv4 contre les erreurs de transmission.</p>
                        </li>
                    </ul>
                    <p>Les autres champs de l'en-tête IPv4 sont utilisés à des fins spécifiques. Le premier est le champ <span class="html">Time To Live (TTL)</span> de 8 bits. Ce champ est utilisé par IPv4 pour éviter le risque qu'un paquet IPv4 soit pris dans une boucle infinie en raison d'une erreur transitoire ou permanente dans les tables de routage. La spécification IP initiale dans RFC 791 suggérait que les routeurs décrémenteraient le <span class="em">TTL</span> d'au moins une fois par seonde. Cela garantirait qu'un paquet ne resterait jamais plus de <span class="em">TTL</span> secondes dans le réseau. Cependant, en pratique, la plupart des implémentations de routeurs choisissent simplement de décrémenter le <span class="em">TTL</span> d'une unité. Considérez par exemple la situation représentée dans la figure ci-dessous où la destination <span class="em">D</span> utilise l'adresse <span class="em">11.0.0.56</span>. Si <span class="em">S</span> envoie un paquet vers cette destination, le paquet est transmis au routeur <span class="em">B</span> qui le transmet au routeur <span class="em">C</span> qui le renvoie au routeur <span class="em">A</span>, etc.</p>
                    <figure>
                        <img src="../images/boucles_transfert_reseau_IP.png" alt="">
                        <figcaption>Figure 5.27 : Boucles de transfert dans un réseau IP</figcaption>
                    </figure>
                    <p>Malheureusement, de tels boucles de redirection peuvent se produire pour deux raisons dans les réseaux IP. Premièrement, si le réseau utilise un routage statique, la boucle peut être causée par une simple erreur de configuration. Deuxièmement, si le réseau utilise un routage dynamique, une telle boucle peut se produire de manière transitoire, par exemple pendant la convergence du protocole de routage après une panne de lien ou de routeur. Le champ <span class="em">TTL</span> de l'en-tête IPv4 garantit que même s'il y a des voucles de redirection dans le réseau, les paquets ne boucleront pas indéfiniment. Les hôtes envoient leurs paquets IPv4 avec un <span class="em">TTL</span> positif (généralement <span class="em">64</span> ou plus). La valeur <span class="em">TTL</span> initialise utilisée pour envoyer les paquets IP varie d'une implémentation à l'autre. La plupart des implémentations IP actuelles utilisent un <span class="em">TTL</span> initial de <span class="em">64</span> ou plus. Consultez <a href="http://members.cox.net/~ndav1/self_published/TTL_values.html" target="_blank">http://members.cox.net/~ndav1/self_published/TTL_values.html</a> pour des informations supplémentaires. Lorsqu'un routeur reçoiy un paquet IPv4, il diminue d'abord le <span class="em">TTL</span> de un. Si le <span class="em">TTL</span> devient <span class="em">0</span>, le paquet est rejeté et un message est renvoyé à la source du paquet (voir la section ICMP). Sinon, le routeur effectue une recherche dans sa table de redirection pour acheminer le paquet.</p>
                    <p>Un deuxième problème pour IPv4 est l'hétérogénéité de la couche de liaison de données. IPv4 est utilisé au-dessus de nombreuses couches de liaison de données très différentes. Chaque couche de liaison de données a ses propres caractéristiques et, comme indiqué précédemment, chaque couche de liaison de données est caractérisée par une taille de trame maximale. Du point de vue de l'IP, une interface de couche de liaison de données est caractérisée par son <span class="html">unité de transmission maximale (MTU pour Maximum Transmission Unit)</span>. La MTU d'une interface est le plus grand paquet IPv4 (y compris l'en-tête) qu'elle peut envoyer. Le tableau ci-dessous fournit quelques tailles de MTU courantes. La prise en charge d'IP sur la technologie de couche de liaison de données 802.15.4 nécessite des mécanismes spéciaux. Consultez la RFC 4944 pour une discussion des problèmes spéciaux posés par 802.15.4.</p>
                    <table class="tableBalises" role="presentation">
                        <thead>
                            <tr>
                                <th>Couche liaison de données</th>
                                <th>MTU</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Ethernet</td>
                                <td>1500 octets</td>
                            </tr>
                            <tr>
                                <td>WiFi</td>
                                <td>2272 octets</td>
                            </tr>
                            <tr>
                                <td>ATM (AAL5)</td>
                                <td>9180 octets</td>
                            </tr>
                            <tr>
                                <td>802.15.4</td>
                                <td>102 ou  81 octets</td>
                            </tr>
                            <tr>
                                <td>Token Ring</td>
                                <td>4464 octets</td>
                            </tr>
                            <tr>
                                <td>FDDI</td>
                                <td>4352 octets</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>Bien que IPv4 puisse envoyer des paquets de 64 Ko, peu de technologies de couche liaison de données utilisées aujourd'hui sont capables d'envoyer un paquet IPv4 de 64 Ko dans une trame. De plus, comme illustré dans la figure ci-dessous, un autre problème est qu'un hôte peut envoyer un paquet qui serait trop grand pour l'une des couches liaison de données utilisées par les routeurs intermédiaires.</p>
                    <figure>
                        <img src="../images/besoin_fragmentation_assemblement.jpg" alt="">
                        <figcaption>Figure 5.28 : Le besoin de fragmentation et de réassemblage</figcaption>
                    </figure>
                    <p>Pour résoudre ces problèmes, IPv4 inclut un mécanisme de fragmentation et de réassemblage de paquets. Les hôtes et les routeurs intermédiaires peuvent tous les deux fragmenter un paquet IPv4 si celui-ci est trop long pour être envoyé via la couche de liaison de données. Dans IPv4, la fragmentation est entièrement réalisée dans la couche IP et un paquet IPv4 de grande taille est fragmenté en deux ou plusieurs paquets IPv4 (appelés fragments). Les fragments IPv4 d'une paquet de grande taolle sont des paquets IPv4 normaux qui sont envoyés vers la destination du paquet de grande taille par des routeurs intermédiaires.</p>
                    <p>Le mécanisme de fragmentation d'IPv4 repose sur quatre champs de l'en-tête IPv4 : la <span class="html">longueur (Length)</span>, l'<span class="html">identification (Identification)</span>, les <span class="html">indicateurs de fragmentation (flags)</span> et l'<span class="html">offset de fragmentation (Fragment Offset)</span>. L'en-tête IPv4 contient deux indicateurs de fragmentation (flags) : <span class="html">More fragments</span> et <span class="html">Don't Fragment (DF)</span>. Lorsque l'indicateur (flag) <span class="em">DF</span> est activé, cela indique que le paquet ne peut pas être fragmenté.</p>
                    <p>Le fonctionnement de base de la fragmentation IPv4 est le suivant. Un paquet de grande taille est fragmenté en deux ou plusieurs fragments. La taille de tous les fragments, sauf le dernier, est égale à l'unité de transmission maximale du lien utilisé pour transmettre le paquet. Chaque paquet IPv4 contient un champ d'<span class="em">identification</span> de 16 bits. Lorsq'un paquet est fragmenté, l'<span class="em">identification</span> du paquet de grande taille est copiée dans tous les fragments pour permettre à la destination de réassembler les fragments reçus. Dans chaque fragment, le décalage de fragment indique, en unités de 8 octets, la position du payload du fragment dans le payload du paquet d'origine. Le champ de <span class="em">longueur</span> de chaque fragment indique la longueur du payload du fragment comme dans un paquet IPv4 normal. Enfin, le drapeau <span class="em">More fragments</span> est activé uniquement dans le dernier fragment d'un paquet de grande taille.</p>
                    <p>Le pseudo-code suivant détaille la fragmentation IPv4, en supposant que le paquet ne contient pas d'options.</p>
<pre><code>#mtu : maximum size of the packet (including header) of outgoing link
    if p.len &lt; mtu :
        send(p)
    # packet is too large
    maxpayload=8*int((mtu-20)/8) # must be n times 8 bytes
    if p.flags=='DF' :
        discard(p)
    # packet must be fragmented
    payload=p[IP].payload
    pos=0
    while len(payload) &gt; 0 :
        if len(payload) &gt; maxpayload :
            toSend=IP(dest=p.dest,src=p.src,
                        ttl=p.ttl, id=p.id,
                        frag=p.frag+(pos/8),
                        len=mtu, proto=p.proto)/payload[0:maxpayload]
            pos=pos+maxpayload
            payload=payload[maxpayload+1:]
        else
            toSend=IP(dest=p.dest,src=p.src,
                        ttl=p.ttl, id=p.id,
                        frag=p.frag+(pos/8),
                        flags=p.flags,
                        len=len(payload), proto=p.proto)/payload
        forward(toSend)</code></pre>
                    <p>Les fragments d'un paquet IPv4 peuvent arriver à destination dans n'importe quel ordre, car chaque fragment est transmis indépendamment dans le réseau et peut suivre des chemins différents. De plus, certains fragments peuvent être perdus et ne jamais atteindre la destination.</p>
                    <p>L'algorithme de réassemblage urilisé par l'hôte de destination est frossièrement le suivant. Tout d'abord, la destination peut vérifier si un paquet IPv4 reçu est un fragment ou non en vérifiant la valeur du bit <span class="em">More fragments</span> et de l'<span class="em">offset de fragment</span>. Si l'<span class="em">offset de fragment</span> est fixé à <span class="em">0</span> et que le bit <span class="em">More fragments</span> est réinitialisé, le paquet reçu n'a pas été fragmenté. Sinon, le paquet a été fragmenté et doit être réassemblé. L'algorithme de réassemblage repose sur le champ <span class="em">Identification</span> des fragments reçus pour associer un fragment au paquet correspondant en cours de réassemblage. De plus, le champ <span class="em">Fragment Offset</span> indique la position du payload du fragment dans le paquet original non fragmenté. Enfin, le paquet avec le bit <span class="em">More fragments</span> réinitialisé permet à la destination de déterminer la longueur totale du paquet original non fragmenté.</p>
                    <p>Notez que l'algoritjme de réassemblage doit faire face à l'absence de fiabilité du réseau IP. Cela implique qu'un fragment peut être dupliqué ou qu'n fragment peut ne jamais atteindre la destination. La destination peut facilement détecter la duplication de fragments grâce à l'<span class="em">offset de fragment</span>. Pour faire face à la perte de fragments, l'algorithle de réassemblage doit limiter le temps pendant lequel les fragments d'un paquet sont stockés dans son tampon pendant que le paquet est en cours de réassemblage. Cela peut être implémenté en démarrant un timer lorsque le premier fragment est reçu. Si le paquet n'a pas été réassemblé à l'expiration du timer, tous les fragments sont jetés et le paquet est considéré comme perdu.</p>
                    <p>La spécification IP originale, dans la RFC 791, a défini plusieurs types d'options qui peuvent être ajoutées à l'en-tête IP. Chaque option est encodée en utilisant un format de <span class="html">valeur de longueur de type</span>. Elles ne sont pas largement utilisées aujourd'hui et sont donc seulement brièvement décrites. Des détails supplémentaires peuvent être trouvés dans la RFC 791.</p>
                    <p>Les options les plus intéressantes d'IPv4 sont les trois options liées au routage. L'option <span class="html">Record route</span> a été définie pour permettre aux gestionnaires de réseau de déterminer le chemin suivi par un paquet. Lorsque l'option <span class="em">Record route</span> était présente, les routeurs sur le chemin du paquet devaient insérer leur adresse IP dans l'option. Cette option a été implémentée, mais comme la partie optionnelle de l'en-tête IPv4 ne peut contenir que 44 octets, il est impossible de découvrir l'ensemble du chemin sur l'Internet mondial. <span class="html">traceroute(8)</span>, malgré ses limites, est une meilleure solution pour enregister le chemin vers une destination.</p>
                    <p>Les autres options de routage sont l'option de <span class="html">route source stricte (Strict source route)</span> et l'option de <span class="html">route source lâche (Loose source route)</span>. L'idée principale derrière ces options est qu'un hôte peut souhaiter, pour une raison quelconque, spécifier le chemin à suivre par les paquets qu'il envoie. L'option de <span class="em">route source stricte</span> permet à un hôte d'indiquer à l'intérieur de chaque paquet le chemin exact à suivre. L'option de <span class="em">route source stricte</span> contient une liste d'adresses IPv4 et un pointeur pour indiquer la prochaine adresse dans la liste. Lorsqu'un routeur reçoit un paquet contenant cette option, il ne cherche pas l'adresse de destination dans sa table de routage mais transfère le paquet directement au prochain routeur de la liste et avance le pointeur. Ceci est illustré dans la figure ci-dessous où <span class="em">S</span> force ses paquets à suivre le chemin <span class="em">RA-RB-RD</span>.</p>
                    <figure>
                        <img src="../images/utilisation_option_route_source_stricte.png" alt="">
                        <figcaption>Figure 5.29 : Utilisation de l'option de route source stricte</figcaption>
                    </figure>
                    <p>La longueur maximale de la partie optionnelle de l'en-tête IPv4 constitue une limitation importante pour l'option de <span class="em">route source stricte</span> et l'option de <span class="em">route d'enregistrement (Record route)</span>. Cependant, l'option de <span class="em">route source lâche</span> ne souffre pas de cette limitation. Cette option permet à l'hôte émetteur d'indiquer à l'intérieur de son paquet certains des routeurs qui doivent être traversés pour atteindre la destination. Cela est illsutré dans la figure ci-dessous. <span class="em">S</span> envoie un paquet contenant une liste d'adresses et un pointeur vers le prochain routeur de la liste. Initialement, ce pointeur pointe vers <span class="em">RB</span>. Lorsque <span class="em">RA</span> reçoit le paquet envoyé par <span class="em">S</span>, il recherche dans sa table de routage l'adresse pointée dans l'option de <span class="em">route source lâche</span> et non l'adresse de destination. Le paquet est alors transmis au routeur <span class="em">RB</span> qui reconnaît son adresse dans l'option et avance le pointeur. Comme il n'y a plus d'adresse répertoriée dans l'option de <span class="em">route source lâche</span>, <span class="em">RB</span> et d'autres routeurs en aval transfèrent le paquet en effectuant une recherche pour l'adresse de destination.</p>
                    <figure>
                        <img src="../images/utilisation_option_route_source_perte.png" alt="">
                        <figcaption>Figure 5.30 : Utilisation de l'option de route source lâche</figcaption>
                    </figure>
                    <p>Ces deux options sont généralement ignorées par les routeurs car elles causent des problèmes de sécurité (RFC 6274).</p>
                    <h5>ICMP version 4 :</h5>
                    <p>Il est parfois nécessaire pour les routeurs intermédiaires ou l'hôte de destination d'informer l'expéditeur du paquet d'un problème survenu lors du traitement d'un paquet. Dans la suite de protocoles TCP/IP, ce reporting est effectué par l'<span class="html">Internet Control Message Protocol (ICMP)</span>. ICMP est défini fans la RFC 792. Les messages ICMP sont transportés en tant que payload des paquets IP (la valeur de protocole réservée pour ICMP est <span class="em">1</span>). Un message ICMP est composé d'un en-tête de 8 octets est d'un payload de longueur variable qui contient généralment les premiers octets du paquet qui a déclenché la transmission du message ICMP.</p>
                    <figure>
                        <img src="../images/ICMPv4.png" alt="">
                        <figcaption>Figure 5.31 : ICMP version 4 (RFC 792)</figcaption>
                    </figure>
                    <p>Dans l'en-tête ICMP, les champs <span class="html">Type</span> et <span class="html">Code</span> indiquent le type de problème qui a été détecté par l'expéditeur du message ICMP. Le <span class="html">Checksum</span> protège l'intégralité du message ICMP contre les erreurs de transmission et le champ <span class="html">Data</span> contient des informations supplémentaires pour certains messages ICMP.</p>
                    <p>Les principaux types de messages ICMP sont :</p>
                    <ul>
                        <li>
                            <p><span class="html">Inaccessibilité de la destination</span> : un message ICMP <span class="html">Destination</span> est envoyé lorsqu'un paquet ne peut pas être livré à sa destination en raison de problèmes de routage. Différents types d'inacessibilité sont distingués :</p>
                            <ul>
                                <li>
                                    <p><span class="html">Inaccessibilité du réseau</span> : ce message ICMP est envoyé par un routeur qui n'a pas de route pour le sous-réseau contenant l'adresse de destination du paquet.</p>
                                </li>
                                <li>
                                    <p><span class="html">Inaccessibilité de l'hôte</span> : ce message est envoyé par un routeur qui est connecté au sous-réseau contenant l'adresse de destination du paquet, mais cette adresse de destination ne peut pas être atteinte pour le moment.</p>
                                </li>
                                <li>
                                    <p><span class="html">Inacessibilité de protocole</span> : ce message ICMP est envoyé par un hôte de destination qui a reçu un paquet, mais ne prend pas en charge le protocole de transport indiqué dans le champ <span class="html">Protocol</span> du paquet.</p>
                                </li>
                                <li>
                                    <p><span class="html">Inaccessibilité de port</span> : ce message ICMP est envoyé par un hôte de destination qui a reçu un paquet destiné à un numéro de port, mais aucun processus serveur n'est lié à ce port.</p>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <p><span class="html">Fragmentation nécessaire</span> : ce message ICMP est envoyé par un routeur qui reçoit un paquet avec le drapeau <span class="em">Don't Fragment</span> activé et qui est plus grand que la MTU de l'interface de sortie.</p>
                        </li>
                        <li>
                            <p><span class="html">Redirect</span> : ce message ICMP peut être envoyé lorsqu'il y a deux routeurs sur le même LAN. Considérez un LAN avec un hôte et deux routeurs : <span class="em">R1</span> et <span class="em">R2</span>. Supposons que <span class="em">R1</span> soit également connecté au sous-réseau<span class="em">130.104.0.0/16</span> tandis que <span class="em">R2</span> est connecté au sous-réseau <span class="em">138.48.0.0/16</span>. Si un hôte sur le LAN envoie un hôte sur le LAN envoie un paquet vers <span class="em">130.104.1.1</span> à <span class="em">R2</span>, <span class="em">R2</span> doit transférer le paquet à nouveau sur le LAN pour atteindre <span class="em">R1</span>. Ce n'est pas optimal car le paquet est envoyé deux fois sur le même LAN. Dans ce cas, <span class="em">R2</span> pourrait envoyer un message ICMP <span class="html">Redirect</span> à l'hôte pour l'informer qu'il aurait dû envoyer le paquet directemnt à <span class="em">R1</span>. Cela permet de à l'hôte d'envoyer les autres paquets à <span class="em">130.104.1.1</span> directement via <span class="em">R1</span>.</p>
                            <figure>
                                <img src="../images/redirection_ICMP.png" alt="">
                                <figcaption>Figure 5.32 : Redirect ICMP</figcaption>
                            </figure>
                        </li>
                        <li>
                            <p><span class="html">Problème de paramètre</span> : ce message ICMP est envoyé lorsqu'un routeur ou un hôte reçoit un paquet IP contenant une erreur (par exemple, une option invalide).</p>
                        </li>
                        <li>
                            <p><span class="html">Source quench</span> : un routeur était censé envoyer ce message lorsqu'il devait rejeter des paquets en raison de la congestion. Cependant, l'envoi de messages ICMP en cas de congestion n'était pas la meilleure façon de réduire la congestion et depuis l'inclusion d'un mécanisme de contrôle de congestion dans TCP, ce message ICMP est obsolète.</p>
                        </li>
                        <li>
                            <p><span class="html">Temps écoulé (Time Exceeded)</span> : il existe deux types de messages ICMP <span class="em">Temps écoulé</span>.</p>
                            <ul>
                                <li>
                                    <p><span class="html">TTL dépassé (TTL exeeded)</span> : un message <span class="em">TTL dépassé</span> est envoyé par un routeur lorsqu'il jette un paquet IPv4 car son <span class="em">TTL</span> est arrivé à <span class="em">0</span>.</p>
                                </li>
                                <li>
                                    <p><span class="html">Temps de réassemblage dépassé (Reassembly time exceeded)</span> : ce message ICMP est envoyé lorsqu'une destination n'a pas pu réassembler tous les fragments d'un paquet avant l'expiration de son timer de réassemblage.</p>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <p><span class="html">Demande d'écho (Echo request)</span> et <span class="html">réponse d'écho (Echo reply)</span> : ces messages ICMP sont utilisés par le logiciel de débogage réseau <span class="html">ping(8)</span>.</p>
                        </li>
                    </ul>
                    <hr>
                    <p>Note : Attaques de redirection :</p>
                    <p>Les messages de redirection ICMP sont utiles lorsque plusieurs routeurs sont attachés au même LAN que les hôtes. Cependant, ils doivent être utilisés avec prudence car ils créent également un risque de sécurité important. L'une des attaques les plus ennuyeuses dans un réseau IP s'appelle l'attaque de l'homme du milieu. Une telle attaque se produit si un attaquant est capable de recevoir, traiter, éventuellement modifier et renvoyer tous les paquets échangés entre une source et une destination. Comme l'attaquant reçoit tous les paquets, il peut facilement collecter des mots de passe ou des numéros de carte de crédit, voire injecter de fausses informations dans une connexion TCP établie. Les redirections ICMP permettent malheureusement à un attaquant de facilement effectuer une telle attaque. Dans la figure ci-dessus, considérez l'hôte <span class="em">H</span> qui est attaché au même LAN que <span class="em">A</span> et <span class="em">R1</span>. Si <span class="em">H</span> envoie à <span class="em">A</span> une redirection ICMP pour le préfixe <span class="em">138.48.0.0/16</span>, <span class="em">A</span> transmet à <span class="em">H</span> tous les paquets qu'il veut envoyer à ce préfixe. <span class="em">H</span> peut ensuite les transmettre à <span class="em">R2</span>. Pour éviter ces attaques, les hôtes doivent ignorer les messages de redirection ICMP qu'ils reçoivent.</p>
                    <hr>
                    <p><span class="em">ping(8)</span> est souvent utilisé par les opérateurs de réseau pour vérifier qu'une adresse IP donnée est accessible. Chaque hôte est censé répondre avec un message ICMP <span class="em">Echo reply</span> lorsqu'il reçoit un message ICMP <span class="em">Echo request</span>. Jusqu'à il y a quelques années, tous les hôtes répondaient aux messages ICMP <span class="em">Echo request</span>. Cependant, en raison des problèmes de sécurité qui ont affecté les implémentations TCP/IP, de nombreuses implémentations peuvent mainteanant être configurées pour désactiver la réponse aux messages ICMP <span class="em">Echo request</span>. Un exemple d'utilisation de <span class="em">ping(8)</span> est présenté ci-dessous.</p>
<pre><code>PING 130.104.1.1 (130.104.1.1): 56 data bytes
64 bytes from 130.104.1.1: icmp_seq=0 ttl=243 time=19.961 ms
64 bytes from 130.104.1.1: icmp_seq=1 ttl=243 time=22.072 ms
64 bytes from 130.104.1.1: icmp_seq=2 ttl=243 time=23.064 ms
64 bytes from 130.104.1.1: icmp_seq=3 ttl=243 time=20.026 ms
64 bytes from 130.104.1.1: icmp_seq=4 ttl=243 time=25.099 ms
--- 130.104.1.1 ping statistics ---
5 packets transmitted, 5 packets received, 0% packet loss
round-trip min/avg/max/stddev = 19.961/22.044/25.099/1.938 ms</code></pre>
                    <p>Un autre outil de débogage très utile est <span class="html">traceroute(8)</span>. La page de manuel de traceroute décrit cet outil comme "<span class="em">affichant le chemin que les paquets empruntent vers l'hôte réseau</span>". traceroute utilise les messages ICMP <span class="em">TTL dépassé</span> pour découvrir les routeurs intermédiaires sur le chemin vers une destination. Le principe derrière traceroute est très simple. Lorsqu'un routeur reçoit un paquet IP dont le <span class="em">TTL</span> est réglé à <span class="em">1</span>, il décrémente le <span class="em">TTL</span> et est obligé de retourner à l'hôte d'envoi un message ICMP <span class="em">TTL dépassé</span> contenant l'en-tête et les premiers octets du paquet IP rejeté. Pour découvrir tous les routeurs sur un chemin de réseau, une solution simple consiste à d'abord envoyer un paquet dont le <span class="em">TTL</span> est réglé à <span class="em">1</span>, puis un paquet dont le <span class="em">TTL</span> est réglé à <span class="em">2</span>, etc. Un exemple de sortie traceroute est présenté ci-dessous.</p>
<pre><code>traceroute www.ietf.org
traceroute to www.ietf.org (64.170.98.32), 64 hops max, 40 byte packets
    1 CsHalles3.sri.ucl.ac.be (192.168.251.230) 5.376 ms 1.217 ms 1.137 ms
    2 CtHalles.sri.ucl.ac.be (192.168.251.229) 1.444 ms 1.669 ms 1.301 ms
    3 CtPythagore.sri.ucl.ac.be (130.104.254.230) 1.950 ms 4.688 ms 1.319 ms
    4 fe.m20.access.lln.belnet.net (193.191.11.9) 1.578 ms 1.272 ms 1.259 ms
    5 10ge.cr2.brueve.belnet.net (193.191.16.22) 5.461 ms 4.241 ms 4.162 ms
    6 212.3.237.13 (212.3.237.13) 5.347 ms 4.544 ms 4.285 ms
    7 ae-11-11.car1.Brussels1.Level3.net (4.69.136.249) 5.195 ms 4.304 ms 4.329 ms
    8 ae-6-6.ebr1.London1.Level3.net (4.69.136.246) 8.892 ms 8.980 ms 8.830 ms
    9 ae-100-100.ebr2.London1.Level3.net (4.69.141.166) 8.925 ms 8.950 ms 9.006 ms
    10 ae-41-41.ebr1.NewYork1.Level3.net (4.69.137.66) 79.590 ms
       ae-43-43.ebr1.NewYork1.Level3.net (4.69.137.74) 78.140 ms
       ae-42-42.ebr1.NewYork1.Level3.net (4.69.137.70) 77.663 ms
    11 ae-2-2.ebr1.Newark1.Level3.net (4.69.132.98) 78.290 ms 83.765 ms 90.006 ms
    12 ae-14-51.car4.Newark1.Level3.net (4.68.99.8) 78.309 ms 78.257 ms 79.709 ms
    13 ex1-tg2-0.eqnwnj.sbcglobal.net (151.164.89.249) 78.460 ms 78.452 ms 78.292 ms
    14 151.164.95.190 (151.164.95.190) 157.198 ms 160.767 ms 159.898 ms
    15 ded-p10-0.pltn13.sbcglobal.net (151.164.191.243) 161.872 ms 156.996 ms 159.425 ms
    16 AMS-1152322.cust-rtr.swbell.net (75.61.192.10) 158.735 ms 158.485 ms 158.588 ms
    17 mail.ietf.org (64.170.98.32) 158.427 ms 158.502 ms 158.567 ms</code></pre>
                    <p>Le résultat de la commande <span class="em">traceroute(8)</span> ci-dessus montre un chemin de 17 sauts entre un hôte de l'UCLouvain et l'un des principaux serveurs de l'IETF. Pour chaque saut, traceroute fournit l'adresse IPv4 du routeur qui a envoyé le message ICMP et le temps aller-retour mesuré entre la source et ce routeur. Traceroute envoie trois sondes avec chaque valeur de <span class="em">TTL</span>. Dans certains cas, comme pour le 10<sup>ème</sup> saut ci-dessus, les messages ICMP peuvent être reçus à partir d'adresses différentes. Cela est généralemnt dû au fait que différents paquets de la même source ont suivi des chemins différents dans le réseau. Une analyse détaillée de la sortie de traceroute dépasse le cadre de ce document. Des informations supplémentaires peuvent être trouvées dans [ACO+2006] et [DT2007].</p>
                    <p>Une autre utilisation importante des messages ICMP est de découvrir la taille maximale de MTU qui peut être utilisée pour atteindre une destination sans fragmentation. Comme expliqué précédemment, lorsqu'un routeur IPv4 reçoit un paquet qui est plus grand que le MTU de la liaison de sortie, il doit fragmenter le paquet. Malheureusement, la fragmentation est une opération complexe et les routeurs ne peuvent pas la réaliser à la cadence de la ligne [KM1995]. De plus, lorsqu'un segment TCP est transporté dans un paquet IP qui est fragmenté dans le réseau, la perte d'un seul fragment oblige TCP à retransmettre l'ensemble du segment (et donc tous les fragments). Si TCP était capable d'envoyer uniquement des paquets qui ne nécessitent pas de fragmentation dans le réseau, il pourrait retransmettre uniquement les informations qui ont été perdues dans le réseau. De plus, la réassemblage IP pose plusieurs défis à grande vitesse, comme discuté dans la RFC 4963. L'utilisation de la fragmentation IP pour permettre aux applications UDP d'échanger de grands messages soulève plusieurs problèmes de sécurité [KPS2003].</p>
                    <p>ICMP, combiné avec le drapeau <span class="em">DF (Don't fragment)</span> IPv4, est utilisé par les implémentations TCP pour découvrir la taille de MTU la plus grande autorisée pour atteindre un hôte de destination sans provoquer de fragmentation du réseau. C'est le mécanisme de <span class="html">découverte du MTU de chemin (Path MTU discovery)</span> dans la RFC 1191. Une implémentation TCP qui inclut la <span class="em">découverte du MTU de chemin</span> (la plupart le font) demande à la couche IPv4 d'énvoyer tous les segments à l'intérieur de paquest IPv4 d'envoyer tous les segments à l'intérieur de paquets IPv4 ayant le drapeau <span class="em">DF</span> défini. Cela interdit aux routeurs intermédiaires de fragmenter ces paquets. Si un routeur doit transférer un paquet non fragmentable sur une liaison avec un MTU plus petit, il renvoie un message ICMP de fragmentation nécessaire à la source, indiquant le MTU de sa liaison de sortie du routeur. À la réception de ce message ICMP, l'implémentation TCP source ajuste sa taille de segment maximum (MSS) de sorte que les paquets contenant les segments qu'elle envoie puissent être transmis par ce routeur sans nécessiter de fragmentation.</p>
                    <h5>Interactions entre IPv4 et la couche liaison de données :</h5>
                    <p>Comme mentionné dans la première section de ce chapitre, il existe trois types principaux de couches liaison de données : les liens point-à-point, les LAN prenant en charge la diffusion et la multidiffusion et les réseaux <span class="em">NBMA</span>. Deux problèmes importants doivent être abordés lors de l'utilisation d'IPv4 dans ces types de réseaux. Le premier problème est de savoir comment est de savoir comment les paquets IPv4 sont échangés sur le service de couche liaison de données.</p>
                    <p>Sur un lien <span class="em">point-à-point</span>, les adresses IPv4 des dispositifs communicants peuvent être configurées manuellement ou en utilisant un protocole simple. Les adresses IPv4 sont souvent configurées manuellement sur des liens <span class="em">point-à-point</span> entre des routeurs. Lorsque des liens <span class="em">point-à-point</span> sont utilisés pour attacher des hôtes au réseau, la configuration automatique est souvent préférée afin d'éviter des problèmes avec des adresses IPv4 incorrectes. Par exemple, le PPP (protocole point-à-point), spécifié dans la RFC 1661, inclut un protocole de contrôle de réseau IP qui peut être utilisé par le routeur dans la figure ci-dessous pour envoyer l'adresse IPv4 que l'hôte connecté doit configurer pour son interface. La transmision de paquets IPv4 sur un lien <span class="em">point-à-point</span> sera discutée dans le chapitre sur les réseaux locaux (<span class="em">chap:lan)</span>).</p>
                    <figure>
                        <img src="../images/IPv4_liens_point_point.jpg" alt="">
                        <figcaption>Figure 5.33 : IPv4 sur des liens point-à-point</figcaption>
                    </figure>
                    <p>L'utilisation d'IPv4 dans un réseau local (LAN) introduit un problème supplémentaire. Sur un LAN, chaque appareil est identifié par son adresse unique de couche liaison de données. Le service de couche liaison de données peut être utilisé par n'importe quel hôte connecté au LAN pour envoyer une trame à n'importe quel hôte au LAN pour envoyer une trame à n'importe quel autre hôte connecté au même LAN. Pour cela, l'hôte émetteur doit connaître l'adresse de couche liaison de données de l'hôte de destination. Par exemple, la figure ci-dessous montre quatre hôtes connectés au même LAN configurés avec des adresses IPv4 dans le sous-réseau <span class="em">10.0.1.0/24</span> et des adresses de couche liaison de données représentées par un seul caractère. En pratique, la plupart des réseaux locaux utilisent des adresses encodées sur un champ de 48 bits [802]_. Certaines technologies de réseau local récentes utilisent des adresses de 64 bits. Dans ce réseau, si l'hôte <span class="em">10.0.1.22/24</span> veut envoyer un paquet IPv4 à l'hôte ayant l'adresse <span class="em">10.0.1.8</span>, il doit savoir que l'adresse de couche liaison de données de cet hôte est <span class="em">C</span>.</p>
                    <figure>
                        <img src="../images/LAN_simple.png" alt="">
                        <figcaption>Figure 5.34 : Un LAN simple</figcaption>
                    </figure>
                    <p>Dans un réseau simple comme celui illustré ci-dessus, il pourrait être possible de configurer manuellement la correspondance entre les adresses IPv4 des hôtes et les adresses de couche liaison de donnés correpondnat à toute adresse IPv4 sur le même LAN. C'est l'objectif du <span class="html">protocole de résolution d'adresse (ARP pour Address Resolution Protocol)</span> défini dans la RFC 826. ARP est un protocole de couche liaison de données utilisé par IPv4. Il repose sur la capacité du service de couche liaison de données à livrer facilement une trame de diffusion à tous les périphériques connectés au même LAN.</p>
                    <p>Le moyen le plus simple de comprendre le fonctionnement d'ARP est de considérer le réseau simple illustré ci-dessus et de supposer que l'hôte <span class="em">10.0.1.22/24</span> doit envoyer un paquet IPv4 à l'hôte <span class="em">10.0.1.8</span>. Comme cette adresse IP appartient au même sous-réseau, le paquet doit être envoyé directement à sa destination via le service de couche liaison de données. Pour utiliser ce service, l'hôte émetteur doit trouver l'adresse de couche liaison de données qui est attachée à l'hôte <span class="em">10.0.1.8</span>. Chaque hôte IPv4 maintient un cache ARP contenant la liste de toutes les correspondances entre les adresses IPv4 et les adresses de couche liaison de données qu'il connaît. Lorsqu'un hôte IPv4 démarre, son cache ARP est vide. Ainsi, <span class="em">10.0.1.22</span> consulte d'abord son cache ARP. Comme le cache ne contient pas la correspondance demandée, l'hôte <span class="em">10.0.1.22</span> envoie une trame de requête ARP de diffusion sur le LAN. La trame contient l'adresse de couche liaison de données de l'hôte émetteur (<span class="em">A</span>) et l'adresse IPv4 demandée (<span class="em">10.0.1.8</span>). Cette trame de diffusion est reçue par tous les périphériques du LAN et seul l'hôte qui possède l'adresse IPv4 demandée répond en renvoyant une trame de réponse ARP unicast avec la correspondance demandée. À la réception de cette réponse, l'hôte émetteur met à jour son cache ARP et envoie le paquet IPv4 en utilisant le service de couche liaison de données. Pour faire face aux périphériques qui se déplacent ou dont les adresses sont reconfigurées, la plupart des implémentations ARP suppriment les entrées du cache qui n'ont pas été utilisées pendant quelques minutes. Certaines implémentations ré-valident les entrées du cache ARP de temps en temps en envoyant des requêtes ARP. Voir le chapitre 28 de [Benvenuti2005] pour une description de l'implémentation d'ARP dans le noyau Linux.</p>
                    <hr>
                    <p>Note : Problèmes de sécurité avec le protocole ARP : </p>
                    <p>ARP est un vieux protocole largement utilisé qui a malheureusement été conçu à une époque où les problèmes de sécurité n'étaient pas une préoccupation. ARP est presque intrinsèquement peu sûr. Les hôtes utilisant ARP peuvent être sujets à plusieurs types d'attaques. Tout d'abord, un hôte malveillant pourrait créer une attaque de déni de service sur un LAN en envoyant des réponses aléatoires aux requêtes ARP reçues. Cela polluerait le cache ARP des autres hôtes sur le même LAN. Sur un réseau fixe, de telles attaques peuvent être détectées par l'administrateur système qui peut physiquement retirer les hôtes malveillants du LAN. Sur un réseau sans fil, le retrait d'un hôte malveillant est beaucoup plus difficile.</p>
                    <p>Un deuxième type d'aataque concerne les attaques <span class="html">man-in-the-middle</span>. Ce nom est utilisé pour les attaques réseau où l'attaquant est capable de lire et éventuellement des modifier tous les messages envoyés par les appareils attaqués. Une telle attaque est possible sur un LAN. Supposons, dans la figure ci-dessus, que l'hôte <span class="em">10.0.1.9</span> est malveillant et souhaite recevoir et modifier tous les paquets envoyés par l'hôte <span class="em">10.0.1.22</span> à l'hôte <span class="em">10.0.1.8</span>. Cela peut être facilement réalisé si l'hôte <span class="em">10.0.1.9</span> réussi, en envoyant de fausses réponses ARP, à convaincre l'hôte <span class="em">10.0.1.22</span> (resp. <span class="em">10.0.1.8</span>) que son propre adresse de couche liaison de données doit être utilisée pour atteindre <span class="em">10.0.1.8</span> (resp. <span class="em">10.0.1.22</span>).</p>
                    <hr>
                    <p>ARP est utilisé par tous les appareils connectés à un réseau local et qui implémentaent IPv4. Les routeurs ainsi que les hôtes finaux implémentent ARP. Lorsq'un hôte doit envoyer un paquet IPv4 à une destination en dehors de son sous-réseau local, il doit d'abord envoyer le paquet à l'un des routeurs qui se trouvent sur ce sous-réseau. Considérez par exemple le résenté dans la figure ci-dessous. Chaque hôte est configuré avec une adresse IPv4 dans le sous-réseau <span class="em">10.0.1.0/24</span> et utilise <span class="em">10.0.1.1</span> comme routeur par défaut. Pour envoyer un paquet à l'adresse <span class="em">1.2.3.4</span>, l'hôte <span class="em">10.0.1.8</span> devra d'abord connaître la couche liaison de données de son routeur par défaut. Il enverra donc une requête ARP pour <span class="em">10.0.1.1</span>. À la réception de la réponse ARP, l'hôte <span class="em">10.0.1.8</span> met à jour sa table ARP et envoie son paquet dans une trame à son routeur par défaut. Le routeur acheminera ensuite le paquet vers sa destination finale.</p>
                    <figure>
                        <img src="../images/LAN_simple_avec_routeur.jpg" alt="">
                        <figcaption>Figure 5.35 : Un LAN simple avec un routeur</figcaption>
                    </figure>
                    <p>Au début de l'Internet, les adresses IP étaient configurées manuellement sur les hôtes et les routeurs et presque jamais modifiées. Cependant, cette configuration manuelle peut être complexe et provoque souvent des erreurs qui sont parfois difficiles à déboguer. Par exemple, considérez toutes les options qui peuvent être spécifiées par l'utilitaire <span class="em">ifconfig</span> sur les hôtes Unix. Les implémentations TCP/IP récentes sont capables de détecter certaines de ces erreurs de configuration. Par exemple, si deux hôtes sont attachés au même sous-réseau avec la même adresse IPv4, ils ne pourront pas communiquer. Pour détecter ce problème, les hôtes envoient une demande ARP pour leur adresse configurée chaque fois que leur adresse est modifiée (RFC 5227). S'ils reçoivent une réponse à cette demande ARP, ils déclenchent une alarme ou informent l'administrateur système.</p>
                    <p>Pour faciliter la connexion des hôtes aux sous-réseaux, la plupart des réseaux prennent en charge le protocole de configuration dynamique des hôtes (DHCP pour Dynamic Host Configuration Protocol) RFC 2131. Le DHCP permey à un hôte de récupérer automatiquement son adresse IPv4 attribuée. Un serveur DHCP est associé à chaque sous-réseau. En pratique, il y a généralement un serveur DHCP par groupe de sous-réseaux et les routeurs capturent sur chaque sous-réseau les messages DHCP et les transfèrent au serveur DHCP. Chaque serveur DHCP gère un pool d'adresses IPv4 attribuées au sous-réseau. Lorsqu'un hôte est d'abord connecté au sous-réseau, il envoie une demande DHCP dans un segment UDP (le serveur DHCP écoute sur le port 67). Comme l'hôte ne connaît ni son adresse IPv4 ni l'adresse IPv4 du serveur DHCP, ce segment UDP est envoyé à l'intérieur d'un paquet IPv4 dont les adresses source et destination sont respectivement <span class="em">0.0.0.0</span> et <span class="em">255.255.255.255</span>. La demande DHCP peut contenir diverses options telles que le nom de l'hôte, son adresse de couche liaison de données, etc. Le serveur capture la demande DHCP et sélectionne une adresse non attribuée dans son pool d'adresses. Il envoie ensuite l'adresse IPv4 attribuée dans un message de réponse DHCP qui contient l'adresse de couche liaison de données de l'hôte et des informations supplémentaires telles que le masque de sous-réseau de l'adresse IPv4, l'adresse du routeur par défaut ou l'adresse du résolveur DNS. Ce message de réponse DHCP est envoyé dans un paquet IPv4 dont les adresses source et destination sont respectivement l'adresse IPv4 du serveur DHCP et l'adresse de diffusion <span class="em">255.255.255.255</span>. La réponse DHCP spécifie également la durée de vie de l'allocation d'adresse. Cela force l'hôte à renouveler son allocation d'adresse une fois celle-ci expirée. Grâce à la durée de bail limitée, les adresses IP sont automatiquement retournées au pool d'adresses lorsque les hôtes sont éteints. Cela réduit le gaspillage des adresses IPv4.</p>
                    <p>Dans un réseau NBMA, les interactions entre IPv4 et la couche de liaison de données sont plus complexes car le protocole ARP ne peut pas être utilisé comme dans un LAN. Ces réseaux NBMA utilisent des serveurs spéciaux qui stockent les correspondances entre les adresses IP et l'adresse de la couche de liaison de données correspondante. Les réseaux Asynchronous Transfer Mode (ATM), par exemple, peuvent utiliser soit le protocole ATMARP défini dans la RFC 2225, soit le protocole de résolution de prochain saut (NHRP pour NextHop Resolution Protocol) défini dans la RFC 2332. Les réseaux ATM sont moins couramment utilisés aujourd'hui et nous ne décrirons pas en détail le fonctionnement de ces serveurs.</p>
                    <p>Pour simplifier la discussion dans cette section, nous ignorons l'utilisation des options IPv4. Ce n'est pas une limitation sévère car aujourd'hui, les paquets IPv4 contiennent rarement des options. Les détails sur le traitement des options IPv4 peuvent être trouvés dans les RFC pertinentes, telles que la RFC 791. De plus, nous supposons également que seules les liaisons point à point sont utilisées. Nous reportons l'explication du fonctionnement d'IPv4 sur les réseaux locaux à la prochaine section.</p>
                    <p>Un hôte ayant <span class="em">n</span> interfaces de couche liaison de données gère <span class="em">n +1</span> adresses IPv4 :</p>
                    <ul>
                        <li>
                            <p>L'adresse IPv4 <span class="em">127.0.0.1/32</span> assignée par convention à son adresse de bouclage.</p>
                        </li>
                        <li>
                            <p>Une adresse IPv4 <span class="html">A.B.C.D/p</span> assignée à chacune de ses <span class="em">n</span> interfaces de couche liaison de données.</p>
                        </li>
                    </ul>
                    <p>un tel hôte maintient une table de routage contenant une entrée pour son adresse de bouclage et une entrée pour chaque identifiant de sous-réseau attribué à ses interfaces. De plus, l'hôte utilise généralement l'une de ses interfaces comme interface <span class="em">par défaut</span> lorsqu'il envoie des paquets qui ne sont pas destinés à une destination directement connectée. Cela est représenté par la route par défaut : <span class="em">0.0.0.0/0</span> qui est associée à une interface.</p>
                    <p>Lorsqu'un protocole de transport en cours d'exécution sur l'hôte demande la transmission d'un segment, il fournit généralement à la couche IPv4 l'adresse de destination IPv4 en plus du segment. Une implémentation de protocole de transport peut également spécifier si le paquet doit être envoyé avec le drapeau <span class="em">DF</span> activé ou désactivé. Une implémentation TCP utilisant <span class="em">Path MTU discovery</span> demandanderait toujours la transmisison de paquets IPv4 avec le drapeau <span class="em">DF</span> activé. L'implémentation IPv4 effectue d'abord une recherche de préfixe le plus long avec l'adresse de destination dans sa table de routage. La recherche renvoie l'identification de l'interface qui doit être utilisée pour envoyer le paquet. L'hôte peut alors créer le paquet IPv4 contenant le segment. L'adresse IPv4 source du paquet est l'adresse IPv4 de l'hôte sur l'interface renvoyée par la recherche de préfixe le plus long. Le champ de <span class="em">protocole</span> du paquet est défini sur l'identification du protocole de transport local qui a créé le segment. Le champ <span class="em">TTL</span> du paquet est défini sur le <span class="em">TTL</span> par d&faut utilisé par l'hôte. L'hôte doit maintenant choisir l'<span class="em">identification</span> du paquet. Cette <span class="em">identification</span> est importante si le paquet est fragmenté dans le réseau, car elle permet de s'assurer que la destination est en mesure de réassembler les fragments reçus. Idéalement, un hôte émetteur ne devrait jamais envoyer deux fois le même paquet avec la même identification vers le même hôte de destination, afin de garantir que tous les fragments sont correctement réassemblés par la destination. Malheureusement, avec un champ d'<span class="em">identification</span> de 16 bits et un MSL attendu de 2 minutes, cela implique que la bande passante maximale vers une destination donnée est limitée à environ 286 Mbps. Avec un MTU plus réaliste de 1500 octets, cette passante tombe à 6,4 Mbps RFC 4963 si la fragmentation doit être possible. Il convient de noter que seuls les paquets qui peuvent être fraglmentés (c'est-à-dire dont le bit <span class="em">DF</span> est réinitialisé) doivent avoir des champs d'<span class="em">identification</span> différents. Le champ d'<span class="em">identification</span> n'est pas utilisé dans les paquets ayant le bit <span class="em">DF</span> défini. Ceci est très faible et est une autre raison pour laquelle il est fortement recommandé aux hôtes d'éviter la fragmentation. Si, malgré tout cela, le MTU de l'interface de sortie est inférieur à la longueur du paquet, le paquet est fragmenté. Enfin, la somme de contrôle du paquet est calculée avant la transmission.</p>
                    <p>Lorsqu'un hôte reçoit un paquet IPv4 destiné à lui-même, plusieurs opérations doivent être effectuées. Tout d'abord, il doit vérifier la somme de contrôle du paquet. Si la somme de contrôle est incorrecte, le paquet est rejeté. Ensuite, il doit vérifier si le paquet a été fragmenté. Si c'est le cas, le paquet est transmis à l'algorithme de réassemblage décrit précédemment. Sinon, le paquet doit être transmis à la couche supérieure. Cela se fait en examinant le champ <span class="em">Protocol</span> (<span class="em">6</span> pour TCP, <span class="em">17</span> pour UDP). Si l'hôte n'implémente pas le protocole de couche de transport correspondant au champ <span class="em">Protocol</span> reçu, il envoie un message ICMP "<span class="html">protocole inaccessible (Protocol unreachable)</span>" à l'hôte émetteur. Si le paquet reçu contient un message ICMP (champ <span class="em">Protocol</span> défini à <span class="em">1</span>), le traitement est plus complexe. Un message ICMP <span class="em">Echo-reply</span>. Les autres types de messages ICMP indiquent une erreur qui a été causée par un paquet précédemment transmis. Ces messages ICMP sont généralement transmis au protocole de transport qui a envoyé le paquet erroné. Cela peut être fait en inspectant le contenu du message ICMP qui inclut l'en-tête et les premiers 64 bits du paquet erroné. Si le paquet IP ne contenait pas d'options, ce qui est le cas pour la plupart des paquets IPv4, le protocole de transport peut trouver dans les 32 premiers bits de l'en-tête de transport les ports source et destination pour déterminer le flux de transport concerné. Ceci est important pour la découverte de MTU de chemin, par exemple.</p>
                    <p>Lorsqu'un routeur reçoit un paquet IPv4, il doit d'abord vérifier le checksum du paquet. S'il est invalide, le paquet est rejeté. Sinon, le routeur doit vérifier si l'adresse de destination est l'une des adresses IPv4 assignées au routeur. Si tel est le cas, le routeir doit se comporter comme un hôte et traiter le paquet comme décrit précédemment. Bien que les routeurs acheminent principalement les paquets IPv4, ils doivent parfois être accessibles en tant qu'hôtes poar les opérateurs de réseau ou les logiciels de gestion de réseau.</p>
                    <p>Si le paquet n'est pas adressé au routeur, il doit être transmis sur une interface de sortie conformément à la table de routage du routeur. Le routeir commence par décrémenter le <span class="em">TTL</span> du paquet. Si le <span class="em">TTL</span> atteint un <span class="em">0</span>, un message ICMP "<span class="em">TTL Exceeded</span>" est renvoyé à la source. Comme l'en-tête du paquet a été modifié, le checksum doit être recalculé. Heureusement, comme IPv4 utilise un checksum arithmétique, un routeur peut mettre à jour de manière incrémentale le checksum du paquet, comme décrit dans la RFC 1624. Ensuite, le routeur effectue une correspondance de préfixe le plus long pour l'adresse de destination du paquet dans sa table de transfert. Si aucune correspondance n'est trouvée, le routeur doit renvoyer un message ICMP "<span class="em">Destination unreachable</span>" à la source. Sinon, la recherche renvoie l'interface sur laquelle le paquet doit être transmis. Avant de transférer le paquet sur cette interface, le routeur doit d'abord comparer la longueur du paquet avec la MTU de l'interface de sortie. Si le paquet est plus petit que la MTU, il est transféré. Sinon, un message ICMP "<span class="html">Fragmentation needed</span>" est envoyé su le flag <span class="em">DF</span> était activé ou le paquet est fragmenté s'il était désactivé.</p>
                    <hr>
                    <p>Note : Correspondance de préfixe le plus long dans les routeurs IP :</p>
                    <p>Effectuer la correspondance de préfixe le plus long à la ligne sur des routeurs nécessite des structures de données et des algorithmes hautement optimisés. Considérons par exemple une implémentation de la correspondance la plus longue basée sur un arbre Radix sur un lien de 10 Gbps. Sur un tel lien, un routeur peut recevoir 31250000 paquets IPv4 de 40 octets chaque seconde. Pour transférer les paquets à la ligne, le routeur doit traiter un paquet IPv4 toutes les 32 nanosecondes. Cela ne peut pas être réalisé par une implémentation logicielle. Pour une implémentation matérielle, la principale difficulté réside dans le nombre d'accès mémoire nécessaires pour effectuer la correspondance de préfixe le plus long. Des informations supplémentaires sur les algorithmes de correspondance de préfixe le plus long plus rapides peuvent être trouvées dans [Varghese2005].</p>
                    <hr>
                    <h5>IP version 6 :</h5>
                    <p>Dans la fin des années 1980 et le début des années 1990, la croissance de l'Internet posait plusieurs problèmes opérationnels sur les routeurs. Beaucoup de ces routeurs avaient un seul CPU et jusqu'à 1 Mo de RAM pour stocker leur système d'exploitation, des tampons de paquets et des tables de routage. Étant donné le rythme d'attribution de préfixes IPv4 aux entreprises et aux universités désireuses de rejoindre l'Internet, les tables de routage augmentaient très rapidement et certains craignaient que tous les préfixes IPv4 soient rapidement attribués. En 1987, une étude citée dans la RFC 1752 estimait qu'il y aurait 100000 réseaux dans un proche avenir. En août 1990, des estimations indiquaient que l'espace de classe B serait épuisé en mars 1994. Deux types de solutions ont été développées pour résoudre ce problème. La première solution à court terme a été l'introduction du <span class="html">routage interdomaines sans classe (CIDR pour Classless Inter Domain Routing)</span>. Une deuxième solution à court terme a été le mécanisme de <span class="html">traduction d'adresse réseau (NAT pour Network Address Translation)</span>, défini dans la RFC 1631. Le NAT permettait à plusieurs hôtes de partager une seule adresse IP publique, cela est expliqué dans la section <span class="em">Middleboxes</span>.</p>
                    <p>Cependant, en parrallèle avec ces solutions à court terme, qui ont permis à l'Internet IPv4 de rester utilisable jusqu'à présent, le groupe de travail de l'Internet Engineering Task Force a commencé à travailler sur le développemnt d'un remplaçant pour IPv4. Ce travail a commencé par un appel ouvert à propositions, décrit dans la RFC 1550. Plusieurs groupes ont répondu à cet appel avec des propositions pour un protocole Internet de nouvelle génération (IPng) :</p>
                    <ul>
                        <li>
                            <p>TUBA proposé dans les RFC 1347 et 1561.</p>
                        </li>
                        <li>
                            <p>PIP proposé dans la RFC 1621.</p>
                        </li>
                        <li>
                            <p>SIPP proposé dans la RFC 1710.</p>
                        </li>
                    </ul>
                    <p>L'IETF a décidé de poursuivre le développement de l'IPng basé sur la proposition SIPP. Comme IPv5 était déjà utilisée par le protocole expérimental ST-2 défini dans la RFC 1819, le successeur de IPv4 est la IPv6. L'IPv6 initiale définie dans la RFC 1752 a été conçue sur la base des hypothèses suivantes :</p>
                    <ul>
                        <li>
                            <p>Les adresses IPv6 sont encodées en tant que champ de 128 bits.</p>
                        </li>
                        <li>
                            <p>L'en-tête IPv6 a un format simple qui peut être facilement analysé par des dispositifs matériels.</p>
                        </li>
                        <li>
                            <p>Un hôte devrait pouvoir configurer automatiquement son adresse IPv6.</p>
                        </li>
                        <li>
                            <p>La sécurité doit faire partie d'IPv6.</p>
                        </li>
                    </ul>
                    <hr>
                    <p>Note : La taille de l'adresse IPng :</p>
                    <p>Lorsque le travail sur l'IPng a commencé, il était clair que 32 bits étaient trop petits pour coder une adresse IPng et toutes les propositions utilisaient des adresses plus longues. Cependant, il y avait de nombreuses discussions sur la longueur d'adresse la plus appropriée. Une première approche, proposée par SIPP dans la RFC 1710, était d'utiliser des adresses de 64 bits. Un espace d'adressage de 64 bits était 4 milliards de fois plus grand que l'espace d'adressage IPv4 et, en outre, du point de vue de l'implémentation, les processeurs de 64 bits étaient envisagés et les adresses de 64 bits s'intégreraient naturellement dans leurs registres. Une autre approche était d'utiliser un format d'adresse existant. C'était la proposition TUBA (RFC 1347) qui réutilisait les adresses ISO CLNP de 20 octets. Les adresses de 20 octets laissaient de la place pour la croissance, mais l'utilisation d'ISO CLNP n'était pas favorisée par l'IETF en partie pour des raisons politiques, malgré le fait que des implémentations CLNP matures étaient déjà disponibles. 128 bits semblait être un compromis raisonnable à l'époque.</p>
                    <hr>
                    <h5>Architecture d'adressage IPv6 :</h5>
                    <p>L'expérience d'IPv4 a révélé que la scalabilité d'un protocole de couche réseau dépend fortement de son architecture d'adressage. Les concepteurs d'IPv6 ont donc consacré beaucoup d'efforts à définir son architecture d'adressage, décrite dans la RFC 3513. Toutes les adresses IPv6 font 128 bits de long. Cela implique qu'il existe 340282366920938463374607431768211456 (3,4 * 10<sup>38</sup>) adresses IPv6 différentes. Comme la surface de la Terre est d'environ 510072000 km<sup>2</sup>, cela signifie qu'il y a environ 6,67 * 10<sup>23</sup> adresses IPv6 par mètre carré sur Terre. Comparé à IPv4, qui offre seulement 9 adresses par kilomètre carré, cela représente une amélioration significative sur papier.</p>
                    <p>IPv6 prend en charge les adresses unicast, multicast et anycast. Comme avec IPv4, une adresse unicast IPv6 est utilisée pour identifier une interface de couche de liaison de données sur un hôte. Si un hôte dispose de plusieurs interfaces de couche liaison de données (par exemple, une interface Ethernet et une interface WiFi), alors il a besoin de plusieurs adresses IPv6. En général, une adresse unicast IPv6 est structurée comme indiqué dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/structure_adresses_unicast_IPv6.jpg" alt="">
                        <figcaption>Figure 5.36 : Structure des adresses unicast IPv6</figcaption>
                    </figure>
                    <p>Une adresse IPv6 unicast est composée de trois parties :</p>
                    <ol>
                        <li>
                            <p>Un préfixe de routage global qui est assigné à l'ISP qui possède ce bloc d'adresses.</p>
                        </li>
                        <li>
                            <p>Un identifiant de sous-réseau qui identifie un client de l'ISP.</p>
                        </li>
                        <li>
                            <p>Un identifiant d'interface qui identifie une interface particulière sur un équipement terminal.</p>
                        </li>
                    </ol>
                    <p>Dans les déploiements actuels, les identifiants d'interface sont toujours de 64 bits. Cela simplique que bien qu'il y a 2<sup>128</sup> adresses IPv6 différentes, elles doivent être regroupées en 2<sup>64</sup> sous-réseaux. Cela peut sembler gaspiller des ressources, mais utiliser 64 bits pour l'identifant d'hôte permet aux adresses IPv6 d'être autoconfigurées et offre également certains avantages d'un point de vue de la sécurité, comme expliqué dans la section ICMPv6.</p>
                    <hr>
                    <p>Note : représentation textuelle des adresses IPv6 :</p>
                    <p>IIl est parfois nécessaire d'écrire des adresses IPv6 sous forme textuelle, par exemple lors de la configuration manuelle d'adresses ou à des fins de documentation. Le format préféré pour l'écriture des adresses IPv6 est <span class="em">x:x:x:x:x:x:x:x</span>, où les <span class="em">x</span> sont des chiffres hexadécimaux représentant les huit parties de 16 bits de l'adresse. Voici quelques exemples d'adresses IPv6 :</p>
                    <ul>
                        <li>
                            <p>abcd:Eef01:2345:6789:abcd:ef01:2345:6789</p>
                        </li>
                        <li>
                            <p>2001:db8:0:0:8:800:200c:417a</p>
                        </li>
                        <li>
                            <p>fe80:0:0:0:219:e3ff:fed7:1204</p>
                        </li>
                    </ul>
                    <p>Les adresses IPv6 contiennent souvent une longue séquence de bits définis à <span class="em">0</span>. Dans ce cas, une notation compacte a été définies. Avec cette notation, <span class="em">::</span> est utilisé pour indiquer un ou plusieurs groupes de blocs de 16 bits conteant uniquement des bits définis à <span class="em">0</span>. Par exemple :</p>
                    <ul>
                        <li>
                            <p><span class="em">2001:db8:0:0:8:800:200c:417a</span> est représenté comme <span class="em">2001:db8::8:800:200c:417a</span></p>
                        </li>
                        <li>
                            <p><span class="em">ff01:0:0:0:0:0:0:101</span> est représenté comme <span class="em">ff01::101</span></p>
                        </li>
                        <li>
                            <p><span class="em">0:0:0:0:0:0:0:1</span> est représenté comme <span class="em">::1</span></p>
                        </li>
                        <li>
                            <p><span class="em">0:0:0:0:0:0:0:0</span> est représenté comme <span class="em">::</span></p>
                        </li>
                    </ul>
                    <p>Un préfixe IPv6 peut être représenté comme adresse/longueur, où la longueur est la longueur du préfixe en bits. Par exemple, les trois notations ci-dessous correspondent au même préfixe IPv6 :</p>
                    <ul>
                        <li>
                            <p>2001:0db8:0000:cd30:0000:0000:0000:0000/60</p>
                        </li>
                        <li>
                            <p>2001:0db8::cd30:0:0:0:0/60</p>
                        </li>
                        <li>
                            <p>2001:0db8:0:cd30::/60</p>
                        </li>
                    </ul>
                    <hr>
                    <p>En pratique, il existe plusieurs types d'adresses IPv6 unicast. La plupart des adresses IPv6 unicast sont allouées par blocs sous la responsabilité de l'IANA. Les allocations IPv6 actuelles dfont partie du bloc d'adresses <span class="em">2000::/3</span>. Les Registres INternet Régionaux (RIR) tels que RIPE en Europe, ARIN en Amérique du Nord ou AfriNIC en Afrique ont chacun reçu un bloc d'adresses IPv6 qu'ils sous-allouent aux fournisseurs de services Internet de leur région. Les FAI (ISP en anglais) allouent ensuite des adresses à leurs clients.</p>
                    <p>Lorsqu'on considère l'allocation des adresses IPv6, deux types d'allocation d'adresses sont souvent distingués. Les RIR allouent des adresses <span class="html">indépendantes des fournisseurs (PI pour provider-independent)</span>. Les adresses PI sont généralement allouées aux fournisseurs d'accès Internet et aux grandes entreprises qui sont connectées à au moins deux fournisseurs d'accès différents [CSP2009]. Une fois qu'un bloc d'adresses PI a été attribué à une entreprise, cette dernière peut utiliser son bloc d'adresses avec le fournisseur de son choix et changer de fournisseur à volonté. Les fournisseurs d'accès Internet allouent des blocs d'adresses <span class="html">agrégables par les fournisseurs (PA pour provider-aggregatable)</span> à partir de leur propre bloc d'adresses PI à leurs clients. Une entreprise connecté à un seul fournisseur, elle doit changer toutes les adresses qu'elle utilise. Cela peut être un cauchemar d'un point de vue opérationnel et de nombreuses entreprises font pression pour obtenir des blocs d'adresses PI même si elles sont petites et connectées à un seul fournisseur. Les tailles typiques des blocs d'adresses IPv6 sont :</p>
                    <ul>
                        <li>
                            <p><span class="em">/32</span> pour un fournisseur d'accès Internet.</p>
                        </li>
                        <li>
                            <p><span class="em">/48</span> pour une seule entreprise.</p>
                        </li>
                        <li>
                            <p><span class="em">/64</span> pour un seul utilisateur (par exemple, un utilisateur domestique connecté via ADSL).</p>
                        </li>
                        <li>
                            <p><span class="em">/128</span> dans le cas rare où il est connu qu'il n'y aura pas plus d'un hôte final attaché.</p>
                        </li>
                    </ul>
                    <p>Pour les entreprises qui veulent utiliser IPv6 sans être connectées à Internet IPv6, la RFC 4193 définit les adresses <span class="html">Unique Local Unicast (ULA)</span> (<span class="em">fc00::/7</span>). Ces adresses ULA jouent un rôle similaire aux adresses IPv4 privées définies dans la RFC 1918. Cependant, la taille du bloc d'adresseses <span class="em">fc00::/7</span> permet à ULA d'être beaucoup plus flexible que les adresses IPv4 privées.</p>
                    <p>De plus, l'IETF a réservé certaines adresses IPv6 pour une utilisation spéciale. Les deux plus importantes sont :</p>
                    <ul>
                        <li>
                            <p><span class="em">0:0:0:0:0:0:0:1</span> (<span class="em">::1</span> en forme compacte) est l'adresse de bouclage IPv6. C'est l'adresse d'une interface logique qui est toujours active et fonctionne sur les hôtes compatibles IPv6. C'est l'équivalent de <span class="em">127.0.0.1</span> en IPv4.</p>
                        </li>
                        <li>
                            <p><span class="em">0:0:0:0:0:0:0:0</span> (<span class="em">::</span> en forme compacte) est l'adresse IPv6 non spécifiée. C'est l'adresse IPv6 qu'un hôte peut utiliser comme adresse source lorsqu'il essaie d'acquérir une adresse officielle.</p>
                        </li>
                    </ul>
                    <p>Le dernier type d'adresses IPv6 unicast est celui des adresses <span class="html">Link Local Unicast</span>. Ces adresses font partie du bloc d'adresses <span class="em">fe80::/10</span> et sont définies dans la RFC 4291. Chaque hôte peut calculer sa propre adresse locale en concaténant le préfixe <span class="em">fe80::/64</span> avec l'identificateur de 64 bits de son interface. Les adresses locales de liaison peuvent être utilisées lorsque les hôtes connectés à la même liaison (ou réseau local) ont besoin d'échanger des paquets. Elles sont notamment utilisées à des fins de découverte d'adresses et de configuration automatique. Leur utilisation est limitée à chaque liaison et un routeur ne peut pas transférer un paquet dont l'adresse source ou de destination est une adresse locale de liaison. Des adresses locales de liaison ont également été définies pour IPv4 dans la RFC 3927. Cependant, les adresses IPv4 locales de laison ne sont utilisées que lorsqu'un hôte ne peut pas obtenir une adresse IPv4 régulière, par exemple sur un LAN isolé.</p>
                    <figure>
                        <img src="../images/structure_adresses_IPv6_liaison locale.png" alt="">
                        <figcaption>Figure 5.37 : Structure des adrresses IPv6 de liaison locale</figcaption>
                    </figure>
                    <p>Une conséquence importante de l'architecture d'adressage unicast IPv6 et de l'utilisation d'adresses de liaison locale est qu'un hôte IPv6 dispose de plusieurs adresses IPv6. Cela implique qu'une pile IPv6 doit être capable de gérer plusieurs adresses IPv6. Ce n'était pas toujours le cas avec IPv4.</p>
                    <p>RFC 4291 définit un type spécial d'adresse IPv6 anycast. Sur un sous-réseau ayant le préfixe <span class="em">p/n</span>, l'adresse IPv6 dont les <span class="em">128-n</span> bits de poids faibles sont tous à <span class="em">0</span> est l'adresse anycast qui correspond à tous les routeurs de ce sous-réseau. Cette adresse anycast peut être utilisée par les hôtes pour envoyer rapidement un paquet à l'un des routeurs de leur propre sous-réseau.</p>
                    <p>Enfin, RFC 4291 définit la structure des adresses multicast IPv6. La liste complète des adresses multicast IPv6 allouées est disponible sur <a href="http://www.iana.org/assignments/ipv6-multicast-addresses" target="_blank">http://www.iana.org/assignments/ipv6-multicast-addresses</a>. Cette structure est représentée dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/structure_adresse_multicast_IPv6.jpg" alt="">
                        <figcaption>Figure 5.38 : Structure d'adresse multicast IPv6</figcaption>
                    </figure>
                    <p>Les 112 bits de poids faible d'une adresse multicast IPv6 sont l'identifiant du groupe. Les bits de poids fort sont utilisés comme marqueur pour distinguer les adresses multicast des adresses unicast. En particulier, les 4 bits du champ <span class="em">flags</span> indiquent si l'adresse est temporaire ou permanente. Enfin, le champ de portée indique les limites de la diffusion des paquets destinés à une adresse particulière. Une portée de lien local indique qu'un routeur ne doit pas transférer un paquet destiné à une telle adresse multicast. Une portée d'organisation locale indique qu'un paquet envoyé à une adresse de destination multicast de ce type ne doit pas quitter l'organisation. Enfin, la portée mondiale est déstinée aux groupes multicast couvrant l'ensemble de l'Internet mondial.</p>
                    <p>Parmi ces adresses, certaines sont bien connues. Par exemple, tous les systèmes finaux appartiennent automatiquement au groupe multicast <span class="em">ff02::1</span> tandis que tous les routeurs appartiennent automatiquement au groupe multicast <span class="em">ff02::2</span>. Nous discutons de la multidiffusion IPv6 plus tard.</p>
                    <h5>Le format de paquet IPv6 :</h5>
                    <p>Le format de paquet IPv6 a été fortement inspiré par le format de paquet proposé pour le protocole SIPP dans RFC 1710. L'en-tête IPv6 standard défini dans RFC 2460 occupe 40 octets et contient 8 champs différents, comme indiqué dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/entete_IPv6.png" alt="">
                        <figcaption>Figure 5.39 : L'en-tête IPv6 (RFC 2460)</figcaption>
                    </figure>
                    <p>À part les adresses source et de destination, l'en-tête IPv6 contient les champs suivants :</p>
                    <ul>
                        <li>
                            <p><span class="em">Version</span> : un champ de 4 bits défini sur 6 et destiné à permettre à IP d'évoluer à l'avenir si nécessaire.</p>
                        </li>
                        <li>
                            <p><span class="em">Traffic class</span> : ce champ de 8 bits joue un rôle similaire à celui de l'octet <span class="em">DS</span> de l'en-tête IPv4.</p>
                        </li>
                        <li>
                            <p><span class="em">Flow label</span> : ce champ était initialement destiné à être utilisé pour étiqueter les paquets appartenant au même flux. Cependant, à l'heure où nous écrivons ces lignes, il n'y a pas de directive claire sur la façon dont ce champ doit être utilisé par les hôtes et les routeurs.</p>
                        </li>
                        <li>
                            <p><span class="em">Payload length</span> : il s'agit de la taille du payload du paquet en octets. Comme la longueur est encodée sur 16 bits, un paquet IPv6 peut contenir jusqu'à 65535 octets de payload.</p>
                        </li>
                        <li>
                            <p><span class="em">Next Header</span> : ce champ de 8 bits indique le type d'en-tête qui suit l'en-tête IPv6. L'IANA maintient la liste de tous les types d'en-tête suivant alloués sur <a href="http://www.iana.org/assignments/protocol-numbers/" target="_blank">http://www.iana.org/assignments/protocol-numbers/</a>. Le même registre est utilisé pour le champ de transport (par exemple <span class="em">6</span> pour TCP ou <span class="em">17</span> pour UDP) ou d'une option IPv6. Le traitement des options en tant qu'en-tête suivant permet de simplifier le traitement des paquets IPv6 par rapport à IPv4.</p>
                        </li>
                        <li>
                            <p><span class="em">Hop Limit</span> : ce champ de 8 bits indique le nombre de routeurs qui peuvent transférer le paquet. Il est décrémenté de un par chaque routeur et le même but que le champ TTL de l'en-tête IPv4.</p>
                        </li>
                    </ul>
                    <p>En comparaison avec IPv4, les paquets IPv6 sont beaucoup plus simples et plus faciles à traiter par les routeurs. Une première différence importante est qu'il n'y a pas de checksum à l'intérieur de l'en-tête IPv6. C'est principalement parce que toutes les couches de liaison de données et les protocoles de transport incluent un checksum ou un CRC pour protéger leurs trames/segments contre les erreurs de transmission. Ajouter un checksum dans l'en-tête IPv6 aurait obligé chque routeur à recalculer le checksum de tous les paquets, avec un bénéfice limité pour détecter les erreurs. En pratique, un checksum IP permet de détecter les erreurs qui se produisent à l'intérieur des routeurs (par exemple, en raison d'une corruption de mémoire) avant que le paquet n'atteigne sa destination. Cependant, on a constaté que ce bénéfice était trop faible compte tenu de la fiabilité des mémoires actuelles et du coût de calcul du checksum sur chaque routeur.</p>
                    <p>La deuxième différence avec IPv4 est que l'en-tête IPv6 ne supporte pas la fragmentation et la réassemblage. L'expérience avec IPv4 a montré que la fragmentation des paquets dans les routeurs était coûteuse [KM1995] et les développeurs d'IPv6 ont décidé que les routeurs ne fragmenteraient plus les paquets. Si un routeur reçoit un paquet trop long pour être transmis, le paquet est abandonné et le routeur renvoie un message ICMPv6 pour informer l'expéditeur du problème. L'expéditeur peut ensuite soit fragmenté le paquet, soit effectuer la découverte du MTU de chemin. En IPv6, la fragmentation de paquets est effectuée uniquement par la source en utilisant des options IPv6.</p>
                    <p>La troisième différence concerne les options IPv6, qui sont plus simples et plus faciles à traiter que les options IPv4.</p>
                    <hr>
                    <p>Note : Compression d'en-tête sur les liaisons à faible bande passante :</p>
                    <p>Les options IPv6 sont des champs variables qui peuvent être ajoutés au-delà de l'en-tête IPv6 de base pour fournir des fonctionnalités supplémentaires. Les options IPv6 sont plus simples et plus faciles à traiter que les options IPv4. Les options IPv6 sont encapsulées dans un en-tête d'option IPv6 et sont traitées de manière séquentielle. La dernière option IPv6 a un indicateur "<span class="em">Next Header</span>" qui indique le type de l'en-tête qui suit les options. Les options IPv6 peuvent être utilisées pour fournir un support de sécurité supplémentaire, un routage souple, une qualité de service et une fragmentation de paquets. L'utilisation de ces options est négociée par les hôtes émetteurs, ce qui signifie qu'elles ne sont utilisées que si les deux hôtes les supportent.</p>
                    <hr>
                    <h5>Options IPv6 :</h5>
                    <p>En IPv6, chaque option est considérée comme un en-tête contenant un multiple de 8 octets pour garantir que les options IPv6 dans un paquet sont alignées sur des limites de 64 bits. IPv6 définit plusieurs types d'options :</p>
                    <ul>
                        <li>
                            <p>Les options saut-par-saut sont des options qui doivent être traitées par les routeurs sur le chemin du paquet.</p>
                        </li>
                        <li>
                            <p>L'en-tête de routage de type 0, qui est similaire à l'option de routage source lâche IPv4.</p>
                        </li>
                        <li>
                            <p>L'option de fragmentation, qui est utilisée lors de la fragmentation d'un paquet IPv6.</p>
                        </li>
                        <li>
                            <p>Les options de destination.</p>
                        </li>
                        <li>
                            <p>Les options de sécurité qui permettent aux hôtes IPv6 d'échanger des paquets avec une authentification cryptographique (en-tête AH) ou une authentification et un chiffrement (en-tête ESP).</p>
                        </li>
                    </ul>
                    <p>RFC 2460 fournit de nombreux détails sur les codages des différents types d'options. Dans cette section, nous ne discutons que de certains d'entre eux. Le lecteur peut consulter le RFC 2460 pour plus d'informations sur les autres options. Le premier point à noter est que chaque option contient un champ "<span class="em">Next Header</span>" qui indique le type d'en-tête qui suit l'option. Un deuxième point à noter est que, afin de permettre aux routeurs de parcourir efficacement les paquets IPv6. Les options qui doivent être traitées par les routeurs (options <span class="em">hop-by-hop (saut-par-saut)</span> et l'<span class="em">en-tête de routage de type 0</span>) doivent apparaître en prmeier dans le paquet. Cela permet au routeur de traiter un paquet sans être obligé d'analyser toutes les options du paquet. Un troisième point à noter est que les options saut-par-saut et de destination sont codées en utilisant un format de <span class="em">type-longueur-valeur</span>. De plus, le champ <span class="em">type</span> contient des bits qui indiquent si un routeur qui ne comprend pas cette option doit ignorer l'option ou rejeter le paquet. Cela permet l'introduction de nouvelles options dans le réseau sans obliger tous les dispositifs à être mis à niveau pour les supporter en même temps.</p>
                    <p>Deux options <span class="em">saut-par-saut (hop-by-hop)</span> ont été dénies. RFC 2675 spécifie le jumbogramme qui permet à IPv6 de prendre en charge des paquets contenant un payload supérieur à 65535 octets. Ces paquets jumbo ont leur <span class="em">longueur de payload</span> définie à <span class="em">0</span> et l'option jumbogramme contient la longueur du paquet sous forme d'un champ de 32 bits. De tels paquets ne peuvent être envoyés d'une source à une destination que si tous les routeurs sur le chemin prennent en charge cette option. Cependant, à l'heure actuelle, il ne semble pas que l'option jumbogramme ait été implémentée. L'option d'alerte de routeur définie dans le RFC 2711 est le deuxième exemple d'une option <span class="em">hop-by-hop</span>. Les paquets contenant cette option doivent être traités de manière spéciale par les routeurs intermédiaires. Cette option est utilisée pour les paquets IP qui transportent des messages du <span class="html">protocole de réservation de ressources (RSVP pour Resource Reservation Protocol)</span>. Son utilisation est expliquée plus tard.</p>
                    <p>L'en-tête de routage de type 0 défini dans la RFC 2460 est un exemple d'option IPv6 qui doit être traitée par certains routeurs. Cette option est encodée comme indiqué ci-dessous.</p>
                    <figure>
                        <img src="../images/entete_routage_type_0.png" alt="">
                        <figcaption>Figure 5.40 : L'en-tête de routage de type 0 (RFC 2460)</figcaption>
                    </figure>
                    <p>L'en-tête de routage de type 0 avait pour objectif de permettre à un hôte d'indiquer une route source approximative que devrait suivre un paquet en spécifiant les adresses de certains des routeurs qui doivent faire avancer ce paquet. Malheureusement, des travaux ultérieurs sur cet en-tête de routage y compris une démonstration amusante avec scapy [BE2007], ont révélé des problèmes de sécrurité graves avec cet en-tête de routage. Pour cette raison, le routage source approximatif avec l'en-tête de routage de type 0 a été supprimé de la spécification IPv6 RFC 5095.</p>
                    <p>En IPv6, la fragmentation est effectuée exclusivement par l'hôte source et repose sur l'en-tête de fragmentation. Cet en-tête de 64 bits est composé de six champs :</p>
                    <ul>
                        <li>
                            <p>un champ <span class="em">Next Header</span> qui indique le type d'en-tête qui suit l'en-tête de fraglentation.</p>
                        </li>
                        <li>
                            <p>un champ <span class="em">reserved</span> mis à <span class="em">0</span>.</p>
                        </li>
                        <li>
                            <p>le <span class="em">Fragment Offset</span> est un entier non signé de 13 bits qui contient l'offset, en unités de 8 octets, des données suivant cet en-tête, par rapport au début du paquet d'origine.</p>
                        </li>
                        <li>
                            <p>le drapeau <span class="em">More</span>, qui est mis à <span class="em">0</span> dans le dernier fragment d'un paquet et à <span class="em">1</span> dans tous les autres fragments.</p>
                        </li>
                        <li>
                            <p>le champ <span class="em">Identification</span> de 32 bits indique à quel paquet d'origine appartient un fragment. Lorsq'un hôte envoie des paquets fragmentés, il doit s'assurer qu'il ne réutilise pas le même champ d'identification pour les paquets envoyés à la même destination pendant une période de MSL secondes. Cela est plus facile avec l'identification de 32 bits utilisée dans l'en-tête de fragmentation IPv6, qu'avec le champ d'identification de 16 bits de l'en-tête IPv4.</p>
                        </li>
                    </ul>
                    <p>Certaines implémentations IPv6 envoient les fragments d'un paquet dans l'ordre croissant des fragments d'offset, en commeçant par le premier fragment. D'autres envoient les fragments dans l'ordre inverse, en commençant par le dernier fragment. Cette dernière solution peut être avantageuse pour l'hôte qui doit réassembler les fragments, car il peut facilement allouer le tampon requis pour réassembler tous les fragments du paquet lors de la réception du dernier fragment. Lorsqu'un hôte reçoit le premier fragment d'un paquet IPv6, il ne peut pas connaître à priori la longueur de l'ensemble du paquet IPv6.</p>
                    <p>La figure ci-dessous fournit un exemple d'un paquet IPv6 fragmenté contenant un segment UDP. Le type <span class="em">Next Header</span> réservé pour l'option de fragmentation IPv6 est 44.</p>
                    <figure>
                        <img src="../images/exemple_fragmentation_IPv6.jpg" alt="">
                        <figcaption>Figure 5.41 : Exemple de fragmentation IPv6</figcaption>
                    </figure>
                    <p>Enfin, le dernier type d'options IPv6 est le <span class="html">Payload de Sécurité Encapsulé (ESP pour Encapsulating Security Payload)</span> défini dans RFC 4303 et l'<span class="html">En-tête d'Authentification (AH pour AUthentification Header)</span> défini dans RFC 4302. Ces deux en-têtes sont utilisés par IPSec RFC 4301. Ils sont discutés dans un autre chapitre.</p>
                    <h5>ICMP version 6 :</h5>
                    <p>ICMPv6 défini dans la RFC 4443 est le protocole compagnon d'IPv6, tout comme ICMPv4 est le protocole compagnon d'IPv4. ICMPv6 est utilisé par les routeurs et les hôtes pour signaler les problèmes lors du traitement des paquets IPv6. Cependant, comme nous le verrons dans le chapitre sur <span class="em">la couche de liaison de données et les réseaux locaux</span>, ICMPv6 est également utilisé lors de la configuration automatique des adresses.</p>
                    <p>L'utilisation traditionnelle d'ICMPv6 est similaire à celle d'ICMPv4. Les messages ICMPv6 sont transportés à l'intérieur de paquets IPv6 (le champ <span class="em">Next Header</span> pour ICMPv6 est 58). Chaque message ICMP contient un en-têtete de 8 bits avec un champ de <span class="em">type</span>, un champ de <span class="em">code</span> et une <span class="em">somme de contrôle </span> de 16 bits calculée sur l'ensemble du message ICMPv6. Le corps du message contient une copie du paquet IPv6 en erreur.</p>
                    <figure>
                        <img src="../images/format_paquet_ICMPv6.png" alt="">
                        <figcaption>Figure 5.42 : Format de paquet ICMPv6</figcaption>
                    </figure>
                    <p>ICMPv6 spécifie deux classes de messages : les messages d'erreur qui indiquent un problème de traitement d'un paquet et les messages d'information. Quatre types de messages d'erreur sont définis dans la RFC 4443 :</p>
                    <ul>
                        <li>
                            <p><span class="em">1</span>[<span class="em">Destination Unreachable</span> : Un tel message ICMPv6 est envoyé lorsque l'adresse de destination d'un paquet est inaccessible. Le champ de <span class="em">code</span> de l'en-têtete ICMP contient des informations supplémentaires sur le type d'inaccessibilité. Les codes suivants sont spécifiés dans le RFC 4443 :]</p>
                            <ul>
                                <li>
                                    <p><span class="em">0</span> : Pas de route vers la destination. Cela indique que le routeur qui a envoyé le message ICMPv6 n'avait pas de route vers la destination du paquet.</p>
                                </li>
                                <li>
                                    <p><span class="em">1</span> : Communication avec la destinayion interdite par l'administrateur. Cela indique qu'un pare-feu a refusé de faire suivre le paquet vers sa destination.</p>
                                </li>
                                <li>
                                    <p><span class="em">2</span> : Hors de portée de l'adresse source. Ce message peut être envoyé si la source utilise des adresses de liaison locale pour atteindre une adresse de liaison unicast globzlr en dehors de son sous-réseau.</p>
                                </li>
                                <li>
                                    <p><span class="em">3</span> : Adresse inaccessible. Ce message indique que le paquet a atteint le sous-réseau de la destination, mais que l'hôte qui possède cette adresse de destination ne peut pas être atteint.</p>
                                </li>
                                <li>
                                    <p><span class="em">4</span> : Port inaccessible. Ce message indique que le paquet IPv6 a été reçu par la destination, mais qu'il n'y avait pas d'application à l'écoute du port spécifié.</p>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <p><span class="em">2</span> : Paquet trop grand. Le routeur qui devait envoyer le message ICMPv6 a reçu un paquet IPv6 plus grand que la MTU du lien de sortie. Le message ICMPv6 contient la MTU de ce lien en octets. Cela permet à l'hôte émetteur d'implémenter la découverte du MTU du chemin (Path MTU discovery) RFC 1981.</p>
                        </li>
                        <li>
                            <p><span class="em">3</span> : Temps dépassé. Ce message d'erreur peut être envoyé soit par un routeur soit par un hôte. Un routeur metterait le <span class="em">code</span> à <span class="em">0</span> pour signaler la réception d'un paquet dont le <span class="em">Hop Limit</span> est tombé à <span class="em">0</span>. Un hôte metterait le <span class="em">code</span> à <span class="em">1</span> pour signaler qu'il n'y a pas pu réassembler les fragments IPv6 reçus.</p>
                        </li>
                        <li>
                            <p><span class="em">4</span> : Problème de paramètre. Ce message ICMPv6 est utilisé pour signaler soit la réception d'un paquet IPv6 avec un champ d'en-tête erroné (type <span class="em">0</span>), soit une option IP ou un <span class="em">Next Header</span> inconnu (types <span class="em">1</span> et <span class="em">2</span>). Dans ce cas, le corps du message contient le paquet IPv6 erroné et les 32 premiers bits du corps du message contiennent un pointeur vers l'erreur.</p>
                        </li>
                    </ul>
                    <p>Deux types de messages ICMPv6 informatifs sont définis dans RFC 4443 : la <span class="em">Echo request</span> et la <span class="em">Echo reply</span>, qui sont utilisées pour tester la disponibilité d'une destination en utilisant <span class="em">ping6(8)</span>.</p>
                    <p>ICMPv6 permet également la découverte du chemin entre une source et une destination en utilisant <span class="em">traceroute6(8)</span>. La sortie ci-dessous montre un traceroute entre un hôte à l'UCLouvain et l'un des principaux serveurs de l'IETF. Notez que ce chemin IPv6 est différent du chemin IPv4 décrit précédemment bien que les deux traceroutes aient été effectués simultanément.</p>
<pre><code>traceroute6 www.ietf.org
traceroute6 to www.ietf.org (2001:1890:1112:1::20) from 2001:6a8:3080:2:217:f2ff:fed6:65c0, 30 hop1 2001:6a8:3080:2::1 13.821 ms 0.301 ms 0.324 ms
2 2001:6a8:3000:8000::1 0.651 ms 0.51 ms 0.495 ms
3 10ge.cr2.bruvil.belnet.net 3.402 ms 3.34 ms 3.33 ms
4 10ge.cr2.brueve.belnet.net 3.668 ms 10ge.cr2.brueve.belnet.net 3.988 ms 10ge.cr2.brueve.beln5 belnet.rt1.ams.nl.geant2.net 10.598 ms 7.214 ms 10.082 ms
6 so-7-0-0.rt2.cop.dk.geant2.net 20.19 ms 20.002 ms 20.064 ms
7 kbn-ipv6-b1.ipv6.telia.net 21.078 ms 20.868 ms 20.864 ms
8 s-ipv6-b1-link.ipv6.telia.net 31.312 ms 31.113 ms 31.411 ms
9 s-ipv6-b1-link.ipv6.telia.net 61.986 ms 61.988 ms 61.994 ms
10 2001:1890:61:8909::1 121.716 ms 121.779 ms 121.177 ms
11 2001:1890:61:9117::2 203.709 ms 203.305 ms 203.07 ms
12 mail.ietf.org 204.172 ms 203.755 ms 203.748 m</code></pre>
                    <hr>
                    <p>Note : Limitation de la fréquence des messages ICMP :</p>
                    <p>Les routeurs haut de gamme utilisent des puces spéciales sur leurs interfaces pour acheminer les paquets IPv6 à la vitesse de la ligne. Ces puces sont optimisées pour traiter les paquets IP corrects. Elles ne sont pas capables de créer des messages de créer des messages ICMP à la vitesse de la ligne. Lorsqu'une telle puce reçoit un paquet IP qui déclenche un message ICMP, elle interrompt le CPU principal du routeur et le logiciel s'exécutant sur ce CPU traite le paquet. Ce CPU est beaucoup plus lent que l'accélération matérielle trouvée sur les interfaces [Gill2004]. Il serait surchargé s'il devait traiter les paquets IP à la vitesse de la ligne et générer un message ICMP pour chaque paquet reçu. Pour protéger ce CPU, les routeurs haut de gamme limitent la vitesse à laquelle le matériel peut interrompre le CPU principal et donc la vitesse à laquelle les messages ICMP peuvent être générés. Cela implique que tous les paqutes IP erronés ne provoquent pas la transmission d'un message ICMP. Le risque de surcharge du CPU principal du routeur est également la raison pour laquelle l'utilisation d'options IPv6 <span class="em">Hop-by-hop</span>, y compris l'option d'alerte de routeur, est découragée. Pour une discussion des problèles avec l'option IP d'alerte de routeur, voir <a href="http://tools.ietf.org/html/draft-rahman-rtg-router-alert-dangerous-00" targte="_blank">http://tools.ietf.org/html/draft-rahman-rtg-router-alert-dangerous-00</a> ou <a href="http://tools.ietf.org/html/draft-rahman-rtg-router-alert-considerations-03" target="_blank">http://tools.ietf.org/html/draft-rahman-rtg-router-alert-considerations-03</a>.</p>
                    <hr>
                    <h5>Interactions entre IPv6 et la couche de liaison de données :</h5>
                    <p>Il existe plusieurs différences entre IPv6 et IPv4 lorsqu'on considère leurs interactions avec la couche de liaison de données. En IPv6, les interactions avec la couche de liaison de données. En IPv6, les interactions entre le réseau et la couche de liaison de données sont effectuées à l'aide d'ICMPv6.</p>
                    <p>Tout d'abord, ICMPv6 est utilisé pour résoudre l'adresse de la couche liaison de données correspondant à une adresse IPv6 donnée. Cette partie d'ICMPv6 est le protocole NDP (Neighbour Discovery Protocol) défini dans la RFC 4861. NDP est similaire à ARP, mais il y a deux différences importantes. Premièrement, les meesages NDP sont échangés dans des messages ICMPv6 tandis que les messages ARP sont envoyés sous forme de trames de couche liaison de données. Deuxièmement, une requête ARP est envoyée en tant que trame de diffusion tandis qu'un message de sollicitation NDP est envoyé en tant que trame de diffusion tandis qu'un message de sollicitation NDP est envoyé en tant que paquet ICMPv6 multicast qui est transporté à l'intérieur d'une trame multicast. Le fonctionnement du protocole NDP est similaire à ARP. Pour obtenir une correspondance d'adresse, un hôte envoie un message de sollicitation de voisinage. Ce message est envoyé à l'intérieur d'un message ICMPv6 qui est placé dans un paquet IPv6 dont l'adresse source est l'adresse IPv6 de lhôte demandeur et l'adresse de destination est l'adresse IPv6 multicast de tous les hîtes (FF02::1) auxquels tous les hôtes IPv6 écoutent. La sollicitation de voisinage contient l'adresse IPv6 demandée. Le propriétaire de l'adresse demandée répond en envoyant un message d'annonce de voisinage unicast à l'hôte demandeur. NDP souffre de problèmes de sécurité similaires au protocole ARP. Cependant, il est possible de sécuriser NDP en utilisant les <span class="html">adresses IPv6 générées de manières cryptographique (CGA pour Cryptographically Generated IPv6 Addresses)</span> définies dans la RFC 3972. Le protocole de découverte de voisinage sécurisé est défini dans la RFC 3971, mais une description détaillée de ce protocole est hors du champ d'application de ce chapitre.</p>
                    <p>Les réseaux IPv6 supportent également le protocole de configuration dynmique des hôtes (DHCP). Les extensions IPv6 à DHCP sont définies dans la RFC 3315. Le fonctionnement de DHCPv6 est similaire à celui de DHCP décrit précédemment. En plus de DHCPv6, les réseaux IPv6 prennent en charge un autre mécanisme pour assigner des adresses IPv6. Il s'agit de la configuration d'addresses sans état (SLAC pour Stateless Address Configuration) définie dans la RFC4882. Lorsqu'un h^te démarre, il dérive son identificateur de son adresse de couche de liaison de données et concatène cet identificateur de 64 bits au préfixe <span class="em">FE80::/64</span> pour obtenir son adresse IPv6 de liaison locale. L'utilisation d'une adresse de couche de liaison de données pour dériver un identificateur de 64 bits pour chaque hôte soulevée des préoccupations en matière de confidentialité, car l'hôte utilisera toujours le même identificateur. Les attaquants pourraient l'utiliser pour suivre les hôtes sur Internet. Une extension du mécanisme de configuration d'adresses sans état qui ne soulève pas de préoccupations en matière de confidentialité est définie dans la RFC 4941. Ces extensions de confidentialité permettent à un hôte de générer son identificateur de 64 bits aléatoirement chaque fois qu'il se connecte à un sous-réseau. Il devient alors impossible pour un attaquant d'utiliser l'identificateur de 64 bits pour suivre un hôte, il envoie ensuite une sollicitaion de voisinage avec son adresse de liaison locale comme cible pour vérifier si un autre hôte utilise la même adresse de liaison locale sur ce sous-réseau. S'il reçoit une annonce de voisinage indiquant que l'adresse de liaison locale est utilisée par un autre hôte, il génère un autre identificateur de 64 bits et envoie à nouveau une sollicitation de voisinage. S'il n'y a pas de réponse, l'hôte considère son adresse de liaison locale comme valide. Cette adressse sera utilisée comme adresse source pour tous les messages NDP envoyés sur le sous-réseau. Pour configurer automatiquement son adresse IPv6 globale, l'hôte doit connaître le préfixe IPv6 globalement routable utilisé sur le sous-réseau local. Les routeurs IPv6 envoient régulièrement des messages d'annonce de routeur ICMPv6 qui indiquent le préfixe IPv6 assigné à chaque sous-réseau. À la réception de ce message, l'hôte peut dériver son adresse IPv6 globale en concaténant son identificateur de 64 bits avec le préfixe reçu. Il conclut la SLAC en envoyant un message de sollicitation de voisinage ciblant son adresse IPv6 globale pour s'assurer qu'un autre hôte n'utilise pas la même adresse IPv6.</p>
                    <h5>Boîtes intermédiaires (Middleboxes) :</h5>
                    <p>Lorsque l'architecture TCP/IP et le protocole IP ont été définis, deux types d'appareils ont été considérés dans la couche réseau : les hôtes finaux et les routeurs. Les hôtes finaux sont les sources et les destinations des paquets IP, tandis que les routeurs les acheminent. Lorsq'un routeur achemine un paquet IP, il consulte sa table de routage, met à jour le TTL du paquet, recalcule son checksum et l'achemine vers le prochain saut. Un routeur n'a pas besoin de lire ou de modifier le contenu du payload du paquet.</p>
                    <p>Cependant, dans l'Internet d'aujourd'hui, il existe des appareils qui ne sont pas strictement des routeurs mais qui traitent, parfois modifient et acheminent les paquets IP. Ces appareils sont souvent appelés des middleboxes RFC 3234. Certaines middleboxes ne fonctionnent que dans la couche réseau, mais la plupart des middleboxes sont capables d'analyser le payload des paquets reçus et d'extraire l'en-tête de transport, et dans certains cas les protocoles de la couche application.</p>
                    <figure>
                        <img src="../images/boites_intermediaires_IP_modele_reference.jpg" alt="">
                        <figcaption>Figure 543 : Middleboxes IP et le modèle de référence</figcaption>
                    </figure>
                    <p>Dans cette section, nous décrivons brièvement deux types de moddleboxes : les pare-feu et les dispositifs de traduction d'adresses réseau (NAT). Une discussion des différents types de middleboxes avec des références peut être trouvée dans la RFC 3234.</p>
                    <p>Lorsque l'Internet n'était qu'un réseau de recherche interconnectant des laboratoires de recherche, la sécurité n'était pas une préoccupation et la plupart des hôtes ont accepté d'échanger des paquets via des connexions TCP avec la plupart des autres hôtes. Cependant, à mesure que de plus en plus d'utilisateurs et d'entreprises se sont connectés à l'Ineternet, le fait de permettre un accès illimité aux hôtes qu'ils gérzient a commencé à préoccuper les entreprises. De plus, à la fin des années 1980, plusieurs problèmes de sécurité ont affecté Internet, tels que le premier ver Internet [RE1989] et certaines violations de sécurité largement médiatisées [Stoll1988] [CB2003] [ Cheswick1990].</p>
                    <p>Ces problèmes de sécurité ont convaincu l'industrie que les réseaux IP sont une partie essentielle de l'infrastructure d'une entreprise, qui devrait être protégée par des dispositifs spéciaux comme des gardes de sécurité et des clôtures sont utilisées pour protéger les bâtiments. Ces dispositifs spéciaux ont rapidement été appelés des pare-feu. Un pare-feu typique a deux interfaces :</p>
                    <ul>
                        <li>
                            <p>une interface externe connectée à Internet.</p>
                        </li>
                        <li>
                            <p>une interface interne connectée  à un réseau de confiance.</p>
                        </li>
                    </ul>
                    <p>Les premiers pare-feu étaient dotés de filtres de paquets configurables. Un filtre de paquets est un ensemble de règles définissant la politique de sécurité d'un réseau. En pratique, ces règles reposent sur les valeurs des champs dans les en-têtes de couche IP ou de transport. N'importe quel champ de l'en-tête IP ou de transport peut être utilisé dans une règle de pare-feu, mais les plus courants sont :</p>
                    <ul>
                        <li>
                            <p>Filtrer sur l'adresse de destination. Par exemple, les hôtes du laboratoire de recherche d'une entreprise peuvent recevoir des paquets provenant d'Internet global, mais pas les hôtes du département financier.</p>
                        </li>
                        <li>
                            <p>Filtrer sur le numéro de protocole trouvé dans l'en-tête IP. Par exemple, une entreprise peut autoriser uniquement ses hôtes à utiliser TCP ou UDP, mais pas d'autres protocoles de transport plus expérimentaux.</p>
                        </li>
                        <li>
                            <p>Filtrer sur les numéros de port TCP ou UDP. Par exemple, seul le serveur DNS d'une entreprise devrait recevoir des segments UDP dont le port de destination est défini sur <span class="em">53</span> ou seulement les serveurs SMTP officiels de l'entreprise peuvent envoyer des segments TCP dont les ports source sont définis sur <span class="em">25</span>.</p>
                        </li>
                        <li>
                            <p>Filtrer sur les indicateurs de drapeau TCP. Par exemple, une solution simple pour interdire aux hôtes d'ouvrir des connexions TCP avec les hôtes à l'intérieur de l'entreprise est de rejeter tous les segments TCP reçus depuis l'interface externe avec uniquement le drapeau SYN activé.</p>
                        </li>
                    </ul>
                    <p>Ces pare-feu sont souvent appelés pare-feu sans état car ils ne conservent pas d'état sur les connexions TCP qui passent à travers eux.</p>
                    <p>Un autre type de pare-feu est le pare-feu <span class="em">étatique</span>. Un pare-feu étatoque suit l'état de chaque connexion TCP qui le traverse et maintient un TCB pour chacune de ces connexions TCP. Ce TCB lui permet de réassembler les segments reçus afin d'extraire leur payload et de procéder à des vérifications dans la couche application. Certains parfeu sont capables d'inspecter les URLs accédées en utilisant HTTP et de journaliser toutes les URLs visitées ou de bloquer les connexions TCP où une URL dangereuse est échangée. Certains pare-feu peuvent vérifier que des commandes SMTP sont utilisées lorsqu'une connexion TCP est établie sur le port <span class="em">25</span> ou qu'une connexion TCP sur le port <span class="em">80</span> transporte des commandes et des réponses HTTP.</p>
                    <hr>
                    <p>Note : Au-delà des pare-feu :</p>
                    <p>En dehors des pare-feu, différents types de dispositifs de sécurité ont été installés à la périphérie des réseaux d'entreprise. Les systèmes de détection d'intrusion (IDS pour Intrusion Detection Systems), tels que le populaire <span class="em">Snort</span>, sont des dispositifs étatiques capables de faire correspondre les segments reconstitués à des expressions régulières correspondant à des signatures de virus, de vers ou d'autres types d'attaques. L'inspection approfondie de paquets (DPI pour Deep Packet Inspection est un autre type de boîtier intermédiaire qui analyse le payload du paquet et est capable de reconstituer des segments TCP afin de détecter des utilisations inappropriées. Bien que les IDS soient principalement utilisés dans les réseaux d'entreprise, le DPI est principalement utilisé chez les fournisseurs de services Internet. Certains fournisseurs de services Internet utilisent le DPI pour détecter et limiter la bande passante consommée par les applications pair-à-pair. Certains pays comme la Chine ou l'Iran utilisent le DPI poyr détecter une utilisation inappropriée d'Internet.</p>
                    <hr>
                    <h5>NAT :</h5>
                    <p>La "Network Address Translation" (NAT) a été proposée dans [TE1993] et RFC 3022 comme une solution à court terme pour faire face à la pénurie prévue d'adresses IPv4 à la fin des années 80 - début des années 90. combinée avec CIDR, NAT a controbué à ralentir considérablement la consommation d'adresses IPv4. Un NAT est une boîte intermédiaire qui interconnecte deux réseaux utilisant des adresses IPv4 de différents espaces d'adressage. Généralement, l'un de ces espaces d'adressage est l'Internet public, tandis que l'autre utilise les adresses IPv4 privées définies dans la RFC 1918.</p>
                    <p>Un déploiement très courant de NAT se trouve dans les routeurs d'accès à large bande, comme le montre la figure ci-dessous. Le routeur d'accès à large bande interconnecte un réseau domestique, soit WiFi soit basé sur Ethernet, et l'Internet global via un FAI sur ADSL ou CATV. Une seule adresse IPv4 est attribuée au routeur d'accès à large bande, et la traduction d'adresse réseau permet à tous les hôtes connectés au réseau domestique de partager une seule adresse IPv4 publique.</p>
                    <figure>
                        <img src="../images/NAT_simple_adresse_IPv4_publique.jpg" alt="">
                        <figcaption>Figure 5.44 : Un NAT simple avec une adresse IPv4 publique</figcaption>
                    </figure>
                    <p>Un deuxième type de déploiement se trouve dans les réseaux d'entreprise, comme le montre la figure ci-dessous. Dans ce cas, la fonctionnalité NAT est installée sur un routeur frontal de l'entreprise. Une adresse IPv4 privée est attribuée à chaque hôte d'entreprise, tandis que le routeur frontal gère une pool contenant plusieurs adresses IPv4 publiques.</p>
                    <figure>
                        <img src="../images/NAT_entreprise_adresses_IPv4_publiques.jpg" alt="">
                        <figcaption>Figure 5.45 : Un NAT d'entreprise avec plusieurs adresses IPv4 publiques</figcaption>
                    </figure>
                    <p>Comme son nom l'indique, un NAT est un dispositif qui "traduit" les adresses IP. Un NAT maintient une table de correspondance entre les adresses IP privées utilisées dans le réseau interne et les adresses IPv4 publiques. NAT permet à un grand nombre d'hôtes de partager une pool d'adreses IP, car ces hôtes n'accèdent pas tous à l'Internet global en même temps.</p>
                    <p>Le NAT le plus simple est une boîte intermédiaire qui utilise une correspondance un-à-un entre une dresse IP privée et une adresse publique. Pour comprendre son fonctionnement, supposons qu'un NAT, tel que celui montré ci-dessus, vient d'être démarré. Lorsque le NAT reçoit le premier paquet de la source <span class="em">S</span> dans le réseau interne qui est destiné à l'Internet public, il crée une correspondance entre l'adresse interne <span class="em">S</span> et la première adresse de sa pool d'adresses publiques (<span class="em">P1</span>). Ensuite, il traduit le paquet reçu afin qu'il puisse être envoyé à l'Internet public. Cette traduction est effectuée comme suit :</p>
                    <ul>
                        <li>
                            <p>L'adresse source du paquet (<span class="em">S</span>) est remplacée par l'adresse publique correspondante (<span class="em">P1</span>).</p>
                        </li>
                        <li>
                            <p>La somme de contrôle de l'en-tête IP est mis à jour de manière incrémentielle car son contenu a changé.</p>
                        </li>
                        <li>
                            <p>Si le paquet contenait un segment TCP ou UDP, la somme de contrôle de la courche transport trouvée dans le segment inclus doit également être mise à jour car elle est calculée sur le segment et un psuedo-en-tête qui inclus les adresses source et de destination.</p>
                        </li>
                    </ul>
                    <p>Lorsqu'un paquet destiné à <span class="em">P1</span> est reçu depuis l'Internet public, le NAT consulte sa table de correspondance pour trouver <span class="em">S</span>. Le paquet reçu est traduit et transmis dans le réseau interne.</p>
                    <p>Cela fonctionne tant que la plage d'adresses IP publiques du NAT n'est pas épuisée. Dans ce cas, une correspondance doit être supprimée de la table de correspondance pour permettre la traduction d'un paquet provenant d'un nouvel hôte. Cette suppression peut être implémentée en ajoutant à chaque entrée de la table de correspondance un horodatage contenant le temps d'utilisation de la dernière entrée de correspondance. Cet horodatage est mis à jour chaque fois que l'entrée correspondante est utilisée. Ensuite, l'algorithme de nettoyage peut supprimer l'entrée de correspondance la plus ancienne de la table.</p>
                    <p>Un inconvénient d'un NAT d'entreprise aussi simple est la taille de la plage d'adresses IPv4 publiques, qui est souvent trop petite pour permettre à un grand nombre d'hôtes de partager un tel NAT. Dans ce cas, une meilleure solution est de permettre au NAT de traduire à la fois les adresses IP et les numéros de port.</p>
                    <p>Un tel NAT maintient une table de correspondance qui associe une adresse IP interne et un numéro de port TCP à une adresse IP externe et un numéro de port TCP. Lorsqu'un tel NAT reçoit un paquet provenant du réseau interne, il effectue une recherche dans la table de correspondance avec l'adresse IP source du paquet et le numéro de port source TCP. Si une correspondance est trouvée, l'adresse IP source et le numéro de port TCP source du paquet sont traduits avec les valeurs trouvées dans la table de correspondance, les sommes de contrôle sont mises à jour et le paquet est envoyé à l'Internet global. Si aucune correspondance n'est trouvée, une nouvelle correspondance est créée avec le premier couple disponible (adresse IP, numéro de port TCP) et le paquet est traduit. Les entrées de la table de correspondance sont soit supprimées à la fin de la connexion TCP correspondante, car le NAT suit l'état de la connexion TCP comme un pare-feu étatique soit après un certain temps d'inactivité.</p>
                    <p>Lorsqu'un tel NAT reçoit un paquet de l'Internet glocal, il recherche dans sa table de correspondance l'adresse IP de destination du paquet et le numéro de port TCP de destination. Si une correspondance est trouvée, le paquet est traduit et retransmis dans le réseau interne. Sinon, le paquet est rejeté car le NAT ne peut pas déterminer à quel hôte interne particulier le paquet doit être transmis. Pour cette raison,</p>
                    <p>Avec 2<sup>16</sup> numéros de port différents, un NAT peut prendre en charge un grand nombre d'hôtes avec une seule adresse IPv4 publique. Cependant, il convient de noter que certaines applications ouvrent un grand nombre de connexions TCP [Miyakawa2008]. Chacune de ces connexions TCP consomme une entrée de correspondance dans la table de correspondance du NAT.</p>
                    <p>Le NAT permet à de nombreux hôtes de partager une ou quelques adresses IPv4 publiques. Cependant, l'utilisation de NAT présente deux inconvénients importants. Premièrement, il est difficile pour les hôtes externes d'ouvrir des connexions TCP avec des hôtes qui se trouvent derrière un NAT. Certains considèrent cela comme un avantage d'un point de vue de la sécurité. Cependant, un NAT ne doit pas être confondu avec un pare-feu car il existe des techniques pour traverser les NAT. Deuxièmement, le NAT rompt la transparence de bout en bout des couches réseau et transport. Le principal problème se pose lorsque le protocole de la couche applicative utilise des adresses IP dans certaines des unités de données applicatives (ADU pour Application Data Unit) qu'il envoie. Un exemple populaire est le protocole FTP défini dans la RFC 959. Dans ce cas, il y a une incohérence entre l'en-tête de paquet traduit par le NAT et le payload du paquet. La seule solution pour résoudre ce problème est de placer une passerelle de niveau applicatif (ALG pour Application Level Gateway) sur le NAT qui comprend le protocole de la couche applicative et peut ainsi traduire les adresses IP et les numéros de port trouvés dans les ADU. Cependant, la définition d'un ALG pour chaque application est coûteuse et les développeurs d'applications devraient éviter d'utiliser des adresses IP dans les messages échangés dans la couche applicative RFC 3235.</p>
                    <hr>
                    <p>Note : IPv6 et NAT :</p>
                    <p>NAT a été très réussi avec IPv4. Compte tenu de la taille de l'espace d'adressage IPv6, les concepteurs d'IPv6 ont prévu que NAT ne serait jamais utile avec IPv6. La transparence de bout en bout d'IPv6 a été l'un de ses points forts par rapport à IPv4. Cependant, la pénurie attendue d'adresses IPv4 a incité les administrateurs de réseau d'entreprise à considérer plus sérieusement IPv6. L'un des résultats de cette analyse est que l'IETF a défini des dispositifs NAT RFC 6296 spécifiques à IPv6. Un autre usage de NAT avec IPv6 est de permettre aux hôtes IPv6 d'accéder à des destinations IPv4 et voce versa. Les premières spécifications IPv6 comprenaient le mécanisme de traduction de protocole NAT (NAT-PT) défini dans la RFC 2766. Ce mécanisme a ensuite été déconseillé dans la RFC 4966 mais a été récemment relancé sous le nom de NAT64 RFC 6144. Un NAT63 est une boîte intermédiaire qui effectue la traduction de paquets IPv6&#9456;IPv4 pour permettre aux hôtes IPv6 de contacter des serveurs IPv3 RFC 6144.</p>
                    <hr>
                    <h4>5.1.3 Routage dans les réseaux IP :</h4>
                    <p>Dans un réseau IP étendu tel que l'Internet global, les routeurs doivent échanger des informations de routage. L'Internet est une interconnexion de réseaux, souvent appelés domaines, qui sont sous des responsabilités différentes. Au moment de la rédaction de ce texte, l'Internet est composé de plus de 30000 domaines différents et ce nombre est toujours en croissance. Un domaine peut être une petite entreprise qui gère quelques routeurs dans un seul bâtiment, une entreprise plus importante avec une centaine de routeurs à plusieurs endroits, ou un grand fournisseur de services Internet qui gère des milliers de routeurs. Deux classes de protocoles de routage sont utilisées pour permettre à ces domaines d'échanger efficacement des informations de routage.</p>
                    <figure>
                        <img src="../images/organisation_petit_internet.jpg" alt="">
                        <figcaption>Figure 5.46 : Organisation d'un petit Internet.</figcaption>
                    </figure>
                    <p>La première classe de protocoles de routage sont les <span class="html">protocoles de routage intradomaine</span> (parfois appelés protocoles de passerelle intérieure ou <span class="html">IGP pour Interior Gateway Protocols</span>). Un protocole de routage intradomaine est utilisé par tous les routeurs à l'intérieur du domaine. Il existe plusieurs protocoles de routage intradomaine. Certains domaines utilisent <span class="html">RIP</span>, qui est un protocole de vecteur de distance. D'autres domaines utilisent des protocoles de routage à état de liaison tels que <span class="html">OSPF</span> ou <span class="html">IS-IS</span>. Enfin, certains domaines utilisent le routage statique ou des protocoles propriétaires tels que <span class="html">IGRP</span> ou <span class="html">EIGRP</span>.</p>
                    <p>Ces protocoles de routage intradomaine ont généralement deux objectifs. Tout d'abord, ils distribuent des informations de routage correspondant au chemin le plus court entre deux routeurs du domaine. Deuxièmement, ils doivent permettre aux routeurs de récupérer rapidement en cas de panne de liaison ou de routeur.</p>
                    <p>La deuxième classe de protocoles de routage est constituée des <span class="html">protocoles de routage interdomaines</span>( parfois appelés protocoles de passerelle externe ou <span class="html">EGP pour Exterior Gateway Protocols</span>). L'objectif d'un protocole de routage interdomaines est de distribuer des informations de routage entre les domaines. Pour des raisons de scalabilité, un protocole de routage interdomaines doit distribuer des informations de routage agrégées et considère chaque domaine comme une boîte noire.</p>
                    <p>Une différence très importante entre le routage intra-domaine et inter-domaine réside dans les <span class="html">politiques de routage</span> utilisées par chaque domaine. À l'intérieur d'un seul domaine, tous les routeurs sont considérés comme égaux, et lorsque plusieurs routes sont disponibles pour atteindre un préfixe de destination donné, la meilleure route est sélectionnée en fonction de critères techniques tels que la route avec le délai le plus court, la route avec le nombre minimal de sauts ou la route avec la bande passante la plus élevée.</p>
                    <p>Lorsque nous considérons l'interconnexion de domaines gérés par des organisations différentes, cela n'est plus vrai. Chaque domaine met en oeuvre sa propre politique de routage. Une politique de routage est composée de trois éléments : un filtre d'importation qui spécifie quelles routes peuvent être acceptées par un domaine, un filtre d'exportation qui spécifie quelles routes peuvent être annoncées par un domaine et un domaine et un algorithme de classement qui sélectionne la meilleure route lorsque plusieurs routes sont connues vers le même préfixe de destination. Comme nous le verrons plus tard, une autre différence importante est que l'objectif du protocole de routage interdomaine est de trouver la route la moins chère vers chaque destination. Il n'y a qu'un seul protocole interdomaine : <span class="html">BGP</span>.</p>
                    <h5>Routage intradomaine :</h5>
                    <p>Dans cette section, nous décrivons brièvement les caractéristiques clés des deux prinicpaux protocoles de routage unicast intradomaine : span.html{RIP} et <span class="html">OSPF</span>.</p>
                    <h5>RIP :</h5>
                    <p>Le <span class="html">protocole d'informations de routage (RIP Routing Information Protocol)</span> est le plus simple des protocoles de routage standardisés pour la suite de protocoles TCP/IP. Le RIP est défini dans la RFC 2453. Des informations supplémentaires sur le RIP peuvent être trouvées dans [Malkin1999].</p>
                    <p>Les routeurs RIP échangent périodiquement des messages RIP. Le format de ces messages est présenté ci-dessous. Un message RIP est envoyé à l'intérieur d'un segment UDP dont le port de destination est défini à <span class="em">521</span>. Un message RIP contient plusieurs champs. Le champ <span class="html">Cmd</span> indique si le message RIP est une demande ou une réponse. Les routeurs envoient une ou plusieurs réponses RIP toutes les 30 secondes. Ces messages contiennent les vecteurs de distance qui résument la table de routage du routeur. Les messages de demande RIP peuvent être utilisés par les routeurs ou les hôtes pour interroger d'autres routeurs sur le contenu de leur table de routage. Une utilisation typique est lorsqu'un routeur démarre et souhaite rapidement recevoir les réponses RIP de ses voisins pour calculer sa propre table de routage. La version actuelle de RIP est la version 2 définie dans la RFC 2453 pour IPv4 et la RFC 2080 pour IPv6.</p>
                    <figure>
                        <img src="../images/format_message_RIP.png" alt="">
                        <figcaption>Figure 5.47 : Format de message RIP</figcaption>
                    </figure>
                    <p>L'en-tête de RIP contient un champ d'authentification. Cette authentification peut être utilisée par les administrateurs réseau pour s'assurer que seuls les messages RIP envoyés par les routeurs qu'ils gèrent sont utilisés pour construire les tables de routage. RFC 2453 ne prend en charge qu'un schéma d'authentification de base où tous les routeurs sont configurés avec le même mot de passe et incluent ce mot de passe dans tous les messages RIP. Ce n'est pas très sécurisé car un attaquant peut connaître le mot de passe en capturant un seul message RIP. Cependant, ce mot de passe peut protéger contre les erreurs de configuration. Des schémas d'authentification plus forts sont décrits dans RFC 2082 et RFC 4822, mais les détails de ces mécanismes sont hors du champ d'application de cette section.</p>
                    <p>Chaque message RIP contient un ensemble d'entrées de route. Chaque entrée de route est codée sous forme de champ de 20 octets dont le format est présenté ci-dessous. RIP a été initialement conçu pour être adapté à différents protocoles de couche réseau. Certaines implémentations de RIP ont été utilisées dans des réseaux XNS ou IPX. Le premier champ de l'entrée de route RIP est l'<span class="html">identifiant de famille d'adresses (AFI pour Address Family Identifier)</span>. Cet identifiant indique le type d'adresse trouvée dans l'entrée de route. Les identifiants de famille d'adresses sont maintenus par l'IANA à <a href="http://www.iana.org/assignments/address-family-numbers/" target="_blank">http://www.iana.org/assignments/address-family-numbers/</a>. IPv4 utilise <span class="em">AFI=1</span>. Les autres champs importants de l'entrée de route sont le préfixe IPv4, le masque de sous-réseau qui indique la longueur de l'identificateur de sous-réseau et est codé sous forme de masque de 32 bits et la métrique. Bien que la métrique soit codée sous forme de champ de 32 bits, la métrique RIP maximale est <span class="em">15</span> (pour RIP, <span class="em">16 = &#8734;</span>).</p>
                    <figure>
                        <img src="../images/format_entrees_route_IPv4_RIP.png" alt="">
                        <figcaption>Figure 5.48 : Format des entrées de route IPv4 RIP (RFC 2453)</figcaption>
                    </figure>
                    <p>Avec une entrée de route de 20 octets, il était difficile d'utiliser le même format que ci-dessus pour prendre en charge IPv6. Au lieu de définir un format d'entrée de route de longueur variable, les concepteurs de la RFC 2080 ont défini un nouveau format qui n'inclut pas de champ <span class="em">AFI</span>. Le format des entrées de route utilisées par la RFC 2080 est indiqué ci-dessous. <span class="em">Plen</span> est la longueur de l'identificateur de sous-réseau en bits et la métrique est codée sur 4 octets.</p>
                    <figure>
                        <img src="../images/format_entrees_route_IPv6_RIP.png" alt="">
                        <figcaption>Figure 5.49 : Format des entrées de route RIP IPv6</figcaption>
                    </figure>
                    <hr>
                    <p>Note : Une note sur les timers :</p>
                    <p>Les premières implémentations de RIP envoyaient leur vecteur de distance exactement toutes les 30 secondes. Cela fonctionnait bien dans la plupart des réseaux, mais certains chercheurs ont remarqué que les routeurs étaient parfois surchargés car ils traitent trop de vecteurs de distance en même temps [FJ1994]. Ils ont collecté des traces de paquets dans ces réseaux et ont constaté qu'après un certain temps, les timers des routeurs étaient synchronisés, c'est-à-dire que presque tous les routeurs envoyaient leurs vecteurs de distance presque en même temps. Cette synchronisation des temps de transmission des vecteurs de distance a provoqué une surcharge sur le processeur des routeurs, mais a également augmenté le temps de convergence du protocole dans certains cas. Cela était principalement dû au fait que tous les routeurs réglaient leurs timers sur la même durée d'expiration après avoir traité les vecteurs de distance reçus. Sally Floyd et Van Jacobson ont proposé dans [FJ1994] une solution simple pour résoudre ce problème de synchronisation Au lieu d'annoncer leur vecteur de distance exactement après 30 secondes, un routeur devrait envoyer son prochain vecteur de distance après un délai choisi au hasard dans l'intervalle <span class="em">[15,45]</span> RFC 2080. Cette randomisation des délais empêche la synchronisation qui se produit avec un délai fixe et est maintenant une pratique recommandée pour les concepteurs de protocoles.</p>
                    <hr>
                    <h5>OSPF :</h5>
                    <p>Les protocoles de routage à état de lien sont utilisés dans les réseaux IP. <span class="html">Open Shortest Path First (OSPF)</span>, défini dans le RFC 2328, est le protocole de routage à état de lien standardisé par l'IETF. La dernière version d'OSPF, qui prend en charge IPv6, est définie dans le RFC 5340. OSPF est fréquemment utilisé dans les réseaux d'entreprise et dans certains réseaux ISP. Cependnat, les réseaux ISP utilisent souvent le protocole de routage à état de lien IS-IS [ISO10589], qui a été développé pour le protocole ISO CLNP mais a été adapté pour être utilisé dans les réseaux IP RFC 1195 avant la finalisation de la standardisation d'OSPF. Une analyse détaillée d'ISIS et d'OSPF peut être trouvée dans [BMO2006] et [Perlman2000]. Des informations supplémentaires sur OSPF peuvent être trouvées dans [Moy1998].</p>
                    <p>Comparé aux bases des protocoles de routage à état de liens que nous avons discutées dans la section "<span class="em">Routage à état de liens</span>", il y a quelques particularités d'OSPF qui méritent d'être discutées. Tout d'abord, dans un grand réseau, la diffusion de l'information sur tous les routeurs et les liens à des milliers de routeurs ou plus peut être coûteuse car chaque routeur doit stocker toutes les informations sur l'ensemble du réseau. Une meilleure approche consisterait à introduire un routage hiérarchique. Le routage hiérarchique divise le réseau en régions. Tous les routeurs à l'intérieur d'une région ont des informations détaillées sur la topologie de la région, mais ne connaissent que des informations agrégées sur la topologie des autres régions et leurs interconnexions. OSPF prend en charge une variante restreinte de routage hiérarchique. Dans la terminologie d'OSPF, une région est appelée une <span class="html">zone</span>.</p>
                    <p>OSPF impose des restrictions sur la façon dont un réseau peut être divisé en zones. Une zone est un ensemble de routeurs et de liens regroupés. Généralement, la topologie d'une zone est choisie de sorte qu'un paquet envoyé par un routeur à l'intérieur de la zone puisse atteindre n'importe quel autre routeur de la zone sans quitter la zone. OSPF peut prendre en charge des liens virtuels pour connecter des routeurs appartenant à la même zone mais qui ne sont pas directement connectés. Cependant, cela dépasse cette introduction à OSPF. Une zone OSPF contient deux types de routeurs RFC 2238 :</p>
                    <ul>
                        <li>
                            <p>Routeur interne : Un routeur dont les réseaux connectés directement appartiennent directement à la zone.</p>
                        </li>
                        <li>
                            <p>Routeurs frontière de zone : Un routeur qui est attaché à plusieurs zones.</p>
                        </li>
                    </ul>
                    <p>La figure ci-dessous montre un réseau divisé en trois zones : la <span class="em">zone 1</span> contenant les routeurs <span class="em">R1</span>, <span class="em">R3</span>, <span class="em">R4</span>, <span class="em">R5</span> et <span class="em">RA</span>, la <span class="em">zone 2</span> contenant les routeurs <span class="em">R7</span>, <span class="em">R8</span>, <span class="em">R9</span>, <span class="em">R10</span>, <span class="em">RB</span> et <span class="em">RC</span>. Les zones OSPF sont identifiées par un entier de 32 bits, qui est parfois représenté comme une adresse IP. Parmi les zones OSPF, la <span class="em">zone 0</span>, également appelée <span class="em">zone de base</span>, a un rôle spécial. La zone de base regroupe tous les routeurs frontière de zone (routeurs <span class="em">RA</span>, <span class="em">RB</span> et <span class="em">RC</span> dans la figure ci-dessous) et les routeurs qui sont directement connectés aux routeurs dorsaux mais ne font pas partie d'une autre zone (routeur <span><em>RD</em> dans la figure ci-dessous). Une restriction importante imposée par OSPF est que le chemin entre deux routeurs appartenant à deux zones différentes (par exemple <span class="em">R1</span> et <span class="em">R8</span> dans la figure ci-dessous) doit passer par la zone de base.</p>
                    <figure>
                        <img src="../images/zones_OSPF.jpg" alt="">
                        <figcaption>Figure 5.50 : Zones OSPF</figcaption>
                    </figure>
                    <p>À l'intérieur de chaque zone qui n'est pas la zone de base, les routeurs distribuent la topologie de la zone en échangeant des paquets d'état de lien avec les autres routeurs de la zone. Les routeurs internes ne connaissent pas la topologie des autres zones, mais chaque routeur sait comment atteindre la zone de base. À l'intérieur d'une zone, les routeurs n'échangent des paquets d'état de lien que pour toutes les destinations atteignables à l'intérieur de la zone. Dans OSPF le routage inter-domaine est effectuée en échangeant des vecteurs de distance. Cela est illustré par la topologie du réseau ci-dessous.</p>
                    <figure>
                        <img src="../images/routage_hierarchique_avec_OSPF.jpg" alt="">
                        <figcaption>Figure 5.51 : Routage hiérarchique avec OSPF</figcaption>
                    </figure>
                    <p>Examinons d'abord le routage OSPF à l'intérieur de la <span class="em">zone 2</span>. Tous les routeurs de la zone apprennent une route vers <span class="em">192.168.1.0/24</span> et <span class="em">192.168/10.0/24</span>. Les deux routeurs frontière de zone, <span class="em">RB</span> et <span class="em">RC</span>, créent des annonces de résumé de réseau. En supposant que toutes les liaisons ont une métrique de lien unitaire, celles-ci seraient :</p>
                    <ul>
                        <li>
                            <p><span class="em">RB</span> annonce <span class="em">192.168.1.0/24</span> à une distance de <span class="em">2</span> et <span class="em">192.168.10.0/24</span> à une distance de <span class="em">3</span>.</p>
                        </li>
                        <li>
                            <p><span class="em">RC</span> annonce <span class="em">192.168.1.0/24</span> à une distance de <span class="em">3</span> et <span class="em">192.168.10.0/24</span> à une distance de <span class="em">2</span>.</p>
                        </li>
                    </ul>
                    <p>Ces annonces de résumé sont diffusées à travers la zone de base connectée aux routeurs <span class="em">RA</span>, <span class="em">RB</span> et <span class="em">RC</span>. Dans sa table de routage, le routeur <span class="em">RA</span> sélectionne le résumé annoncé par <span class="em">RB</span> pour atteindre <span class="em">192.168.1.0/24</span> et le résumé annoncé par <span class="em">RC</span> pour atteindre <span class="em">192.168.10.0/24</span>. À l'intérieur de la <span class="em">zone 1</span>, le routeur <span class="em">RA</span> annonce un résumé indiquant que <span class="em">192.168.1.0/24</span> et <span class="em">192.168.19.0/24</span> sont tous deux à distance de <span class="em">3</span> de lui-même.</p>
                    <p>D'autre part, considérons les préfixes <span class="em">10.0.0.0/24</span> et <span class="em">10.0.1.0/24</span> qui sont dans la <span class="em">zone 1</span>. Le routeur <span class="em">RA</span> est le seul routeur frontière de zone qui est attaché à cette zone. Ce routeur peut créer deux annonces de résumé réseau différentes :</p>
                    <ul>
                        <li>
                            <p><span class="em">10.0.0.0/24</span> à une distance de <span class="em">1</span> et <span class="em">10.0.1.0/24</span> à une distance de <span class="em">2</span> de <span class="em">RA</span>.</p>
                        </li>
                        <li>
                            <p><span class="em">10.0.0.0/23</span> à une distance de <span class="em">2</span> de <span class="em">RA</span>.</p>
                        </li>
                    </ul>
                    <p>Le premier résumé fournit des informations précises sur la distance utilisée pour atteindre chaque préfixe. Cependant, tous les routeurs du réseau doivent maintenir une route vers <span class="em">10.0.0.0/24</span> et une route vers <span class="em">10.0.1.0/24</span> qui passent toutes deux par le routeur <span class="em">RA</span>. La deuxième annonce amélirorerait la scalabilité d'OSPF en réduisant le nombre de routes qui sont annoncées à travers les frontières de zone. Cependant, en pratique cela nécessite une configuration manuelle sur les routeurs de bordure.</p>
                    <p>La deuxième particularité d'OSPF qui vaut la peine d'être discutée est le supoort des Réseaux Locaux (LAN). Comme illustré dans l'exemple, ci-dessous, plusieurs routeurs peuvent être connectés au même LAN.</p>
                    <figure>
                        <img src="../images/LAN_OSPF_contenant_routeurs.png" alt="">
                        <figcaption>Figure 5.52 : Un LAN OSPF contenant plusieurs routeurs</figcaption>
                    </figure>
                    <p>Une première solution pour supporter un réseau local (LAN) avec un protocole de routage à état de lien serait de considérer qu'un LAN est équivalent à un maillage complète de liaisons point-à-point comme si chaque routeur peut atteindre directement n'importe quel autre routeur sur le LAN. Cependant, cette approche présente deux inconvénients importants :</p>
                    <ol>
                        <li>
                            <p>Chaque routeur doit échanger des HELLOs et des LSP avec tous les autres routeurs sur le LAN. Cela augmente le nombre de paquets OSPF qui sont envoyés et traités par chaque routeur.</p>
                        </li>
                        <li>
                            <p>Les routeurs distants, lorsqu'ils regardent la topologie distribuée par OSPF, considèrent qu'il y a un maillage complète de liens dentre tous les routeurs du LAN. Un tel maillage complète implique beaucoup de redondance en cas de défaillance, alors qu'en pratique, l'ensemble du LAN, tous les routeurs doivent détecter les défaillances et diffuser des LSP avant que le LAN ne soit complètement supprimé de la topologie OSPF par les routeurs distants.</p>
                        </li>
                    </ol>
                    <p>Pour mieux représenter les LAN et réduire le nombre de paquets OSPF échangés, OSPF traite les LAN différemment. Lorsque les routeurs OSPF démarrent sur un LAN, ils élisent l'un d'entre eux comme routeur désigné (<span class="em">DR</span>) (RFC 2328). La procédure d'élection du routeur désigné OSPF est définie dans la RFC 2328. Chaque routeur peut être configuré avec une priorité de routeur qui influence le processus d'élection, car le routeur avec la priorité la plus élevée est préféré lorsqu'une élection est exécutée. Le routeur <span class="em">DR</span> représente le réseau local et annonce le sous-réseau LAN (<span class="em">138.48.4.0/24</span> dans l'exemple ci-dessus). De plus, les routeurs LAN n'échangent des paquets HELLO qu'avec le <span class="em">DR</span>. Grâce à l'utilisation d'un <span class="em">DR</span>, la topologie du LAN apparaît comme un ensemble de liaisons point-à-point connectées au <span class="em">DR</span>, comme illustré dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/representation_OSPF_LAN.jpg" alt="">
                        <figcaption>Figure 5.53 : Représentation OSPF avec un LAN</figcaption>
                    </figure>
                    <hr>
                    <p>Note : Comment détecter rapidement une défaillance de liaison ?</p>
                    <p>Les opérateurs de réseau s'attendent à ce qu'un réseau OSPF soit capable de récupérer rapidement des pannes de liaison ou de routeur [VPD2004]. Dans un réseau OSPF, la récupération après une panne est effectuée en trois étapes [FFEB2005] :</p>
                    <ul>
                        <li>
                            <p>Les routeurs adjacents à la panne la détectent rapidement. La solution par défaut est de s'appuyer sur l'échange régulier de paquets HELLO. Cependant, l'intervalle entre les HELLO successifs est souvent fixé à 10 secondes... Le fait de régler la minuterie HELO à quelques millisecondes est difficile car les paquets HELLO sont créés et traités par le CPU principal des routeurs et ces routeurs ne peuvent pas facilement générer et traiter un paquet HELLO toutes les millisecondes sur chacune de leurs interfaces. Une meilleure solution est d'utiliser un protocole de détection de défaillance dédié tel que le protocole Bidirectinal Forwarding Detection (BFD) défini dans [KW2009] qui peut être mis en oeuvre directement sur les interfaces de routeur. Une autre solution pour détecter la panne consiste à instrumenter la couche physique et la couche de liaison de données de manière à ce qu'elles puissent interrompre le routeur lorsqu'une liaison échoue. Malheureusement, une telle solution ne peut pas être utilisée sur tous les types de couches physiques et de liaison de données.</p>
                        </li>
                        <li>
                            <p>Les routeurs qui ont détecté la panne diffusent leurs LSP mis à jour dans le réseau.</p>
                        </li>
                        <li>
                            <p>Tous les routeurs mettent à jour leur table de routage.</p>
                        </li>
                    </ul>
                    <hr>
                    <h5>Routage interdomaine :</h5>
                    <p>Comme expliqué précédemment, Internet est composé de plus de 30000 réseaux différents appelés <span class="html">domaines</span>. Une analyse de l'évolution du nombre de domaines sur l'Internet mondial au cours des dix dernières années peut être trouvée à l'adresse <a href="http://www.potaroo.net/tools/asn32/" target="_blank">http://www.potaroo.net/tools/asn32/</a>. Chaque domaine est composé d'un groupe de routeurs et dhôtes gérés par la même organisation. Des exemples de domaines incluent <span class="em">belnet</span>, <span class="em">sprint</span>, <span class="em">level3</span>, <span class="em">geant</span>, <span class="em">abilene</span>, <span class="em">cisco</span> ou <span class="em">google</span>...</p>
                    <p>Chaque domaine contient un ensmeble de routeurs. Du point de vie du routage, ces domaines peuvent être divisés en deux classes : les domaines de <span class="html">transit</span> et les domaines d'<span class="html">échappement (stub)</span>. Un domaine déchappement envoie et reçoit des paquets dont la source ou la destination est l'un de ses propres hôtes. Un domaine de transit est un domaine qui fournit un service de transit pour d'autres domaines, c'est-à-dire que les routeurs de ce domaine font avancer des paquets dont la source et la destination n'appartiennent pas au domaine de transit. Au moment de la rédaction de ce texte, environ 85% des domaines de l'Internet sont des domaines d'échappement. Plusieurs sites web collectent et analysent des données sur l'évolution de BGP dans l'Internet mondial. <a href="http://bgp.potaroo.net/" target="_blank">http://bgp.potaroo.net/</a> fournit de nombreuses statistiques et analyses mises à jour quotidiennement. Un domaine d'échappement connecté à un seul domaine de transit est appelé un domaine d'échappement à un seul lien. Un domaine d'échappement multi-attaché est un domaine d'échappement connecté à deux ou plusieurs fournisseurs de transit.</p>
                    <figure>
                        <img src="../images/domaines_transit_stub.jpg" alt="">
                        <figcaption>Figure 5.54 : Domaines de transit et d'échappement</figcaption>
                    </figure>
                    <p>Les domaines stub peuvent être classés en fonction de leur activité principale, c'est-à-dire s'ils envoient ou reçoivent principalement des paquets. Un domaine stub <span class="html">riche en accès</span> est un domaine qui contient des hôtes qui reçoivent principalement des paquets. Des exemples typiques comprennent les petits fournisseurs de services Internet basés sur l'ADSL et le câble ou les réseaux d'entreprise. En revanche, un domaine stub <span class="html">riche en contenu</span> est un domaine qui produit principalement des paquets. Des exemples de domaines stub riches en contenu comprenant <span class="em">Google</span>, <span class="em">Yahoo</span>, <span class="em">Microsoft</span>, <span class="em">Facebook</span> ou les réseaux de distribution de contenu tels que <span class="em">Akamai</span> ou <span class="em">Limelight</span>. Ces dernières années, nous assisté à une croissance rapide de ces domaines stub riches en contenu. Des mesures récentes [ATLAS2009] indiquent qu'une proportion croissante de tous les paquets échangés sur Internet sont produits dans les centres de données gérés par ces fournisseurs de contenu.</p>
                    <p>Les domaines doivent être interconnectés pour permettre à un hôte situé dans un domaine d'échanger des paquets IP avec des hôtes situés dans d'autres domaines. Du point de vue physiuqe, les domaines peuvent être interconnectés de deux manières différentes. La première solution consiste à connecter directement un routeur appartenant au premier domaine à un routeur à l'intérieur du deuxième domaine. De tels liens entre les domaines sont appelés liens d'interconnexion privés ou <span class="html">liens de peering privés</span>. En pratique, pour des raisons de redondance ou de performance, des liens physiques distincts sont généralement établis entre différents routeurs dans les deux domaines interconnectés.</p>
                    <figure>
                        <img src="../images/interconnexion_domaines_liaison_peering_privee.png" alt="">
                        <figcaption>Figure 5.55 : Interconnexion de deux domaines via un lien de peering privé</figcaption>
                    </figure>
                    <p>Les liens privés de peering sont utiles lorsque, par exemple, un réseau d'entreprise ou universitaire doit être connecté à son fournisseur de service Internet. Cependant, certains domaines sont connectés à des centaines d'autres domaines. Pour ce domaines, utiliser uniquement des liens privés de peering serait trop coûteux. Une meilleure solution pour permettre à de nombreux domaines de s'interconnecter à moindre coût sont les <span class="html">points d'échange Internet (IXP pour Internet eXchange Points)</span>. Un <span class="em">IXP</span> est généralement un espace dans un centre de données qui héberge des routeurs appartenant à différents domaines. UN domaine souhaitant échanger des paquets avec d'autres domaines présents sur l'<span class="em">IXP</span> installe l'un de ses routeurs sur l'IXP et le connecte à d'autres routeurs à l'intérieur de son propre réseau. L'<span class="em">IXP</span> contient un réseau local auquel tous les routeurs participants sont connectés. Lorsque deux domaines présents sur l'<span class="em">IXP</span> souhaitent échanger des paquets, ils utilisent simplement le réseau local. Les <span class="em">IXP</span> sont très populaires en Europe et de nombreux fournisseurs de services Internet et fournisseurs de contenu y sont présents. Deux routeurs attachés au même <span class="em">IXP</span> n'échangent des paquets que lorsque les propriétaires de leurs domaines ont une incitation économique à échanger des paquets sur cet <span class="em">IXP</span>. Habituellement, un routeur sur un <span class="em">IXP</span> n'est capable d'échanger des paquets qu'avec une petite fraction des routeurs qui sont présents sur le même <span class="em">IXP</span>.</p>
                    <figure>
                        <img src="../images/interconnexion_domaines_IXP.jpg" alt="">
                        <figcaption>Figure 5.56 : Interconnexion de deux domaines à un point d'échange Internet</figcaption>
                    </figure>
                    <p>Au début de l'Internet, les domaines échangeaient simplement toutes les routes qu'ils connaissaient pour permettre à un hôte à l'intérieur d'un domaine d'atteindre n'importe quel hôte dans l'Internet global. Cependant, dans l'Internet commercial d'aujourd'hui, cela n'est plus vrai car le routage interdomaine doit principalement prendre en compte les relations économiques entre les domaines. De plus, alors que le routage intradomaine préfère généralement certaines routes par rapport à d'autres en fonction de leurs mérites techniques (par exemple, préférer une route avec le nombre minimal de sauts, préférer une route avec le délai minimal, préférer les routes à bande passante élevée plutôt que faible, etc.), le routage interdomaine traite principalement des problèmes économiques. Pour le routage interdomaine, le coût d'utilisation d'une route est souvent plus important que la qualité de la route mesurée par sa latence ou sa bande passante.</p>
                    <p>Il existe différents types de relations économiques qui peuvent exister entre les domaines. Le routage interdomaine convertit ces relations en relations de peering entre les domaines qui sont connectés via des liens de peering.</p>
                    <p>La première catégorie de relation de peering est la relation <span class="em">client&#8594;fournisseur</span>. Une telle relation est utilisée lorsqu'un domaine client paie un fournisseur de services Internet pour pouvoir échanger des paquets avec l'Internet mondial via une liaison interdomaine. Une relation similaire est utilisée lorsqu'un petit fournissseur de services Internet paie un plus grand fournisseur de services Internet pour échanger des paquets avec l'Internet mondial.</p>
                    <figure>
                        <img src="../images/internet_simple_relations_peering.jpg" alt="">
                        <figcaption>Figure 5.57 : Un internet simple avec des relations de peering</figcaption>
                    </figure>
                    <p>Pour comprendre la relation <span class="em">client&#8594;fournisseur</span>, considérons le réseau illustré dans la figure ci-dessus. Dans ce réseau, <span class="em">AS7</span> est un domaine de support connecté à un seul fournisseur, <span class="em">AS4</span>. Le contrat entre <span class="em">AS4</span> et <span class="em">AS7</span> permet à un hôte situé à l'intérieur d'<span class="em">AS7</span> d'échanger des paquets avec n'importe quel hôte dans le réseau. Pour permettre ccet échange de paquets, <span class="Em">AS7</span> doit connaître une route vers n'importe quel domaine et tous les domaines du réseau doivent connaître une route via <span class="em">AS4</span> qui leur permet de rejoindre les hôtes à l'intérieur d'<span class="em">AS7</span>. D'un point de vue de routagen le contrat commercial entre <span class="em">AS7</span> et <span class="em">AS4</span> conduit aux règles suivantes :</p>
                    <ul>
                        <li>
                            <p>via une relation <span class="em">client&#8594;fournisseur</span>, le domaine <span class="em">client</span> annonce à son fournisseur toutes ses routes et toutes les routes qu'il a apprises de ses propres clients.</p>
                        </li>
                        <li>
                            <p>via une relation <span class="em">fournisseur&#8594;client</span>, le <span class="em">fournisseur</span> annonce toutes les routes qu'il connaît à son client.</p>
                        </li>
                    </ul>
                    <p>La deuxième règle garantit que le domaine client reçoit une route vers toutes les destinations atteignables via son fournisseur. La première règle permet aux routes du domaine client d'être distribuées dans l'ensemble d'Internet.</p>
                    <p>Revenant à la figure ci-dessus, <span class="em">AS4</span> annonce à ses deux fournisseurs <span class="em">AS1</span> et <span class="em">AS2</span> ses propres routes ainsi que les routes apprises de son client, <span class="em">AS7</span>. D'autre part, <span class="em">AS4</span> annonce à <span class="em">AS7</span> qu'il connaît.</p>
                    <p>Le deuxième type de relation d'interconnexion est la relation d'interconnexion à <span class="html">coût partagé</span>. Une telle relation n'implique généralement pas de paiement d'un domaine à l'autre, contrairement à la relation <span class="em">client&#8594;fournisseur</span>. Une relation d'interconnexion à coût partagé est généralement établie entre des domaines de taille et de couverture géographique similaires. Par exemple, considérons la figure ci-dessus. Si <span class="em">AS3</span> et <span class="em">AS4</span> échangent de nombreux paquets via <span class="em">AS1</span>, ils doivent tous deux payer <span class="em">AS1</span>. Une alternative moins coûteuse pour <span class="em">AS3</span> et <span class="em">AS4</span> serait d'établir une interconnexion à coût partagé. Une telle interconnexion peut être établie au niveau des IXP où <span class="em">AS3</span> et <span class="em">AS4</span> sont présents ou en utilisant des liens d'interconnexion privés. Cette interconnexion à côut ârtagé devrait être utilisée pour échanger des paquets entre les hôtes à l'intérieur d'<span class="em">AS3</span> et les hôtes d'<span class="em">AS4</span>. Cependnat, <span class="em">AS3</span> ne veut pas recevoir sur les liens d'interconnexion à coût partagé <span class="em">AS3-AS4</span> des paquets dont la destination appartient à <span class="em">AS1</span>, car <span class="em">AS3</span> devrait payer pour envoyer ces paquets à <span class="em">AS1</span>.</p>
                    <p>D'un point de vue routage, dans une relation de peering à coûts partagés, un domaine n'annonce que ses routes internes et les routes qu'il a apprises de ses clients. Cette restriction garantit que seuls les paquets destinés au domaine local ou à l'un de ses clients sont reçus sur la relation de peering à coûts partagés. Cela implique que les routes apprises d'un fournisseur ou d'un autre pair à coûts partagés ne sont pas annoncées sur une relation de peering à coûts partagés. Cette restriction est motivée par des raisons économiques. Si un domaine annonçait les routes qu'il a appris d'un fournisseur sur une relation de peering à coûts partagés qui ne rapporte pas de revenus, il permettrait à son pair à coûts partagés d'utiliser le lien avec son fournisseur sans aucun paiement. Si un domaine annonçait les routes apprises sur une relation de perring à coûts partagés sur une autre relation de peering à coûts partagés, il permettrait à ces pairs à coûts partagés d'utiliser son propre réseau (qui peut s'étendre sur un ou plusieurs continents librement pour échanger des paquets.</p>
                    <p>Enfin, le dernier type de relation de pering est le <span class="html">sibling</span>. Une telle relation est utilisée lorsque deux domaines échangent toutes leurs routes dans les deux sens. En pratique, une telle relation n'est utilisée qu'entre des domaines appartenant à la même entreprise.</p>
                    <p>Ces différents types de relations sont implémentés dans les <span class="html">politiques de routage interdomaine</span> définies par chaque domaine. La politique de routage interdomaine d'un domaine est composée de trois parties principales :</p>
                    <ul>
                        <li>
                            <p>Le <span class="html">filtre d'importation</span> qui spécifie, pour chaque relation de peering, les routes qui peuvent être acceptées du domaine voisin (les routes non acceptables sont ignorées et le domaine ne les utilise jamais pour faire avancer les paquets).</p>
                        </li>
                        <li>
                            <p>Le <span class="html">filtre d'exportation</span> qui spécifie, pour chaque relation de peering, les routes qui peuvent être annoncées au domaine voisin.</p>
                        </li>
                        <li>
                            <p>L'algorithme de <span class="html">classement</span> qui est utilisé pour sélectionner la meilelure route parmi toutes les routes que le domaine a reçues vers le même préfixe de destination.</p>
                        </li>
                    </ul>
                    <p>Les filtres d'importation et d'exportation d'un domaine peuvent être définis en utilisant le langage de spécification de politique de routage (RPSL pour Route Policy Specification Language) soécifié dans la RFC 2622 [GAVE1999]. Certains fournisseurs de services Internet, notamment en Europe, utilisent RPSL pour documenter leurs politiques d'importation et d'exportation. Consultez <a href="ftp://ftp.ripe.net/ripe/dbase" target="_blank">ftp://ftp.ripe.net/ripe/dbase</a> pour la base de données RIPE qui contient les politiques d'importation et d'exportation de nombreux fournisseurs de services Internet européens. Plusieurs outils aident à convertir facilement une politique RPSL en commandes de routeur.</p>
                    <p>La figure ci-dessous fournit un exemple simple de filtres d'importation et d'exportation pour deux domaines dans un inter-réseau simple. En RPSL, le mot-clé <span class="html">ANY</span> est utilisé pour remplacer n'importe quelle route de n'importe quel domaine. Il est généralement utilisé par un fournisseur pour indiquer qu'il annonce toutes ses routes à un client via une relation <span class="em">fournisseur&#8594;client</span>. C'est le cas de la politique d'exportation d'<span class="em">AS4</span>. L'exemple ci-dessous montre clairement la différence entre une relation <span class="em">fournisseur&#8594;client</span> et une relation de coûts partagés. Le filtre d'exportation d'<span class="em">AS4</span> indique qu'il annonce uniquement ses routes internes (<span class="em">AS4</span>) et les routes apprises auprès de ses clients (<span class="em">AS7</span>) sont son partage de coût avec <span class="em">AS3</span>, tandis qu'il annonce toutes les routes qu'il utilise (y compris les routes apprises auprès d'<span class="em">AS3</span>) à <span class="em">AS7</span>.</p>
                    <figure>
                        <img src="../images/politiques_importation_exportation.jpg" alt="">
                        <figcaption>Figure 5.58 : Politiques d'importation et d'exportation</figcaption>
                    </figure>
                    <h5>Le protocole de passerelle frontière (BGP) :</h5>
                    <p>L'Internet utilise un seul protocole de protocole interdomaine : le <span class="html">Border Gateway Protocol (BGP)</span>. La version actuelle de BGP est définie dans la RFC 4271. BGP diffère des protocoles de routage intradomaine que nous avons déjà discutés de plusieurs façons. Tout d'abord, BGP est un protocole de vecteur de chemin. Lorsqu'un routeur BGP annonce une route vers un préfixe, il annonce le préfixe IP et le chemin interdomaine utilisé pour atteindre ce préfixe. Du point de vue de BGP, chaque domaine est identifié par un numéri de <span class="html">système autonome (AS pour Autonomous System)</span> unique et le chemin interdomaine contient les numéros AS des domaines de transit qui sont utilisés pour atteindre le préfixe associé. Dans ce texte, nous considérons les termes "système autonome" et "domaine" comme synonymes. En pratique, un domaine peut être divisé en plusieurs systèmes autonomes, mais nous ignorons ce détail. Ce chemin interdomaine est appelé le <span class="html">chemin AS</span>. Grâce à ces chemins AS, BGP ne souffre pas des problèmes de comptage à l'infini qui affectent les protocoles de routage de vecteur de distance. De plus, le chemin AS peut être utilisé pour implémenter certaines politiques de routage. Une autre différence entre BGP et les protocoles de routage intradomaine est qu'un routeur BGP n'envoie pas régulièrement à ses voisins l'intégralité du contenu de sa table de routage. Étant donné la taille de l'Internet global, les routeurs seraient surchargés par le nombre de messages BGP qu'ils devraient traiter. BGP utilises des mises à jour incrémentales, c'est-à-dire qu'il ne signale à ses voisins que les routes qui ont changé.</p>
                    <p>La figure ci-dessous montre un exemple simple des routes BGP échangées entre les domaines. Dans cet exemple, le préfixe <span class="em">1.0.0.0/8</span> est annoncé par <span class="em">AS1</span>. <span class="em">AS1</span> annonce une route BGP vers ce préfixe à <span class="em">AS2</span>. Le chelin AS de cette route indique qu'<span class="em">AS1</span> est l'originateur du préfixe. Lorsque <span class="em">AS4</span> reçoit la route BGP d'<span class="em">AS1</span>, il la réannonce à <span class="em">AS2</span> et ajoute son numéro d'AS au chemin AS. <span class="em">AS2</span> a appris deux routes vers le préfixe <span class="em">1.0.0.0/8</span>. Il compare les deux routes et préfère la route apprise d'<span class="em">AS4</span> en fonction de son propre algorithme de classement. <span class="em">AS2</span> annonce à <span class="em">AS5</span> une route vers <span class="em">1.0.0.0/8</span> avec son chemin AS défini à <span class="em">AS2:AS4:AS1</span>. Grâce au chemin AS, <span class="em">AS5</span> sait que si elle envoie un paquet vers <span class="em">1.0.0.0/8</span>, le paquet passe d'abord par <span class="em">AS2</span>, puis par <span class="em">AS4</span> avant d'atteindre sa destination à l'intérieur d'<span class="em">AS1</span>.</p>
                    <figure>
                        <img src="../images/echange_simple_routes_BGP.jpg" alt="">
                        <figcaption>Figure 5.59 : Échange simple de routes BGP</figcaption>
                    </figure>
                    <p>Les routeurs BGP échangent des routes via des sessions BGP. Une session BGP est établie entre deux routeurs appartenant à deux différents qui sont directement connectés. Comme expliqué précédemment, la connexion physique entre les deux routeurs peut être mise en place comme une liaison privée ou via un IPX. Une session BGP entre deux routeurs adjacents fonctionne au-dessus d'une connexion TCP (le port BGP par défaut est <span class="em">179</span>). Contrairement aux protocoles de routage intradomaine qui échangent des paquets IP ou des segments UDP, BGP fonctionne au-dessus de TCP car TCP assure une livraison fiable des messages BGP envoués par chaque routeur sans forcer les routeurs à implémenter des accusés de réception, des sommes de contrôle, etc. De plus, les deux routeurs considèrent que la liaison de peering est active tant que la session BGP et la connexion TCP sous-jacente restent actives. Les deux extrémités d'une session BGP sont appelées <span class="html">pairs BGP</span>. Les sessions BGP et la connexion TCP sous-jacente sont généralement établies par les routeurs lorsqu'ils démarrent en fonction des informations trouvées dans leur configuration Les sessions BGP sont rarement libérées, sauf si la liaison de perring correspondante échoue ou si l'une des extrémités tombe en panne ou doit être redémarrée.</p>
                    <figure>
                        <img src="../images/session_peering_BGP_routeurs_directement_connectes.png" alt="">
                        <figcaption>Figure 5.60 : Une session de peering BGP entre deux routeurs directement connectés</figcaption>
                    </figure>
                    <p>En pratique, pour établir une session BGP entre les routeurs <span class="em">R1</span> et <span class="em">R2</span> dans la figure ci-dessus, l'administrateur réseau de <span class="em">AS3</span> doit d'abord configurer sur <span class="em">R1</span> l'adresse IP de <span class="em">R2</span> sur le lien <span class="em">R1-R2</span> ainsi que le numéro AS de <span class="em">R2</span>. Le routeur <span class="em">R1</span> essaie ensuite régulièrement d'atablir la session BGP avec <span class="em">R2</span>. <span class="em">R2</span> n'accepte d'établir la session BGP avec <span class="em">R1</span> lorsqu'il a été configuré avec l'adresse IP de <span class="em">R1</span> et son numéro AS. Pour des raisons de sécurité, un routeur n'établit jamais de session BGP qui n'a pas été configurée manuellement sur le routeur.</p>
                    <p>Le protocole BGP RFC 4271 définit plusieurs types de messages qui peuvent être échangés au cours d'une session BGP :</p>
                    <ul>
                        <li>
                            <p><span class="html">OPEN</span> : ce message est envoyé dès que la connexion TCP entre les deux routeurs a été établie. Il initialise la session BGP et permet la négociation de certaines options. Les détails de ce message peuvent être trouvés dans la RFC 4271.</p>
                        </li>
                        <li>
                            <p><span class="html">NOTIFICATION</span> : ce message est utilisé pour terminer une session BGP, généralement parce qu'une erreur a été détectée par le pair BGP. Un routeur qui envoie ou reçoit un message <span class="em">NOTIFICATION</span> arrête immédiatement la session BGP correspondante.</p>
                        </li>
                        <li>
                            <p><span class="html">UPDATE</span> : ce message est utilisé pour annoncer de nouvelles routes ou des routes modifiées ou pour retirer des routes précédemment annoncées.</p>
                        </li>
                        <li>
                            <p><span class="html">KEEPALIVE</span> : ce message est utilisé pour assurer un échange régulier de messages sur la session BGP, même en l'absence de changements de routes. Lorsqu'un routeur BGP n'a pas envoyé de message <span class="em">UPDATE</span> au cours des dernières 30 secondes, il envoie un message <span class="em">KEEPALIVE</span> pour confirmer à l'autre pair qu'il est toujours actif. Si un pair ne reçoit aucun message BGP pendant une période de 90 secondes, la session BGP est considérée comme étant interrompue et toutes les routes apprises via cette session sont retirées. 90 secondes est le délai par défaut recommandé par la RFC 4271. Cependant, deux pairs BGP peuvent négocier un timer différent lors de l'établissement de leur session BGP. L'utilisation d'un intervalle trop court pour détecter les pannes de session BGP n'est pas recommandée. BFD [KW2009] peut être utilisé pour remplacer le mécanisme <span class="em">KEEPALIVE</span> de BGP si une détection rapide des pannes de lien interdomaine est nécessaire.</p>
                        </li>
                    </ul>
                    <p>Comme expliqué précédemment, BGP repose sur des mises à jour incrémentielles. Cela signifie que lorsqu'une session BGP démarre, chaque routeur envoie d'abord des messages BGP <span class="em">UPDATE</span> pour annoncer à l'autre pair toutes les routes exportables qu'il connaît. Une fois que toutes ces routes ont été annoncées, le routeur BGP n'envoie des messages BGP <span class="em">UPDATE</span> pour un préfixe que si la route est nouvelle, si l'un de ses attributs a changé ou si la route est devenue inaccessible et doit être retirée. Le message BGP <span class="em">UPDATE</span> permet aux routeurs BGP d'échanger efficacement ces informations tout en minimisant le nombre d'octets échangés. Chaque message BGP <span class="em">UPDATE</span> contient :</p>
                    <ul>
                        <li>
                            <p>une liste de préfixes IP sont retirés.</p>
                        </li>
                        <li>
                            <p>une liste de préfixes IP qui sont (ré-)annoncés.</p>
                        </li>
                        <li>
                            <p>l'ensemble des attributs (par exemple, le chemin AS) associés aux préfixes annoncés.</p>
                        </li>
                    </ul>
                    <p>Dans le reste de chapitre, et bien que toutes les informations de routage soient échangées à l'aide de message BGP <span class="em">UPDATE</span>, nous supposons pour simplifier qu'un message BGP contient uniquement des informations sur un préfixe et nous utilisons les termes suivants :</p>
                    <ul>
                        <li>
                            <p><span class="html">Message de retrait (Withdraw message)</span> pour indiquer un message BGP <span class="em">UPDATE</span> contenant une route retirée.</p>
                        </li>
                        <li>
                            <p><span class="html">Message de mise à jour (Update message)</span> pour indiquer un message BGP <span class="em">UPDATE</span> contenant une nouvelle route ou une route mise à jour vers un préfixe de destination avec ses attributs.</p>
                        </li>
                    </ul>
                    <p>D'un point de vue conceptuel, un routeur BGP connecté à <span class="em">N</span> pairs BGP peut être décrit comme étant composé de quatre parties, comme le montre la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/organisation_routeur_BGP.jpg" alt="">
                        <figcaption>Figure 5.61 : Organisation d'un routeur BGP</figcaption>
                    </figure>
                    <p>Dans cette figure, le routeur reçoit des messages BGP sur la partie gauche de la figure, traite ces messages et envoie éventuellement des messages BGP sur la partie droite de la figure. Un routeur contient trois structures de données importantes :</p>
                    <ul>
                        <li>
                            <p>L'<span class="em">Adj-RIB-In</span> contient les routes BGP qui ont été reçues de chaque pair BGP. Les routes de l'<span class="em">Adj-RIB-IN</span> sont filtrées par le <span class="em">filtre d'importation</span> avant d'être placées dans la <span class="em">BGP-Loc-RIB</span>. Il y a un <span class="em">filtre d'importation</span> par pair BGP.</p>
                        </li>
                        <li>
                            <p>La <span class="html">base d'information de routage locale (Loc-RIB pour Local Routing Information Base)</span> contient toutes les routes considérées comme acceptables par le routeur. Le <span class="em">Loc-RIB</span> peut contenir plusieurs routes, apprises à partir de différents pairs BGP, vers le même préfixe de destination.</p>
                        </li>
                        <li>
                            <p>La <span class="html">base d'informations de forwarding (FIB pour Forwarding Information Base)</span> est utilisée par le plan de données pour acheminer les paquets vers leur destination. La FIB contient, pour chaque destination, la meilleure route sélectionnée par le processus de décision BGP. Ce processus de décision est un algorithme qui sélectionne, pour chaque préfixe de destination, la meilleure route selon l'algorithme de classement de politiques du routeur.</p>
                        </li>
                        <li>
                            <p>L'<span class="html">Adj-RIB-Out</span> contient les routes BGP qui ont été annoncées à chaque pair BGP. L'<span class="em">Adj-RIB-Out</span> pour un pair donné est construit en appliquant le <span class="em">filtre d'exportation</span> du pair aux routes qui ont été installées dans la FIB. Il y a un <span class="em">filtre d'exportation</span> par pair BGP. Pour cette raison, l'<span class="em">Adj-RIB-Out</span> d'un pair peut contenir des routes différentes de l'<span class="em">Adj-RIB-Out</span> d'un autre pair.</p>
                        </li>
                    </ul>
                    <p>Lorsqu'une session BGP démarre, les routeurs échangent d'abord des messages <span class="em">OPEN</span> pour négocier les options qui s'appliquent tout au long de la session. Ensuite, chaque routeur extrait de sa FIB les routes à annoncer à son pair. Il est important de noter que, pour chaque préfixe de destination connu, un routeur BGP ne peut annoncer à un pair que la route qu'il a lui-même installée dans sa FIB. Les routes qui sont annoncées à un pair doivent passer le <span class="em">filtre d'exportation</span> du pair. Le <span class="em">filtre d'exportation</span> est un ensemble de règles qui définissent quelles routes peuvent être annoncées sur la session correspondante, éventuellement après avoir modifié certaines de ses attributs. Un <span class="em">filtre d'exportation</span> est associé à chaque session BGP. Par exemple, sur une interconnexion à coût partagé, le filtre d'exportation ne sélectionne que les routes internes et les routes apprises auprès d'un client. Le pseudo-code ci-dessous montre l'initialisation d'une session BGP.</p>
<pre><code>def initiliaze_BGP_session( RemoteAS, RemoteIP):
    # Initialize and start BGP session
    # Send BGP OPEN Message to RemoteIP on port 179
    # Follow BGP state machine
    # advertise local routes and routes learned from peers*/
    for d in BGPLocRIB :
        B=build_BGP_Update(d)
        S=Apply_Export_Filter(RemoteAS,B)
        if (S != None) :
            send_Update(S,RemoteAS,RemoteIP)
    # entire RIB has been sent
    # new Updates will be sent to reflect local or distant
    # changes in routers</code></pre>
                    <p>Dans le pseudo-code ci-dessus, la procédure <span class="em">build_BGP_UPDATE(d)</span> extrait de la <span class="em">BGP-Loc-RIB</span> la meilleure route vers la destination <span class="em">d</span> (c'est-à-dire la route installée dans le FIB) et prépare le message BGP <span class="em">UPDATE</span> correspondant.</p>
                    <p>Ce message est ensuite transmis au filtre d'<span class="em">filtre d'exportation</span> qui renvoie NULL si la route ne peut pas être annoncée au pair ou le message BGP <span class="em">UPDATE</span> (éventuellement modifié) à annoncer. Les routeurs BGP permettent aux administrateurs réseau de spécifier des <span class="em">filtres d'exportation</span> très complexes, voir par exemple [WMS2004]. Un <span class="em">filtre d'exportation</span> simple qui implémente l'équivalent de l'horizon divisé est présenté ci-dessous.</p>
<pre><code>def apply_export_filter(RemoteAS, BGPMsg) :
    # check if RemoteAS already received route
    if RemoteAS is BGPMsg.ASPath :
        BGPMsg=None
        # Many additional export policies can be configured :
        # Accept or refuse the BGPMsg
        # Modify selected attributes inside BGPMsg
    return BGPMsg</code></pre>
                    <p>À ce stade, le routeur distant a reçu toutes les routes BGP exportables. Après cet échange initial, le routeur envoie uniquement des messages BGP <span class="em">UPDATE</span> lorsqu'il y a un changement (ajout ou suppression d'une route ou changement des attributs d'une route) dans l'une de ces routes exportables. Un tel changement peut se produire lorsque le routeur reçoit un message BGP. Le pseudo-code ci-dessous résume le traitement de ces messages BGP.</p>
<pre><code>def Recvd_BGPMsg(Msg, RemoteAS) :
    B=apply_import_filer(Msg,RemoteAS)
    if (B== None): # Msg not acceptable
        return
    if IsUPDATE(Msg):
        Old_Route=BestRoute(Msg.prefix)
        Insert_in_RIB(Msg)
        Run_Decision_Process(RIB)
        if (BestRoute(Msg.prefix) != Old_Route) :
            # best route changed
            B=build_BGP_Message(Msg.prefix);
            S=apply_export_filter(RemoteAS,B);
            if (S!=None) : # announce best route
                send_UPDATE(S,RemoteAS,RemoteIP);
            else if (Old_Route != None) :
                send_WITHDRAW(Msg.prefix,RemoteAS, RemoteIP)
    else : # Msg is WITHDRAW
        Old_Route=BestRoute(Msg.prefix)
        Remove_from_RIB(Msg)
        Run_Decision_Process(RIB)
        if (Best_Route(Msg.prefix) !=Old_Route):
            # best route changed
            B=build_BGP_Message(Msg.prefix)
            S=apply_export_filter(RemoteAS,B)
            if (S != None) : # still one best route towards Msg.prefix
                send_UPDATE(S,RemoteAS, RemoteIP);
            else if(Old_Route != None) : # No best route anymore
                send_WITHDRAW(Msg.prefix,RemoteAS,RemoteIP);</code></pre>
                    <p>Lorsqu'un message BGP est reçu, le routeur applique d'abord le <span class="em">filtre d'importation</span> du pair pour vérifier si le message est acceptable ou non. Si le message n'est pas acceptable, le traitement s'arrête. Le pseudo-code ci-dessous montre un <span class="em">filtre d'importation</span> simple. Ce <span class="em">filtre d'importation</span> accepte toutes les routes, sauf celles qui contiennent déjà le système autonome local dans leur chemin d'AS. Si un telle route était utilisée, elle causerait une boucle de routage. Un autre exemple de <span class="em">filtre d'importation</span> serait un filtre utilisé par un fournisseur de services Internet sur une session avec un client pour n'accepter que les routes vers les préfixes IP attribués au client par le fournisseur. Sur de vrais routeurs, les <span class="em">filtres d'importation</span> peuvent être beaucoup plus complexes et certains <span class="em">filtres d'importation</span> modifient les attributs de la BGP <span class="em">UPDATE</span> reçue [WMS2004].</p>
<pre><code>def apply_import_filter(RemoteAS, BGPMsg):
    if MysAS in BGPMsg.ASPath :
        BGPMsg=None
        # Many additional import policies can be configured :
        # Accept or refuse the BGPMsg
        # Modify selected attributes inside BGPMsg
    return BGPMsg</code></pre>
                    <hr>
                    <p>Note : Les filtres bogon :</p>
                    <p>Un autre exemple de filtres d'importation couramment utilisés sont les filtres que les fournisseurs de services Internet utilisent pour ignorer les routeurs bogon. Dans la communauté des fournisseurs de services Internet, une route bogon est une route qui ne doit pas être annoncée sur Internet. Des exemples typiques incluent les préfixes IPv4 privés définis dans la RFC 1918, les préfixes de bouclage (<span class="em">127.0.0.1/8</span> et <span class="em">::1/128</span>) ou les préfixes IP qui n'ont pas encore été attribués par l'IANA. Un routeur BGP bien géré doit veiller à ne jamais annoncer de bogons sur Internet. Des informations détaillées sur ces bogons peuvent être trouvées sur <a href="http://www.team-cymru.org/Services/Bogons/" target="_blank">http://www.team-cymru.org/Services/Bogons/</a>.</p>
                    <hr>
                    <p>Si le filtre d'importation accepte le message BGP, le pseudo-code distingue deux cas. S'il s'agit d'un <span class="em">message de mise à jour</span> pour le préfixe <span class="em">p</span>, cela peut être une nouvelle route pour ce préfixe ou une modification des attributs de la route. Le routeur récupère d'abord de sa table RIP la meilleur route vers le préfixe <span class="em">p</span>. Ensuite, la nouvelle route est insérée dans la table RIB et le processus de décision BGP est exécuté pour déterminer si la meilleure route vers la destination <span class="em">p</span> a changé. Un message BGP ne doit être envoyer aux pairs du routeur que si la meilleure route a changé. Pour chaque pair, le routeur applique le <span class="em">filtre d'exportation</span> pour vérifier si la route peut être annoncée. Si c'est le cas, le message BGP filtré est envoyé. Sinon, un message de retrait est envoyé. Lorsque le routeur reçoit un message de retrait, il vérifie également si la suppression de la route de sa table RIB a causé un changement dans la meilleure route vers ce préfixe. Il convient de noter que, selon le contenu de la table RIB et les <span class="em">filtres d'exportation</span>, un routeur BGP peut avoir besoin d'envoyer un message de retrait à un pair après avoir reçu un <span class="em">message de mise à jour</span> d'un autre pair et inversement.</p>
                    <p>Expliquons maintenant en détail le fonctionnement de BGP dans un réseau IPv4. Pour cela, considérons le réseau simple composé de trois routeurs situés dans trois AS différents et représentés dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/utilisation_attribut_nexthop_BGP.png" alt="">
                        <figcaption>Figure 5.62 : Utilisation de l'attribut NextHop de BGP</figcaption>
                    </figure>
                    <p>Ce réseau contient trois routeurs : <span class="em">R1</span>, <span class="em">R2</span> et <span class="em">R3</span>. Chaque routeur est connecté à un sous-réseau IPv4 local qu'il annonce à l'aide de BGP. Il y a deux sessions BGP, une entre <span class="em">R1</span> et <span class="em">R2</span> et la deuxième entre <span class="em">R2</span> et <span class="em">R3</span>. Un sous-réseau <span class="em">/30</span> est utilisé sur chaque lien interdomaine (<span class="em">195.100.0.0/30</span> sur <span class="em">R1-R2</span> et <span class="em">195.100.0.4/30</span> sur <span class="em">R2-R3</span>). Les sessions BGP fonctionnent au-dessus de connexions établies entre les routeurs voisins (par exemple, <span class="em">195.100.0.1-195.100.0.2</span> pour la session <span class="em">R1-R2</span>).</p>
                    <p>Supposons que la session BGP <span class="em">R1-R2</span> est la première à être établie. Un message BGP <span class="em">BGP</span> envoyé sur une telle session contient trois champs :</p>
                    <ul>
                        <li>
                            <p>le préfixe annoncé</p>
                        </li>
                        <li>
                            <p>le NextHop de BGP</p>
                        </li>
                        <li>
                            <p>les attributs, y compris le chemin AS</p>
                        </li>
                    </ul>
                    <p>Nous utilisons la notation <span class="html">U(préfixe, nexthop, attributs)</span> pour représenter un tel message BGP <span class="em">Update</span>. De même, <span class="html">W(préfixe)</span> représente un retrait BGP pour le préfixe spécifié. Une fois que la session <span class="em">R1-R2</span> a été établie, <span class="em">R1</span> envoie <span class="em">U(194.100.0.0/24,195.100.0.1,AS10)</span> à <span class="em">R2</span> et <span class="em">R2</span> envoie <span class="em">U(194.100.2.0/23,195.100.0.2,AS20)</span>. À ce stade, <span class="em">R1</span> peut atteindre <span class="em">194.100.2.0/23</span> via <span class="em">195.100.0.2</span> et <span class="em">R2</span> peut atteindre <span class="em">194.100.0.0/24</span> via <span class="em">195.100.0.1</span>.</p>
                    <p>Une fois que la session <span class="em">R2-R3</span> a été établie, <span class="em">R3</span> envoie <span class="em">U(194.100.1.0/24,195.100.0.6,AS30)</span>. <span class="em">R2</span> annonce sur la session <span class="em">R2-R3</span> toutes les routes à l'intérieur de sa RIB. Il envoie donc à <span class="em">R3</span> : <span class="em">U(194.100.0.0/24,195.100.0.5,AS20:AS10)</span> et <span class="em">U(194.100.2.0/23;195.100.0.5,AS20)</span>. Notez que lorsque <span class="em">R2</span> annonce la route qu'il a apprise de <span class="em">R1</span>, il met à jour le NextHop de BGP et ajoute son numéro AS au chemin AS. <span class="em">R2</span> envoie également <span class="em">U(194.100.1.0/24,195.100.0.2,1S20:AS30)</span> à <span class="em">R1</span> sur la session <span class="em">R1-R3</span>. À ce stade, toutes les routes BGP ont été échangées et tous les routeurs peuvent atteindre <span class="em">194.100.0.0/24</span>, <span class="em">194.100.2.0/23</span> et <span class="em">194.100.1.0.24</span>.</p>
                    <p>Si le lien entre <span class="em">R2</span> et <span class="em">R3</span> échoue, <span class="em">R3</span> détecte la défaillance car elle n'a pas reçu de messages <span class="em">KEEPALIVE</span> de <span class="em">R2</span> récemment. À ce moment, <span class="em">R3</span> supprime de sa table de routage toutes les routes apprises via la session BGP <span class="em">R2-R3</span>. <span class="em">R2</span> supprime également de sa table de routage les routes apprises de <span class="em">R3</span>. <span class="em">R2</span> envoie également <span class="em">W(194.100.1.0/24)</span> à <span class="em">R1</span> via la session BGP <span class="em">R1-R3</span>, car il n'a plus de route vers ce préfixe.</p>
                    <hr>
                    <p>Note : Origine des routes annoncées par un routeur BGP :</p>
                    <p>Une question pratique fréquente concernant le fonctionnement de BGP est de savoir comment un routeur BGP décide de produire ou d'annoncer une route pour la première fois. En pratique, cela se produit dans deux situations :</p>
                    <ul>
                        <li>
                            <p>Le routeur a été configuré manuellement par l'opérateur réseau pour toujours annoncer une ou plusieurs routes sur une session BGP. Par exemple, sur la session BGP entre l'UCLouvain et son fournisseur <span class="em">Belnet</span>, le routeur de l'UCLouvain annonce toujours le préfixe IPv4 <span class="em">130.104.0.0/16</span> assigné au réseau du campus.</p>
                        </li>
                        <li>
                            <p>Le routeur a été configuré par l'opérateur réseau pour annoncer sur sa session BGP certaines des routes qu'il apprend avec son protocole de routage intradomaine. Par exemple, un routeur d'entreprise peut annoncer sur une session BGP avec son fournisseur les routes vers des sites distants lorsque ces routes sont accessibles et annoncées par le protocole de routage intradomaine.</p>
                        </li>
                    </ul>
                    <p>La première solution est la plus fréquente. Il n'est pas recommandé d'annoncer des routes apprises à partir d'un protocole de routage intradomaine, car si la route fluctue, cela causerait un grand nombre de messages BGP échangés dans l'Internet mondial. On dit qu'un lien est en train de fluctuer s'il passe plusieurs fois entre un état opérationnele et un état désactivé dans un court laps de temps. Un routeur attaché à un tel lien aurait besoin d'envoyer fréquemment des messages de routage.</p>
                    <hr>
                    <p>La plupart des réseaux qui utilisent BGP contiennent plus d'un routeur. Par exemple, considérons le réseau illustré dans la figure ci-dessous où <span class="em">AS20</span> contient deux routeurs reliés par des liens interdomaines : <span class="em">R2</span> et <span class="em">R4</span>. Dans ce réseau, deux protocoles de routage sont utilisés par <span class="em">R2</span> et <span class="em">R4</span>. Ils utilisent un protocole de routage intradomaine tel que OSPF pour distribuer les routes vers les préfixes internes : <span class="em">195.100.0.8/30</span>, <span class="em">195.100.0.0/30</span>, ... <span class="em">R2</span> et <span class="em">R4</span> utilisent également BGP. <span class="em">R2</span> reçoit les routes annoncées par <span class="em">AS10</span> tandis que <span class="em">R4</span> reçoit les routes annoncées par <span class="em">AS30</span>. Ces deux routeurs doivent échanger les routes qu'ils ont respectivement reçues via leurs sessions BGP.</p>
                    <figure>
                        <img src="../images/reseau_plus_large_utilisant_BGP.png" alt="">
                        <figcaption>Figure 5.63 : Un réseau plus large utilisant BGP</figcaption>
                    </figure>
                    <p>Une première solution pour permettre à <span class="em">R2</span> et <span class="em">R3</span> d'échanger les routes interdomaines qu'ils ont apprises via leurs sessions BGP respectives serait de configurer le protocole de routage intradomaine pour distribuer à l'intérieur de l'<span class="em">AS20</span> les routes apprises via les sessions BGP. Bien que les routeurs actuels prennent en charge cette fonctionnalité, c'est une mauvaise solution pour deux raisons :</p>
                    <ol>
                        <li>
                            <p>Les protocoles de routage intradomaines ne peuvent pas distribuer les attributs attachés à une route BGP. Si <span class="em">R4</span> recevait via le protocole de routage intradomaine une route vers <span class="em">194.100.0.0/23</span> que <span class="em">R2</span> a apprise via BGP, il ne saurait pas que la route a été initiée par <span class="em">AS10</span> et la seule annonce qu'il pourrait envoyer à <span class="em">R3</span> contiendrait un chemin AS incorrect.</p>
                        </li>
                    </ol>
                    <p>La meilleure solution pour permettre aux routeurs BGP de distribuer, à l'intérieur d'un AS, toutes les routes apprises via des sessions BGP, consiste à établir des sessions BGP entre tous les routeurs BGP à l'intérieur de l'AS. En pratique, il existe deux types de sessions BGP :</p>
                    <ul>
                        <li>
                            <p>Une session <span class="html">eBGP</span> ou <span class="html">session BGP externe</span>. Une telle session BGP est établie entre deux routeurs qui sont directement connectés et appartiennent à deux domaines différents.</p>
                        </li>
                        <li>
                            <p>Une session <span class="html">iBGP</span> ou <span class="html">session BGP interne</span>. Une telle session BGP est établie entre deux routeurs appartenant au même domaine. Ces deux routeurs n'ont pas besoin d'être directement connectés.</p>
                        </li>
                    </ul>
                    <p>En pratique, chaque routeur BGP à l'intérieur d'un domaine maintient une <span class="em">session iBGP</span> avec tous les autres routeurs BGP du domaine. L'utilisation d'un maillage complet de <span class="em">sessions iBGP</span> est adaptée aux petits réseaux. Cependant, cette solution ne permet pas de scaler les grands réseaux contenant des centaines ou plus de routeurs, car <span class="em">(n*(n-1))/2</span> sessions iBGP doivent être établies dans un domaine contenant <span class="em">n</span> routeurs BGP. Les grands domaines utilisent soit la <span class="html">réflexion de route (Route Reflection)</span> RFC 4456, soit les confédérations RFC 5065 pour scaler leur iBGP, mais cela dépasse le cadre de cette introduction. Cela crée un maillage complet de <span class="em">sessions iBGP</span> entre tous les routeurs BGP du domaine. Les <span class="em">sessions iBGP</span>, tout comme les <span class="em">sessions eBGP</span>, s'exécutent sur des connexions TCP. Notez que contrairement aux <span class="em">sessions eBGP</span> qui sont établies entre des routeurs directement connectés, les <span class="em">sessions iBGP</span> sont souvent établies entre des routeurs qui ne sont pas directement connectés.</p>
                    <p>Un point important à noter à propos des <span class="em">sessions iBGP</span> est qu'un routeur BGP ne fiffuse une route que sur une <span class="em">session iBGP</span> à condition que :</p>
                    <ul>
                        <li>
                            <p>le routeur utilise cette route pour faire avancer les paquets, et</p>
                        </li>
                        <li>
                            <p>la route a été apprise via l'une des <span class="em">sessions eBGP</span> du routeur.</p>
                        </li>
                    </ul>
                    <p>Un routeur n'annonce pas une route qu'il a apprise sur une <span class="em">session iBGP</span> sur une autre <span class="em">session iBGP</span>. Notez qu'un routeur peut bien sûr annoncer sur une <span class="em">session eBGP</span> une route qu'il a apprise sur une <span class="em">session iBGP</span>. Cette différence de comportement d'un routeur BGP sur une <span class="em">session iBGP</span> et une <span class="em">session eBGP</span> est due à l'utilisation d'un maillage complet de <span class="em">sessions iBGP</span>. Considérez un réseau contenant trois routeurs BGP : <span class="em">A</span>, <span class="em">B</span> et <span class="em">C</span> interconnectés via un maillage complet de sessions iBGP. Si le routeur <span class="em">A</span> apprend une route vers le préfixe <span class="em">p</span> depuis le routeur <span class="em">B</span>, le routeur <span class="em">A</span> n'a pas besoin d'annoncer la route reçue au routeur <span class="em">C</span> car le routeur <span class="em">C</span> apprend égalemnt la même route sur la <span class="em">session iBGP</span> <span class="em">C-B</span>.</p>
                    <p>Pour comprendre l'utilisation d'une <span class="em">session iBGP</span>, prenons l'exemple où le routeur <span class="em">R1</span> envoie <span class="em">U(194.100.0.0/23,195.100.0.1,AS10)</span> dans le réseau illustré ci-dessous. Ce message BGP est traité par <span class="em">R2</span> qui l'annonce sur sa <span class="em">session iBGP</span> avec <span class="em">R4</span>. L'<span class="em">Update BGP</span> envoyé par <span class="em">R2</span> contient le même nexthop et le même chemin AS que dans l'<span class="em">Update BGP</span> reçu par <span class="em">R2</span>. <span class="em">R4</span> envoie alors <span class="em">U(194.100.0.0/23,195.100.0.5,AS20:AS10)</span> à <span class="em">R3</span>. Notez que le nexthop BGP et le chemin AS ne sont mis à jour que lorsqu'une route BGP est annoncée sur une <span class="em">session eBGP</span>. Certains routeurs, lorsqu'ils reçoivent un <span class="em">Update BGP</span> sur une <span class="em">session eBGP</span>, définissent le nexthop de la route reçue sur l'une de leurs propres adresses. Cela s'appelle <span class="html">nexthop-self</span>. Voir, par exemple, [WMS2004] pour plus de détails.</p>
                    <figure>
                        <img src="../images/sessions_iBGP_eBGP.png" alt="">
                        <figcaption>Figure 5.64 : Sessions iBGP et eBGP</figcaption>
                    </figure>
                    <hr>
                    <p>Note : Interfaces de loopback et sessions iBGP :</p>
                    <p>En plus de leurs interfaces physiques, les routeurs peuvent également être configurés avec une interface de loopback spéciale [#fbgploop]_. Une interface de loopback est une interface logicielle qui est toujours active. Lorsqu'une interface de loopback est configurée sur un routeur, l'adresse associée à cette interface est annoncée par le protocole de routage intradomaine.</p>
                    <p>COnsidérons par exemple un routeur avec deux interfaces point-à-point et une interface de loopback. Lorsqu'une interface point-à-point échoue, elle devient inaccessible et le routeur ne peut plus recevoir de paquets via cette adresse IP. Ce n'est pas le cas pour l'interface de loopback. Elle reste accessible tant qu'au moins l'une des interfaces du routeur reste active. Les <span class="em">sessions iBGP</span> sont généralement établies en utilisant les adresses de loopback du routeur en tant que points d'extrémité. Cela permet à la <span class="em">session iBGP</span> et à sa connexion TCP sous-jacente de rester active même si des interfaces physiques échouent sur les routeurs.</p>
                    <hr>
                    <p>Maintenant que les routeurs peuvent apprendre des routes inter-domaines via des sessions iBGP et eBGP, examinons ce qui se passe lorsque le routeur <span class="em">R3</span> envoie un paquet destiné à <span class="em">194.100.1.234</span>. <span class="em">R3</span> transfère ce paquet à <span class="em">R4</span>. <span class="em">R4</span> utilise un protocole de routage intra-domaine et BGP. Sa table de routage BGP contient la correspondance de préfixe la plus longue suivante :</p>
                    <ul>
                        <li>
                            <p><span class="em">194.100.0.0/23</span> via <span class="em">195.100.0.1</span></p>
                        </li>
                    </ul>
                    <p>
                        Cette route indique que pour faire avancer un paquet vers 
                        <soan class="em">194.100.0.0/23</soan>
                        , <span class="em">R4</span> doit faire avancer le paquet le long de la route vers <span class="em">195.100.0.1</span>. Cependant, <span class="em">R4</span> n'est pas directement connecté à <span class="em">195.100.0.1</span>. <span class="em">R4</span> a appris une route qui correspond à cette adresse grâce à son protocole de routage intra-domaine qui a distribué les routes suivantes :</p>
                        <ul>
                            <li>
                                <p><span class="em">195.100.0.0/30</span> via <span class="em">195.100.0.10</span></p>
                            </li>
                            <li>
                                <p><span class="em">195.100.0.4</span> Est</p>
                            </li>
                            <li>
                                <p><span class="em">195.100.0.8/30</span> Nord</p>
                            </li>
                            <li>
                                <p><span class="em">194.100.2.0/23</span> via <span class="em">195.100.0.10</span></p>
                            </li>
                            <li>
                                <p><span class="em">194.100.0.4/23</span> Ouest</p>
                            </li>
                        </ul>
                        <p>Pour construire sa table de routage, <span class="em">R4</span> doit combiner les routes apprises du protocole de routage intradomaine avec celles apprises de BGP. Grâce à sa table de routage intradomaine, pour chaque route interdomaine, <span class="em">R4</span> remplace le nexhop BGP par son chemin le plus court calculé par le protocole de routage intradomaine. Dans la figure ci-dessus, <span class="em">R4</span> envoie des paquests vers <span class="em">194.100.0.0/23</span> via <span class="em">195.100.0.10</span>, à laquelle il est directement connecté via son interface <span class="em">North</span>. La table de routage résultante de <span class="em">R4</span>, qui associe une interface sortante pour un préfixe directement connecté ou un nexthop directement connecté et une interface sortante pour les préfixes appris via BGP, est la suivante :</p>
                        <ul>
                            <li>
                                <p><span class="em">194.100.0.0/23</span> via <span class="em">195.100.0.10</span> (Nord)</p>
                            </li>
                            <li>
                                <p><span class="em">195.100.0.0/30</span> via <span class="em">195.100.0.10</span> (Nord)</p>
                            </li>
                            <li>
                                <p><span class="em">195.100.0.4/30</span> Est</p>
                            </li>
                            <li>
                                <p><span class="em">195.100.0.8/30</span> Nord</p>
                            </li>
                            <li>
                                <p><span class="em">194.100.2.0/23</span> via <span class="em">195.100.0.10</span> (Nord)</p>
                            </li>
                            <li>
                                <p><span class="em">194.100.4.0/23</span> Ouest</p>
                            </li>
                        </ul>
                        <p>Il y a donc un couplage entre les tables de routage interdomaine et intradomaine. Si les routes intradomaines changent, par exemple en raison de pannes de liens ou de changements de métriques de liens, alors la table de forwariding doit être mise à jour sur chaque routeur car le chemin le plus court vers un nexthop BGP peut avoir changé.</p>
                        <p>Le dernier point à discuter avant d'examiner le processus de décision BGP est qu'un réseau oeut contenir des routeurs qui ne maintiennent aucune session eBGP. Ces routeurs peuvent petre des routeurs stub attachés à un seul routeur dans le réseau ou des routeurs centraux qui résident sur le chemin entre deux routeurs frontière qui utilisent BGP comme illustré dans la figure ci-dessous.</p>
                        <figure>
                            <img src="../images/comment_gerer_routeurs_non_BGP.png" alt="">
                            <figcaption>Figure 5.65 : Comment gérer les routeurs non-BGP ?</figcaption>
                        </figure>
                        <p>Dans le scénario ci-dessus, le routeur <span class="em">R2</span> doit être en mesure de transférer un paquet vers n'importe quelle destination dans le préfixe <span class="em">12.0.0.0/8</span> à l'intérieur de l'<span class="em">AS30</span>. Un tel paquet devrait être transmis par le routeur <span class="em">R5</span>, car ce routeur réside sur le chemin entre <span class="em">R2</span> et son nexthop BGP attaché à <span class="em">R4</span>. Deux solutions peuvent être utilisées pour s'assurer que <span class="em">R2</span> est en mesure de transférer de tels paquets inter-domaines :</p>
                        <ul>
                            <li>
                                <p>Activier BGP sur le routeur <span class="em">R5</span> et inclure ce routeur dans le maillage complet <span class="em">iBGP</span>. Deux sessions iBGP seraient ajoutées dans la figure ci-dessus : <span class="em">R2-R5</span> et <span class="em">R4-R5</span>. Cette solution fonctionne et est utilisée par de nombreux AS. Cependant, elle oblige tous les routeurs à avoir suffisament de ressources (CPU et mémoire) pour ecécuter BGP et maintenir une grande table de routage.</p>
                            </li>
                            <li>
                                <p>Encapsuler les paquets inter-domaines envoyés à travers l'AS afin que le routeur <span class="Em">R5</span> n'ait jamais besoin de transmettre un paquet dont la destination est en dehors de l'AS local. Différents mécanismes d'encapsulation existent. MultiProtocol Label Switching (MPLS) RFC 3031 et le Layer 2 Tunneling Protocol (L2TP) RFC 3931 sont fréquemment utilisés dans les grands domaines, mais une explication détaillée de ces techniques est hors de portée de cette section. Le schéma d'encapsulation le plus simple à comprendre est celui de l'IP sur IP défini dans la RFC 2003. Ce schéma d'encapsulation place un paquet IP (appelé paquet intérieur), incluant son payload, en tant que payload d'un paquet IP plus grand (appelé paquet extérieur). Il peut être utilisé par les routeurs de frontière pour transmettre des paquets via des routeurs qui ne maintiennent pas de table de routage BGP. Par exemple, dans la figure ci-dessus, si le routeur <span class="em">R2</span> doit transférer un paquet vers la destination <span class="em">12.0.0.1</span>, il peut ajouter à l'avant de ce paquet un en-tête IPv4 dont l'adresse source est définie sur l'une de ses adresses IPv4 et dont l'adresse de destination est l'une des adresses IPv4 de <span class="em">R4</span>. Le champ <span class="em">Protocol</span> de l'en-tête IP est défini sur <span class="em">4</span> pour indiquer qu'il contient un paquet IPv4. Le paquet est transmis par <span class="em">R5</span> à <span class="em">R4</span> en fonction de la table de transfert qu'il a construit grâce à sa table de routage intra-domaine. Á la réception du paquet, <span class="em">R4</span> supprime l'en-tête extérieur et consulte sa table de transfert (BGP) pour transférer le paquet vers <span class="em">R3</span>.</p>
                            </li>
                        </ul>
                        <p><span class="html">Le processus de décision de BGP</span> : Outre les filtres d'importation et d'exportation, une différence clé entre BGP et les protocoles de routage intra-domaine est que chaque domaine est que chaque domaine peut définir son propre algorithme de classement pour déterminer quelle route est choisie pour acheminer les paquets lorsque plusieurs routes ont été apprises vers le même préfixe. Ce classement dépend de plusieurs attributs BGP qui peuvent être attachés à une route BGP.</p>
                        <p>Le premier attribut BGP utilisé pour classer les routes BGP est l'attribut de <span class="html">préférence locale (local-pref pour local-preference)</span>. Cet attribut est un entier non signé qui est attaché à chaque route BGP reçue sur une session eBGP par le filtre d'importation associé.</p>
                        <p>Lors de la comparaison des routes vers le même préfixe de destination, un routeur BGP préfère toujours les routes avec la valeur <span class="em">local-pref</span>la plus élevée. Si le routeur BGP connaît plusieurs routes avec la même <span class="em">local-pref</span>, il préfère parmi les routes ayant cette <span class="em">local-pref</span> avec le chemin AS le plus court.</p>
                        <p>L'attribut <span class="em">local-pref</span> est souvent utilisé pour préférer certaines routes par rapport à d'autres. Cet attribut est toujours présent dans les <span class="em">BGP Update</span> échangées sur des <span class="em">sessions iBGP</span>, mais jamais présent dans les messages échangés sur des <span class="em">sessions eBGP</span>.</p>
                        <p>Une utilisation courante de la <span class="em">local-pref</span> est de prendre en charge les liens de secours. Considérons la situation représentée dans la figure ci-dessous. <span class="em">AS1</span> souhaite toujours utiliser le lien à haute bande passante pour envoyer et recevoir des paquets via <span class="em">AS2</span> et n'utilisent le lien de secours qu'en cas de défaillance du lien primaire.</p>
                        <figure>
                            <img src="../images/comment_creer_lien_sauvegarde_BGP.png" alt="">
                            <figcaption>Figure 5.66 : Comment créer un lien de sauvegarde avec BGP ?</figcaption>
                        </figure>
                        <p>Comme les routeurs BGP préfèrent toujours les routes avec l'attribut <span class="em">local-pref</span> le plus élevé, cette politique peut être implémentée en utilisant le filtre d'importation suivant sur <span class="em">R1</span>.</p>
<pre><code>import: from AS2 RA at R1 set localpref=100;
        from AS2 RB at R1 set localpref=200;
        accept ANY</code></pre>
                        <p>Avec ce filtre d'importation, toutes les routes BGP apprises de <span class="em">RB</span> via les liens haute bande passante sont préférées aux routes apprises via le lien de sauvegarde. Si le lien principal échoue, les routes correspondantes sont supprimées de la table de RIB de <span class="em">R1</span> et <span class="em">R1</span> utilise la route apprise de <span class="em">RA</span>. <span class="em">R1</span> réutilise les routes via <span class="em">RB</span> dès qu'elles sont annoncées par <span class="em">RB</span> une fois que le lien <span class="em">R1-RB</span> est rétabli.</p>
                        <p>Le filtre d'importation ci-dessus modifie la sélection des routes BGP à l'intérieur d'<span class="em">AS1</span>. Ainsi, il influence la route suivie par les paquets transmis par <span class="em">AS1</span>. En plus d'utiliser le lien principal pour envoyer des paquets, <span class="em">AS1</span> souhaite recevoir ses paquets via le lien à haute bande passante. Pour cela, <span class="em">AS2</span> doit également définir l'attribut de <span class="em">local-pref</span> dans son filtre d'importation.</p>
<pre><code>import: from AS1 R1 at RA set localpref=100;
        from AS1 R1 at RB set localpref=200;
        accept AS1</code></pre>
                        <p>Parfois, l'attribut de <span class="em">local-pref</span> est utilisé pour préférer une liaison <span class="em">bon marché</span> par rapport à une plus coûteuse. Par exemple, dans le réseau ci-dessous, <span class="em">AS1</span> peut souhaiter envoyer et recevoir des paquets principalement via sa liaison interdomaine avec <span class="em">AS4</span>.</p>
                        <figure>
                            <img src="../images/comment_preferer_lien_bon_marche_plus_cher.png" alt="">
                            <figcaption>Figure 5.67 : Comment préférer une liaison bon marché par rapport à une plus coûteuse ?</figcaption>
                        </figure>
                        <p><span class="em">AS1</span> peut installer le filtre d'importation suivant sur <span class="em">R1</span> pour s'assurer qu'il envoie toujours des paquets via <span class="em">R2</span> lorsqu'il a appris une route via <span class="em">AS2</span> et une autre via <span class="em">AS4</span>.</p>
<pre><code>import: from AS2 RA at R1 set localpref=100;
        from AS4 R2 at R1 set localpref=200;
        accept ANY</code></pre>
                        <p>Cependant, ce filtre d'importation n'influence pas la façon dont <span class="em">AS3</span>, par exemple, préfère certaines routes par rapport à d'autres. Si le lien entre <span class="em">AS3</span> et <span class="em">AS2</span> est moins cher que le lien entre <span class="em">AS3</span> et <span class="em">AS4</span>, <span class="em">AS3</span> pourrait envoyer tous ses paquets via <span class="em">AS2</span> et <span class="em">AS1</span> recevrait des paquets via son lien coûteux. Un point important à retenir à propos de <span class="em">local-pref</span> est qu'il peut être utilisé pour préférer certaines routes à d'autres pour envoyer des paquets, mais il n'a aucune influence sur les routes suivies par les paquets reçus.</p>
                        <p>Une autre utilisation importante de l'attribut <span class="em">local-pref</span> est de supporter les relations d'interconnexion <span class="em">client&#8594;fournisseur</span> et de partage des coûts. Du point de vue économique, il existe une différence importante entre ces trois types de relations d'interconnexion. Un domaine gagne généralement de l'argent lorsqu'il envoie des paquets via une relation <span class="em">client&#8594;fournisseur</span>. En revanche, il doit payer son fournisseur lorsqu'il envoie des paquets via une relation <span class="em">fournisseur&#8594;client</span>. Utiliser une interconnexion de partage des coûts pour envoyer des paquets est généralemnt neutre du point de vue économique. Pour prendre en compte ces problèmes économiques, les domaines configurent généralement les filtres d'importation sur leurs routeurs de la manière suivante :</p>
                        <ul>
                            <li>
                                <p>Insérer un attribut <span class="em">local-pref</span> élevé dans les routes apprises d'un client.</p>
                            </li>
                            <li>
                                <p>Insérer un attribut <span class="em">local-pref</span> moyen dans les routes apprises via une interconnexion de partage des coûts.</p>
                            </li>
                            <li>
                                <p>Insérer un attribut <span class="em">local-pref</span> faible dans les routes apprises d'un fournisseur.</p>
                            </li>
                        </ul>
                        <p>Avec un tel filtre d'importation, les routeurs d'un domaine préfèrent toujours atteindre les destinations via leurs clients chaque fois qu'une telle route existe. Sinon, ils préfèrent utiliser des relations d'échange de coûts partagés et ne n'envoient des paquets via leurs fournisseurs que lorsqu'ils ne connaissent aucune route alternative. Une conséquence de la définition de l'attribut de <span class="em">local-pref</span> de cette manière est que les chemins Internet sont souvent asymétriques. Considérez par exemple l'inter-réseau montré dans la figure ci-dessous.</p>
                        <figure>
                            <img src="../images/asymetrie_chemins_Internet.png" alt="">
                            <figcaption>Figure 5.68 : Asymétrie des chemins Internet</figcaption>
                        </figure>
                        <p>Considérons, dans cet inter-réseau, les routes disponibles à l'intérieur de <span class="em">AS1</span> pour atteindre <span class="em">AS5</span>. <span class="em">AS1</span> apprend le chemin <span class="em">AS4:AS6:AS7:AS5</span> depuis <span class="em">AS4</span>, le chemin <span class="em">AS3:AS8:AS5</span> depuis <span class="em">AS3</span> et le chemin <span class="em">AS2:AS5</span> depuis <span class="em">AS2</span>. Le premier chemin est choisi car il a été appris depuis un client. D'autre part, <span class="em">AS5</span> reçoit trois chemins vers <span class="em">AS1</span> via ses fournisseurs. Il peut sélectionner n'importe lequel de ces chemins pour atteindre <span class="em">AS1</span>, en fonction de ses préférences pour un fournisseur plutôt qu'un autre.</p>
                        <p>En revenant à l'organisation d'un routeur BGP présentée dans la figure "<span class="em">Organisation d'un routeur BGP</span>", la dernière partie à discuter est le processus de décision BGP. Le processus de décision BGP est l'algorithme utilisé par les routeurs pour sélectionner la route à installer dans la FIB lorsqu'il y a plusieurs routes vers le même préfixe. Le processus de décision BGP reçoit un ensemble de routes candidates vers le même préfixe et utilise sept étapes. À chaque étape, certaines routes sont retirées de l'ensemble des candidats et le processus lorsque l'ensemble ne contient plus qu'une seule route. Certaines implémentations de BGP peuvent être configurées pour installer plusieurs routes vers un même préfixe dans leur FIB à des fins d'équilibrage de charge. Cependant, cela dépasse cette introduction à BGP.</p>
                        <ol>
                            <li>
                                <p>Ignorer les routes ayant un nexthop BGP injoignable.</p>
                            </li>
                            <li>
                                <p>Préférer les routes ayant la local-pref la plus haute.</p>
                            </li>
                            <li>
                                <p>Préférer les routes ayant le plus court chemin AS.</p>
                            </li>
                            <li>
                                <p>Préférer les routes ayant le MED le plus petit.</p>
                            </li>
                            <li>
                                <p>Préférer les routes apprises via les sessions eBGP aux routes apprises via les sessions iBGP.</p>
                            </li>
                            <li>
                                <p>Préférer les routes ayant le nexthop le plus proche.</p>
                            </li>
                            <li>
                                <p>Règles de résolution des égalités : préférer les routes apprises du routeur ayant le plus bas ID de routeur.</p>
                            </li>
                        </ol>
                        <p>La première étape du processus de décision BGP assure qu'un routeur BGP n'installe pas dans sa table FIB une route dont le nexthop est considéré comme inaccessible par le protocole de routage intra-domaine. Cela pourrait se produire, par exemple, lorsqu'un routeur a planté. Le protcole de routage intra-domaine signale généralement la défaillance de ce routeur avant la défaillance des sessions BGP qu'il termine. Cette règle implique que le processus de décision BGP doit être relancé chaque fois que le prototole de routage intra-domaine signale une modification de la disponibilité d'un préfixe contenant un ou plusieurs nexthops BGP.</p>
                        <p>La deuxième règle permet à chaque domaine de définir ses préférences de routage. L'attribut <span class="em">local-pref</span> est défini par le filtre d'importation du routeur qui a appris une route via une session eBGP.</p>
                        <p>En contraste avec les protocoles de routage intradomaine, BGP ne contient pas de métrique explicite. Cela est dû au fait qu'il est impossible pour tous les domaines de s'entendre sur une métrique commune qui répondrait aux exigences de tous les domaines. Malgré cela, les routeurs BGP préfèrent les routes ayant un attribut de chemin AS court par rapport aux routes ayant un chemin AS long. Cette étape du processus de décision de BGP est motivée par le fait que les opérateurs s'attendent à ce qu'une route avec un chemin AS long soit de moins bonne qualité qu'une route avec un chemin AS plus court. Cependant, des études ont montré qu'il n'y avait pas toujours une forte corrélation entre la qualité d'une route et la longueur de son chemin AS [HFPMC2002].</p>
                        <p>Avant d'expliquer la quatrième étape du processus de décision de BGP, expliquons d'abord les cinquième et sixième étapes. Ces deux étapes sont utilisées pour implémenter le <span class="html">routage de la patate chaude</span>. Intuitivement, lorsqu'un domaine implémente le <span class="em">routage de la patate chaude</span>, il essaie de transférer les paquets destinés à des adresses en dehors de son domaine, vers d'autres domaines aussi rapidement que possible.</p>
                        <p>Pour comprendre le <span class="em">routage de la patate chaude</span>, considérons les deux domaines présentés dans la figure ci-dessous. <span class="em">AS2</span> annonce le préfixe <span class="em">1.0.0.0/8</span> sur les liens de peering <span class="em">R2-R6</span> et <span class="em">R3-R7</span>. Les routeurs à l'intérieur d'<span class="em">AS1</span> apprennent deux routes vers <span class="em">1.0.0.0/8</span> : l'une via <span class="em">R6-R2</span> et la deuxième via <span class="em">R7-R3</span>.</p>
                        <figure>
                            <img src="../images/routage_chaud_froid_patate.png" alt="">
                            <figcaption>Figure 5.69 : Routage de la patate chaude et de la patate froide</figcaption>
                        </figure>
                        <p>Avec la cinquième étape du processus de décision BGP, un routeur oréfère toujours utiliser une route apprise via une <span class="em">session eBGP</span> par rapport à une route apprise via une <span class="em">session iBGP</span>. Ainsi, le routeur <span class="em">R6</span> (resp. <span class="em">R7</span>) préfère utiliser la route via le routeur <span class="em">R2</span> (resp. <span class="em">R3</span>) pour atteindre le préfixe <span class="em">1.0.0.0/8</span>.</p>
                        <p>La sixième étape du processus de décision BGP prend en compte la distance, mesurée comme la longueur du chemin intradomaine le plus court, entre un routeur BGP et le nexthop BGP pour les routes apprises via les <span class="em">sessions iBGP</span>. Cette règle est utilis&e sur le routeur <span class="em">R8</span> dans l'exemple ci-dessus. Ce routeur a reçu deux routes vers <span class="em">1.0.0.0/8</span> :</p>
                        <ul>
                            <li>
                                <p><span class="em">1.0.0.0/8</span> via <span class="em">R7</span> qui est à une distance de <span class="em">1</span> de <span class="em">R8</span>.</p>
                            </li>
                            <li>
                                <p><span class="em">1.0.0.0/8</span> via <span class="em">R6</span> qui est à une distance de <span class="em">50</span> de <span class="em">R8</span>.</p>
                            </li>
                        </ul>
                        <p>La première route, via <span class="em">R7</span>, est celle que le routeur <span class="em">R8</span> préfère, car c'est la route qui minimise le coût de transfert des paquets à l'intérieur de <span class="em">AS1</span> avant de les envoyer à <span class="em">AS2</span>.</p>
                        <p>Le <span class="html">routage de la patata chaude (hot potato routing)</span> permet à <span class="em">AS1</span> de minimiser le coût de transfert des paquets vers <span class="em">AS2</span>. Cependant, il existe des situations où cela n'est pas souhaitable. Par exemple, supposons que <span class="em">AS1</span> et <span class="em">AS2</span> sont des domaines avec des routeurs sur côtes Est et Ouest des États-Unis. Dans ces deux domaines, la métrique élevée associée aux liens <span class="em">R6-R8</span> et <span class="em">R0-R2</span> correspond au coût de transfert d'un paquet à travers les États-Unis. Si <span class="em">AS2</span> est un client qui paie <span class="em">AS1</span>, il préférerait recevoir les paquets destinés à <span class="em">1.0.0.0/8</span> via le lien <span class="em">R2-R6</span> plutôt que via le lien <span class="em">R7-R3</span>. C'est l'objectif du <span class="html">routage de la patate froide (cold patato routing)</span>.</p>
                        <p>Le <span class="html">routage de la patate froide (cold potato routing)</span> est implémenté en utilisant l'attribut <span class="html">Multi-Exit Discriminator (MED)</span>. Cet attribut est un attribut optionnel BGP qui peut être défini par les routeurs de bordure lors de l'annonce d'une route BGP sur une <span class="em">session eBGP</span>. L'attribut MED peut être utilisé sur les relations d'interconnexion <span class="em">client&#8594;fournisseur</span> à la demande du client. Sur les relations d'interconnexion à coût partagé, l'attribut MED n'est activé que s'il y a un accord explicite entre les deux pairs. L'attribut MED est généralement utilisé pour indiquer sur une <span class="em">session eBGP</span> le coût pour atteindre le nexthop BGP pour la route annoncée. L'attribut MED est défini par le routeur qui annonce une route sur une <span class="em">session eBGP</span>. Dans l'exemple ci-dessus, le routeur <span class="em">R2</span> envoie <span class="em">U(1.0.0.0/8,R2,AS2,MED=1)</span> tandis que <span class="em">R3</span> envoie <span class="em">U(1.0.0.0/8,R3,AS2,MED=98)</span>.</p>
                        <p>On suppose que la session BGP <span class="em">R7-R3</span> est la première  à être établie. <span class="em">R7</span> envoie <span class="em">U(1.0.0.0/8,R3,AS2,MED=98)</span> à la fois à <span class="em">R8</span> et à <span class="em">R6</span>. À ce stade, tous les routeurs à l'intérieur de <span class="em">AS1</span> envoient les paquets vers <span class="em">1.0.0.0/8</span> via <span class="em">R7-R3</span>. Ensuite, la session BGP <span class="em">R6-R2</span> est établie et le routeur <span class="em">R6</span> reçoit <span class="em">U(1.0.0.0/8,R2,AS2,MED=1)</span>. Le routeur <span class="em">R6</span> exécute son processus de décision pour la destination <span class="em">1.0.0.0/8</span> et sélectionne la route via <span class="em">R2</span> comme la route choisie pour atteindre ce préfixe, car c'est la seule route qu'il connaît. <span class="em">R6</span> envoie <span class="em">U(1.0.0.0/8,R2,AS2,MED=1)</span> aux routeurs <span class="em">R8</span> et <span class="em">R7</span>. Ils exécutent tous deux leur processus de décision et préfèrent la route annoncée par <span class="em">R6</span>, car elle contient le MED le plus petit. Maintenant, tous les routeurs à l'intérieur de <span class="em">AS1</span> transfèrent les paquets vers <span class="em">1.0.0.0/8</span> via le lien <span class="em">R6-R2</span> comme prévu par <span class="em">AS2</span>. Comme le routeur <span class="em">R7</span> n'utilise plus la route BGP apprise via <span class="em">R3</span>, il doit cesser de l'annoncer sur les <span class="em">sessions iBGP</span> et envoie <span class="em">W(1.0.0.0/8)</span> sur ses <span class="em">sessions iBGP</span> avec <span class="em">R6</span> et <span class="em">R8</span>. Cependant, le routeur <span class="em">R7</span> conserve toujours la route apprise de <span class="em">R3</span> à l'intérieur de son <span class="em">Adj-RIB-In</span>. Si le lien <span class="em">R6-R2</span> échoue, <span class="em">R6</span> envoie <span class="em">W(1.0.0.0/8)</span> sur ses <span class="em">sessions iBGP</span> et le routeur <span class="em">R7</span> répond en envoyant <span class="em">U(1.0.0.0/8,R3,AS2,MED=98)</span> sur ses <span class="em">sessions iBGP</span>.</p>
                        <p>En pratique, la cinquième étape du processus de décision BGP est légèrement plus complexe car les routes vers un préfixe donné peuvent être apprises à partir de différents AS. Par exemple, supposons que dans la figure "<span class="em">Routage de la patate chaude et de la patate froide</span>", <span class="em">1.0.0.0/8</span> est également annoncé par <span class="em">AS3</span> (non représenté dans la figure) qui a des liens de peering avec les routeurs <span class="em">R6</span> et <span class="em">R8</span>. Si <span class="em">AS3</span> annonc e une route dont l'attribut MED est défini sur <span class="em">2</span> et une autre avec un MED sur <span class="em">3</span>, comment le routeur d'<span class="em">AS1</span> doit-il comparer les quatre routes BGP vers <span class="em">1.0.0.0/8</span> ? Une valeur MED de <span class="em">1</span> provenant d'<span class="em">AS2</span> est-elle meilleure qu'une valeur MED de <span class="em">2</span> provenant d'<span class="em">AS3</span> ? La cinquième étape de processus de décision BGP résout ce problème en ne comparant que l'attribut MED des routes apprises à partir du même AS voisin. Des détails supplémentaires sur l'attribut MED peuvent être trouvés dans la RFC 4451. Il convient de noter que l'utilisation de l'attribut MED peut causer certains problèmes dans les réseaux BGP comme expliqué dans [GW2002]. En pratique, l'attribut MED n'est pas utilisé sur les <span class="em">sessions eBGP</span> à moins que les deux domaines ne conviennent de l'activer.</p>
                        <p>La dernière étape de la décision BGP permet de sélectionner une seule route lorsqu'un routeur BGP a reçu plusieurs routes considérées comme équivalentes par les six premières étapes du processus de décision. Cela peut se produire, par exemple, dans une extrémité double attachée à deux fournisseurs différents. Comme illustré dans la figure ci-dessous, le routeur <span class="em">R1</span> reçoit deux routes BGP également bonnes vers <span class="em">1.0.0.0/8</span>. Pour rompre l'égalité, chaque routeur est identifié par un identifiant de routeur unique qui, en pratique, est l'une des adresses IP attribuées au routeur. Sur certains routeurs, l'étape de l'identifiant de routeur le plus bas dans le processus de décision BGP est remplacée par la sélection de la route la plus ancienne RFC 5004. Préférer la route la plus ancienne pour rompre les égalités est utilisé pour privilégier les chemins stables par rapport aux chemins instables. Cependnat, un inconvénient de cette approche est que la sélection des routes BGP dépend des heures d'arrivée des messages correspondants Cela rend le processus de sélection BGP non déterministe et peut conduire à des problèmes difficiles à déboguer.</p>
                        <figure>
                            <img src="../images/stub_connecte_2_fournisseurs.png" alt="">
                            <figcaption>Figure 5.70 : Un stub connecté à deux fournisseurs</figcaption>
                        </figure>
                        <p><span class="html">La convergence BGP :</span> Dans les sections précédentes, nous avons expliqué le fonctionnement des routeurs BGP. Comparé aux protocoles de routage intradomaine, une caractéristqique clé de BGP est sa capacité à prendre en charge des politiques de routage interdomaine qui sont définies par chaque domaine en tant que filtres d'importation et d'exportation et processus de classement. Un domaine peut définir ses propres politiques de routage et les fournisseurs de routeurs ont implémenté de nombreux ajustements de configuration pour prendre en charge des politiques de routage complexes. Cependant, la politique de routage choisie par un domaine peut interférer avec la politique de routage choisie par un autre domaine. Pour comprendre cette question, examinons d'abord l'inter-réseau simple illustré ci-dessous.</p>
                        <figure>
                            <img src="../images/inter-reseau_en_desaccord.png" alt="">
                            <figcaption>Figure 5.71 : L'inter-réseau en désaccord</figcaption>
                        </figure>
                        <p>Dans cet inter-réseau, nous nous concentrons sur la route vers <span class="em">1.0.0.0/8</span> qui est annoncée par <span class="em">AS1</span>. Supposons également que <span class="em">AS3</span> (resp. <span class="em">AS4</span>) préfère, par exemple pour des raisons économiques, une route apprise d'<span class="em">AS4</span> (resp. <span class="em">AS3</span>) plutôt qu'une route apprise d'<span class="em">AS1</span>. Lorsque <span class="em">AS1</span> envoie <span class="em">U(1.0.0.0/8,AS1)</span> à <span class="em">AS3</span> et <span class="em">AS4</span>, trois séquences d'échanges de messages BGP sont possibles :</p>
                        <ol>
                            <li>
                                <p><span class="em">AS3</span> envoie d'abord <span class="em">U(1.0.0.0/8,AS3:AS1)</span> à <span class="em">AS4</span>. <span class="em">AS4</span> a appris deux routes vers <span class="em">1.0.0.0/8</span>. Il exécute son processus de décision BGP et sélectionne la route via <span class="em">AS3</span> et n'annonce pas de route à <span class="em">AS3</span>.</p>
                            </li>
                            <li>
                                <p><span class="em">AS4</span> envoie d'abord <span class="em">U(1.0.0.0/8,AS3:AS1)</span> à <span class="em">AS3</span>. <span class="em">AS3</span> a appris deux routes vers <span class="em">1.0.0.0/8</span>. Il exécute son processus de décision BGP et sélectionne la route via <span class="em">AS4</span> et n'annonce pas de route à <span class="em">AS4</span>.</p>
                            </li>
                            <li>
                                <p><span class="em">AS3</span> envoie <span class="em">U(1.0.0.0/8,AS3:AS1)</span> à <span class="em">AS4</span> et, en même temps, <span class="em">AS4</span> envoie <span class="em">U(1.0.0.0/8,AS4:AS1)</span>. <span class="em">AS3</span> préfère la route via <span class="em">AS4</span> et envoie donc <span class="em">W(1.0.0.0/8)</span> à <span class="em">AS4</span>. Pendant ce temps, <span class="em">AS4</span> préfère la route via <span class="em">AS3</span> et envoie donc <span class="em">W(1.0.0.0/8)</span> à <span class="em">AS3</span>. À la réception des retraits BGP, <span class="em">AS3</span> et <span class="em">AS4</span> ne connaissent que la route directe vers <span class="em">1.0.0.0/8</span>. <span class="em">AS3</span> (resp. <span class="em">AS4</span>) envoie <span class="em">U(1.0.0.0/8),AS3:AS1)</span> (resp. <span class="em">U(1.0.0.0/8,AS4:AS1)</span>) à <span class="em">AS4</span> (resp. <span class="em">AS3</span>). <span class="em">AS3</span> et <span class="em">AS4</span> pourraient théoriquement continuer à échanger des mmesages BGP pour toujours. En pratique, l'un d'eux envoie un message plus rapidement que l'autre et BGP converge.</p>
                            </li>
                        </ol>
                        <p>L'exemple ci-dessus a montré que les routes sélectionnées par les routeurs BGP peuvent parfois dépendre de l'ordre des messages BGP échangés. D'autres scénarios similaires peuvent être trouvés dans la RFC 4264.</p>
                        <p>D'un point de vue opérationnel, la configuration ci-dessus est ennuyeuse car les opérateurs réseau ne peuvent pas facilement prédire quelles sont les chemins choisis. Malheureusement, il existe des configurations BGP encore plus ennuyeuses. Par exemple, considérons la configuration ci-dessous qui est souvent appelée Bad Gadget [GW1999].</p>
                        <p>Dans cet inter-réseau, il y a quatre AS. <span class="em">AS20</span> annonce une route vers un préfixe et nous n'analysons que les routes vers ce préfixe. Les préférence de routage d'<span class="em">AS1</span>, <span class="em">AS3</span> et <span class="em">AS4</span> sont les suivantes :</p>
                        <ul>
                            <li>
                                <p><span class="em">AS1</span> préfère le chemin <span class="em">AS3:AS0</span> sur tous les autres chemins.</p>
                            </li>
                            <li>
                                <p><span class="em">AS3</span> préfère le chemin <span class="em">AS4:AS0</span> sur tous les autres chemins.</p>
                            </li>
                            <li>
                                <p><span class="em">AS4</span> préfère le chemin <span class="em">AS1:AS0</span> sur tous autres chemins.</p>
                            </li>
                        </ul>
                        <p><span class="em">AS0</span> envoie <span class="em">U(p,AS0)</span> à <span class="em">AS1</span>, <span class="em">AS3</span> et <span class="em">AS4</span>. Comme c'est la seule route connue par <span class="em">AS1</span>, <span class="em">AS3</span> et <span class="em">AS4</span> vers <span class="em">p</span>, il sélectionnent tous le chemin direct. Considérons maintenant un échange possible de messages BGP :</p>
                        <ol>
                            <li>
                                <p><span class="em">AS1</span> envoie <span class="em">U(p,AS1:AS0)</span> à <span class="em">AS3</span> et <span class="Em">AS4</span>. <span class="em">AS4</span> sélectionne le chemin via <span class="em">AS1</span> car c'est son chemin préféré. <span class="em">AS3</span> utilise toujours le chemin direct.</p>
                            </li>
                            <li>
                                <p><span class="em">AS4</span> annonce <span class="em">U(p,AS4:AS1:AS0)</span> à <span class="em">AS3</span>.</p>
                            </li>
                            <li>
                                <p><span class="em">AS3</span> envoie <span class="em">U(p,AS3:AS0)</span> à <span class="em">AS1</span> et <span class="em">AS4</span>. <span class="em">AS1</span> sélectionne le chemin via <span class="em">AS3</span> car c'est son chemin préféré. <span class="em">AS4</span> utilise toujours le chemin via <span class="em">AS1</span>.</p>
                            </li>
                            <li>
                                <p>Comme <span class="em">AS1</span> a changé son chemin, il envoie <span class="em">U(p,AS1:AS3:AS0)</span> à <span class="em">AS4</span> et <span class="em">W(p)</span> à <span class="em">AS3</span>. <span class="em">AS4</span> renvient au chemin direct.</p>
                            </li>
                            <li>
                                <p><span class="em">AS4</span> envoie <span class="em">U(p,AS4:AS0)</span> à <span class="em">AS1</span> et <span class="em">AS3</span>. <span class="em">AS3</span> préfère le chemin via <span class="em">AS4</span>.</p>
                            </li>
                            <li>
                                <p><span class="em">AS3</span> envoie <span class="em">U(p,AS3:AS4:AS0)</span> à <span class="em">AS1</span> et <span class="Em">W(p)</span> à <span class="em">AS4</span>. <span class="em">AS1</span> revient au chemin direct et nous sommes de retour à la première étape.</p>
                            </li>
                        </ol>
                        <figure>
                            <img src="../images/" alt="">
                            <figcaption>Figure 5.72 : L'inter-réseau de mauvais gadget</figcaption>
                        </figure>
                        <p>Cet exemple montre malheureusement que la convergence de BGP n'est pas toujours garantie car certaines politiques de routage interdomaine peuvent interférer de manière complexe les unes avec les autres. [GW1999] ont montré que vérifier la convergence est soit NP-complet, soit NP-difficile. Voir [GSW2002] pour une discussion plus détaillée.</p>
                        <p>Heureusement, il existe des directives opérationnelles [GR2001] [GGR2001] qui peuvent garantir la convergence de BGP dans l'Internet mondial. Pour garantir la convergence de BGP, ces directives considèrent qu'il existe deux types de relations d'interconnexion : <span class="em">client&#8594;fournisseur</span> et <span class="em">partage de coût</span>. Dans ce cas, la convergence de BGP est garantie à condition que les conditions suivantes soient remplies :</p>
                        <ol>
                            <li>
                                <p>La topologie composée de tous les liens d'interconnexion dirigés <span class="em">client&#8594;fournisseur</span> forme un graphe acyclique.</p>
                            </li>
                            <li>
                                <p>Un AS préfère toujours une route reçue d'un client à une route reçue d'un pair de <span class="em">partage de coûts</span> ou d'un <span class="em">fournisseur</span>.</p>
                            </li>
                        </ol>
                        <p>La première directive implique que le fournisseur du fournisseur de <span class="em">ASx</span> ne peut pas être un client de <span class="em">ASx</span>. Une telle relation n'aurait pas de sens d'un point de vue économique car elle impliquerait des paiements circulaires. De plus, les fournisseurs sont généralement plus importants que les clients.</p>
                        <p>La deuxième directive correspond également aux préférences économiques. Étant donné qu'un fournisseur gagne de l'argent en envoyant des paquets à l'un de ses clients, il est logique de préférer de telles routes apprises à partir de clients plutôt que des routes apprises à partir de fournisseurs. [GR2001] montre également que la convergence de BGP est garantie même si un AS associe la même préférence aux routes apprises à partir d'un pair à <span class="em">coût partagé</span> et aux routes apprises à partir d'un client.</p>
                        <p>D'un point de vue théorique, ces directives devraient être vérifiées automatiquement pour s'assurer que BGP convergera toujours dans l'Internet global. Cependant, une telle vérification ne peut pas être effectuée en pratique car cela obligerait tous les domaines à divulguer leurs politiques de routage (et peu sont disposés à le faire) et en outre le problème est connu pour être NP-difficile [GW1999].</p>
                        <p>En pratique, les chercheurs et les opérateurs s'attendent à ce que ces directives soient vérifiées dans la plupart des domaines. Certains chercheurs, comme [MUF+2007], ont montré que la modélisation de la topologie de l'Internet au niveau des AS nécessite plus que les relations de peering de <span class="em">partage de coûts</span> et de <span class="em">client&#8594;fournisseur</span>. Cependant, il n'existe aucun modèle disponible publiquement qui va au-delà de ces relations de peering classiques. Grâce à la grande quantité de données BGP qui ont été collectées par les opérateurs et les chercheurs, plusieurs études ont analysé la topologie de l'Internet au niveau des AS. Les données BGP sont souvent collectées en établissant des sessions BGP entre des hôtes Unix exécutant un démon BGP et des routeurs BGP dans différents AS. Les hôtes Unix stockent tous les messages BGP reçus et des sauvegardes régulières de leur table de routage BGP. Voir <a href="http://www.routeviews.org/" target="_blank">http://www.routeviews.org/</a>, <a href="http://www.ripe.net/ris" target="_blank">http://www.ripe.net/ris</a>, <a href="http://bgp.potaroo.net/" target="_blank">http://bgp.potaroo.net/</a> ou <a href="http://irl.cs.ucla.edu/topology/" target="_blank">http://irl.cs.ucla.edu/topology/</a>. [SARK2002] est l'une des premières analyses. Des études plus récentes comprennent [COZ2008] et [DKF+2007].</p>
                        <p>D'après ces études et [ATLAS2009], la topologie de l'Internet au niveau des AS peut être résumée comme indiqué dans la figure ci-dessous.</p>
                        <figure>
                            <img src="../images/structure_couches_internet_mondial.png" alt="">
                            <figcaption>Figure 5.73 : La structure en couches de l'Internet global</figcaption>
                        </figure>
                        <p>Les domaines sur Internet peuvent être divisés en environ quatre catégories selon leur rôle et leur position dans la topologie au niveau des AS :</p>
                        <ul>
                            <li>
                                <p>Le coeur d'Internet est composé d'une douzaine à vingt des fournisseurs de services Internet (ISP) de <span class="html">niveau 1 (Tier-1)</span>. Un <span class="em">niveau 1</span> est un domaine qui n'a pas de <span class="em">fournisseur</span>. Ce type de ISP a des relations de peering à <span class="em">coûts partagés</span> avec tous les autres ISP de <span class="em">niveau 1</span> et des relations de <span class="em">fournisseur&#8594;client</span> avec les plus petits ISP. Des exemples de ISP de <span class="em">niveau 1</span> incluent <span class="html">Sprint</span>, <span class="html">Level3</span> ou <span class="html">Opentransit</span>.</p>
                            </li>
                            <li>
                                <p>Les ISP de <span class="html">niveau 3 (Tier-2)</span> sont des ISP nationaux ou continentaux qui sont clients de ISP de <span class="em">niveau 1</span>. Ces ISP de <span class="em">niveau 2</span> ont des clients plus epetits et des relations de peering à <span class="em">coûts partagés</span> avec d'autres ISP de <span class="em">niveau 2</span>. Des exemples de ISP de <span class="em">niveau 2</span> incluent France Telecom, Belgacom, British Telecom, ...</p>
                            </li>
                            <li>
                                <p>Les réseaux de <span class="html">niveau 3 (Tier-3)</span> sont soit des domaines stub tels que des réseaux de réseaux d'ntreprise ou de campus, soit des ISP plus petits. Ils sont clients de ISP de niveau 1 et de niveau 3 et ont parfois des realtions de peering à <span class="em">coûts partagés</span>.</p>
                            </li>
                            <li>
                                <p>Les grands fournisseur de contenu qui gèrent de grands centres de données. Ces fournisseurs de contenu produisent une fraction croissante des paquets échangés sur l'Internet mondial [ATLAS2009]. Certains de ces fournisseurs de contenu sont clients de ISP de niveau 1 ou de niveau 2, mais ils essaient d'établir des relations de peering à <span class="em">coûts partagés</span>, par exemple dans les IXP, avec de nombreux ISP de niveau 1 et de niveau 2.</p>
                            </li>
                        </ul>
                        <p>En raison de cette organisation de l'Internet et du processus de décision BGP, la plupart des chemins au niveau des AS sur Internet ont une longueur de <span class="em">3</span> à <span class="em">5</span> sauts d'AS.</p>
                        <h4>5.1.5 : Résumé :</h4>
                        <h4>5.1.5 Exercices</h4>
                        <h5>Principes :</h5>
                        <ol>
                            <li>
                                <p>Les protocoles de routage utilisés dans les réseaux de donénes n'utilisent que des pondérations de lien positives. Que se passerait-il avec un protocole de routage vectoriel de distance dans le réseau ci-dessous qui contient une pondération de lien négative ?</p>
                                <figure>
                                    <img src="../images/reseau_simple.jpg" alt="">
                                    <figcaption>Figure 5.74 : Réseau simple</figcaption>
                                </figure>
                            </li>
                            <li>
                                <p>Lorsqu'un spécialiste résoudre conçoit un réseau, un des problèmes qu'il doit résoudre est de déninir les métriques des liens dans son réseau. Aux États-Unis, le réseau Abilene intereconnecte la plupart des laboratoires de recherche et universités. La figure ci-dessous montre la toppologie de ce réseau en 2009. Cette figure a été télechagée depuis l'observatoire Albilene <a href="http://www.internet2.edu/observatory/archive/data-views.html" target="_blank">http://www.internet2.edu/observatory/archive/data-views.html</a>. Cet observatoire contient une description détaillée du réseau Abilene, y compris des statistiques réseau détaillées et toute la configuration de l'équipement utilisé dans le réseau.</p>
                                <figure>
                                    <img src="../images/reseau_Abilene.jpg" alt="">
                                    <figcaption>Figure 5.75 : Le réseau Abilene</figcaption>
                                </figure>
                                <p>Dans ce réseau, supposez que tous les poids de lien sont fixés à 1. Quels sont les chemins empruntés par un paquet envoyé par le routeur situé à <span class="em">Los Angeles</span> pour atteindre :</p>
                                <ul>
                                    <li>
                                        <p>le routeur situé à <span class="em">New York</span> ?</p>
                                    </li>
                                    <li>
                                        <p>le routeur situé à <span class="em">Washington</span> ?</p>
                                    </li>
                                </ul>
                                <p>Est-il possible de configurer les métriques de lien de sorte que les paquets envoyés par le routeur situé à <span class="em">Los Angeles</span> vers les routeurs situés respectivemnt à <span class="em">New York</span> et à <span class="em">Washington</span> ne suivent pas le même chemin ?</p>
                                <p>Est-il possible de configurer les poids de lien de sorte que les pquets envoyés par le routeur situé à <span class="em">Los Angeles</span> vers le routeur situé à <span class="em">New York</span> empruntent un chemin différent de lui emprunté par les paquets envoyés par le routeur situé à <span class="em">New York</span> vers le routeur situé à <span class="em">Los Angeles</span> ?</p>
                                <p>Supposez que les routeurs situés à <span class="em">Denver</span> et à <span class="em">Kansas City</span> doivent échanger beaucoup de paquets. Pouvez-vous configurer les métriques de lien de sorte que le lien entre ces deux routeurs ne transporte aucun paquet envoyé par un autre routeur dans le réseau ?</p>
                            </li>
                            <li>
                                <p>Dans le réseau à cinq noeuds ci-dessous, pouvez-vous configurer les métriques de lien de sorte que les paquets envoyés par le routeur <span class="em">E</span> au routeur <span class="em">A</span> utilisent le lien <span class="em">B&#8594;A</span> tandis que les paquets envoyés par le routeur <span class="em">B</span> utilisent les liens <span class="em">B&#8594;D</span> et <span class="em">D&#8594;A</span> ?</p>
                                <figure>
                                    <img src="../images/reseau_simple_cinq_noeuds.png" alt="">
                                    <figcaption>Figure 5.76  : Réseau simple à cinq noeuds</figcaption>
                                </figure>
                            </li>
                            <li>
                                <p>Dans le réseau à cinq noeuds ci-dessus, pouvez-vous configurer les poids des liens de sorte que les paquets envoyés par le routeur <span class="em">E</span>(resp. <span class="em">F</span>) suivent le chemin <span class="em">E&#8594;B&#8594;A</span> (resp. <span class="em">F&#8594;D&#8594;B&#8594;A</span>) ?</p>
                            </li>
                            <li>
                                <p>Dans les questions précédentes, vous avez travaillé sur l'état stable des tables de routage calculées par les protocoles de routage. Considérons maintenant les problèmes transitoires qui peuvent survenir lorsque la topologie du réseau change. Les principaux événements qui peuvent affecter la topologie d'un réseau sont les suivants :</p>
                                <ul>
                                    <li>
                                        <p>La défaillance d'un lien : Les mesures effectuées dans les réseaux IP ont montré que de telles défaillances se produisent fréquemment et généralement pour des périodes relativement courtes.</p>
                                    </li>
                                    <li>
                                        <p>L'ajout d'un lien dans le réseau : Cela peut être dû à la provision d'un nouveau lien ou plus fréquement parce que le lien a échoué il y a quelque temps et est maintenant de retour.</p>
                                    </li>
                                    <li>
                                        <p>La défaillance/crash d'un routeur suivie de son redémarrage.</p>
                                    </li>
                                    <li>
                                        <p>Un changement dans la métrique d'un lien en reconfigurant les routeurs attachés au lien. Voir <a href="http://totem.info.ucl.ac.be/lisis_tool/lisis-example/" target="_blank">http://totem.info.ucl.ac.be/lisis_tool/lisis-example/</a> pour une analyse des pannes à l'intérieur du réseau Abilene en juin 2005 ou <a href="http://citeseer.ist.psu.edu/old/markopoulou04characterization.html" target="_blank">http://citeseer.ist.psu.edu/old/markopoulou04characterization.html</a> pour une analyse des pannes affectant un plus grand réseau de ISP.</p>
                                    </li>
                                </ul>
                                <p>Pour cela, considérez la topologie du réseau montrée dans la figure ci-dessous et supposez que tous les routeurs utilisent un protocole de vecteur de distance qui utilise "Split Horizon".</p>
                                <figure>
                                    <img src="../images/reseau_simple_liens_redondants.png" alt="">
                                    <figcaption>Figure 5.77 : Réseau simple avec des liens redondants</figcaption>
                                </figure>
                                <p>Si vous calculez les tables de routage de tous les routeurs de ce réseau, vous obtiendrez une table telle que celle ci-dessous :</p>
                                <table class="tableBalises" role="presentation">
                                    <thead>
                                        <tr>
                                            <th>Destination</th>
                                            <th>Routes sur A</th>
                                            <th>Routes sur B</th>
                                            <th>Routes sur C</th>
                                            <th>Routes sur D</th>
                                            <th>Routes sur</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr>
                                            <td>A</td>
                                            <td>0</td>
                                            <td>1 via A</td>
                                            <td>2 via B</td>
                                            <td>3 via C</td>
                                            <td>4 via D</td>
                                        </tr>
                                        <tr>
                                            <td>B</td>
                                            <td>A via B</td>
                                            <td>0</td>
                                            <td>1 via B</td>
                                            <td>2 via C</td>
                                            <td>3 via D</td>
                                        </tr>
                                        <tr>
                                            <td>C</td>
                                            <td>2 via B</td>
                                            <td>1 via C</td>
                                            <td>0</td>
                                            <td>1 via C</td>
                                            <td>2 via D</td>
                                        </tr>
                                        <tr>
                                            <td>D</td>
                                            <td>3 via B</td>
                                            <td>2 via C</td>
                                            <td>1 via D</td>
                                            <td>0</td>
                                            <td>1 via D</td>
                                        </tr>
                                        <tr>
                                            <td>E</td>
                                            <td>4 via B</td>
                                            <td>3 via C</td>
                                            <td>2 via D</td>
                                            <td>1 via E</td>
                                            <td>0</td>
                                        </tr>
                                    </tbody>
                                </table>
                                <p>Les protocoles de vecteur de distance peuvent fonctionner selon deux modes différents : les <span class="html">mises à jour périodiques (periodic updates)</span> et les <span class="html">mises à jour déclenchées (triggered updates)</span>. Les <span class="em">mises à jour périodiques</span> sont le mode par défaut pour un protocole de vecteur de distance. Par exemple, chaque routeur pourrait diffuser son vecteur de distance toutes les trente secondes. Avec les <span class="em">mises à jour déclenchées</span>, un routeur envoie son vecteur de distance lorsqu'il y a des changements dans sa table de routage (et périodiquement s'il n'y a pas de changements).</p>
                                <ul>
                                    <li>
                                        <p>Considérons un protocole de routage par vecteur de distance utilisant la technique de split horizon et des <span class="em">mises à jour périodiques</span>. Supposons que le lien <span class="em">B-C</span> tombe en panne. <span class="em">B</span> et <span class="em">C</span> mettent à jour leur table de routage locale, mais ne l'annoncent qu'à la fin de leur période. Sélectionnez un ordre pour les <span class="em">mises à jour périodiques</span> et à chaque fois qu'un routeur envoie son vecteur de distance, indiquez le vecteur envoyé à chaque voisin et mettez à jour la table ci-dessus. Combien de période sont nécessaires pour permettre au réseau de converger vers un état stable ?</p>
                                    </li>
                                    <li>
                                        <p>Considérons le même protocole de routage par vecteur de distance, mais maintenant avec des <span class="em">mises à jour déclenchées</span>. Lorsque le lien <span class="em">B-C</span> tombe en panne, supposons que <span class="em">B</span> mette à jour immédiatement sa table de routage et envoie son vecteur de distance à <span class="em">A</span> et <span class="em">D</span>. Supposons que <span class="em">A</span> et <span class="em">D</span> traitent tous deux le vecteur de distance reçu et que <span class="em">A</span> envoie son propre vecteur de distance,... Indiquez tous les vecteurs de distance échangés et mettez à jour la table ci-dessus chaque fois qu'un vecteur de distance est envoyé par un routeur (et reçu par d'autres routeurs) jusqu'à ce que tous les routeurs aient appris une nouvelle route vers chaque destination. Combien de messages de vecteur de distance doivent être échangés jusqu'à ce que le réseau converge vers un état stable ?</p>
                                    </li>
                                </ul>
                            </li>
                            <li>
                                <p>Considérez le réseau ci-dessous. Dans ce réseau, la métrique de chaque lien est fixée à <span class="em">1</span>, sauf le lien <span class="em">A-B</span> dont la métrique est fixée à <span class="em">4</span> dans les deux directions. Dans ce réseau, il y a deux chemins avec le même coût entre <span class="em">D</span> et <span class="em">C</span>. Les anciens choisiraient au hasard l'un de ces chemins de coût égal et l'installerianet dans leur table de transfert. Les routeurs récents sont capables d'utiliser jusqu'à <span class="em">N</span> chemins de coût égal vers la même direction.</p>
                                <figure>
                                    <img src="../images/reseau_simple_fonctionnant_OSPF.png" alt="">
                                    <figcaption>Figure 5.78 : Un réseau simple exécutant OSPF</figcaption>
                                </figure>
                                <p>Sur les routeurs récents, une recherche dans la table de transfert pour une adresse de destination renvoie un ensmeble d'interfaces de sortie. Comment concevriez-vous un algorithme qui sélectionne l'interface de sortie utilisée pour chaque paquet, sachant que pour éviter le réordonnancement, tous les segments d'une m^me connexion TCP doivent suivre le même chemin ?</p>
                            </li>
                            <li>
                                <p>Considérons à nouveau le réseau présenté ci-dessus. Après un certain temps, OSPF converge et tous les routeurs calculent les tables de routage suivantes :</p>
                                <table class="tableBalises" role="presentation">
                                    <thead>
                                        <tr>
                                            <th>Destination</th>
                                            <th>Routes sur A</th>
                                            <th>Routes sur B</th>
                                            <th>Routes sur C</th>
                                            <th>Routes sur D</th>
                                            <th>Routes sur</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr>
                                            <td>A</td>
                                            <td>0</td>
                                            <td>2 via C</td>
                                            <td>1 via A</td>
                                            <td>3 via B, E</td>
                                            <td>2 via C</td>
                                        </tr>
                                        <tr>
                                            <td>B</td>
                                            <td>2 via C</td>
                                            <td>0</td>
                                            <td>1 via B</td>
                                            <td>1 via B</td>
                                            <td>2 via D, C</td>
                                        </tr>
                                        <tr>
                                            <td>C</td>
                                            <td>1 via C</td>
                                            <td>1 via C</td>
                                            <td>0</td>
                                            <td>2 via B, E</td>
                                            <td>1 via C</td>
                                        </tr>
                                        <tr>
                                            <td>D</td>
                                            <td>3 via C</td>
                                            <td>1 via D</td>
                                            <td>2 via B, E</td>
                                            <td>0</td>
                                            <td>1 via D</td>
                                        </tr>
                                        <tr>
                                            <td>E</td>
                                            <td>2 via C</td>
                                            <td>2 via C, D</td>
                                            <td>1 via E</td>
                                            <td>1 via E</td>
                                            <td>0</td>
                                        </tr>
                                    </tbody>
                                </table>
                                <p>Une différence importante entre OSPF et RIP est que les routeurs OSPF diffusent des LSP qui permettent aux autres routeurs de recalculer leurs propres tables de routage, tandis que les routeurs RIP échangent des vecteurs de distance. Supposons que le lien <span class="em">B-C</span> tombe en panne et que le routeur <span class="em">B</span> soit le premier à détecter la défaillance. À ce stade, <span class="em">B</span> ne peut plus atteindre <span class="em">A</span>, <span class="em">C</span> et 50% de ses chemins vers <span class="em">E</span> ont échoué. <span class="em">C</span> ne peut plus atteindre <span class="em">B</span> et la moitié de ses chemins vers <span class="em">D</span>ont échoué.</p>
                                <p>Le routeur <span class="em">B</span> diffusera son LSP mis à jour dans l'ensemble du réseau et tous les routeurs recalculeront leur table de transfert. À la réception d'un LSP, les routeurs inondent généralement le LSP reçu, puis recalculent leur table de transfert, suivi de <span class="em">D</span>, <span class="em">A</span>, <span class="em">C</span> et enfin <span class="em">E</span>.</p>
                            </li>
                            <li>
                                <p>Après chaque mise à jour d'une table de routage, vérifiez quelles paires de routeurs peuvent échanger des paquets. Fournissez votre réponse en utilisant un tableau similaire à celui illustré ci-dessus.</p>
                            </li>
                            <li>
                                <p>Pouvez-vous trouver un ordre de mise à jour des tables de routage qui évite tous les problèmes transitoires ?</p>
                            </li>
                            <li>
                                <p>Considérez le réseau simple représenté dans la figure ci-dessous. Sur chaque sous-réseau, chaque routeur a une adresse IP dont l'identificateur d'hôte est l'identifiant numérique du routeur (par exemple, le routeur <span class="em">R1</span> utilise les adresses <span class="em">10.1.0.1</span> et <span class="em">10.2.0.1</span>). Supposons que tous les hôtes et routeurs ont été démarrés et qu'aucun paquet n'a été envoyé. Expliquez en détail tous les paquets et trames échangés lorsque :</p>
                                <ol>
                                    <li>
                                        <p><span class="em">10.1.0.10</span> effectue un <span class="em">ping(8)</span> vers <span class="em">10.3.0.3</span>.</p>
                                    </li>
                                    <li>
                                        <p><span class="em">10.1.0.10</span> effectue un <span class="em">traceroute(8)</span> vers <span class="em">10.3.0.3</span>. Supposons que <span class="em">traceroute(8)</span> est utilisé avec l'option <span class="em">-n</span> afin qu'aucune recherche DNS inverse ne soit effectuée pour chaque adresse IP et que tous les appareils ont été redémarrés après la première question.</p>
                                    </li>
                                </ol>
                                <figure>
                                    <img src="../images/petit_reseau_utilisant_IPv4.png" alt="">
                                    <figcaption>Figure 5.79 : Un petit réseau utilisant IPv4</figcaption>
                                </figure>
                            </li>
                            <li>
                                <p>Considérez le réseau simple représenté dans la figure ci-dessous. Sur chaque sous-réseau, chaque routeur dispose d'une adresse IPv6 dont l'identificateur d'hôte est l'identifiant numérique du routeur. Supposons que tous les hôtes et routeurs ont été démarrés et qu'aucun paquet n'a été envoyé. Expliquez en détail tous les paquets et trames échangés lorsque :</p>
                                <ol>
                                    <li>
                                        <p><span class="em">2001:db8:cafe::abcd</span> effectue un <span class="em">ping6(8)</span> vers <span class="em">2001:db8:dead::3</span>.</p>
                                    </li>
                                    <li>
                                        <p><span class="em">2001:db8:cafe::abcd</span> effectue un <span class="em">traceroute6(8)</span> vers <span class="em">2001:db8:dead::3</span>. Supposons que <span class="em">traceroute(8)</span> est utilisé avec l'option <span class="em">-n</span> afin qu'aucune recherche DNS inverse ne soit effectuée pour chaque adresse IPv6 et que tous les appareils ont été redémarrés après la première question.</p>
                                    </li>
                                </ol>
                                <figure>
                                    <img src="../images/petit_reseau_utilisant_IPv6.png" alt="">
                                    <figcaption>Figure 5.80 : Un petit réseau utilisant IPv6</figcaption>
                                </figure>
                            </li>
                            <li>
                                <p>De nombreuses implémentations TCP/IP envoient aujourd'hui une requête ARP pour leur propre adresse IP avant d'envoyer leur premier paquet IP en utilisant cette adresse. Pouvez-vous expliquer pourquoi cela est utile en pratique ?</p>
                            </li>
                            <li>
                                <p>Considérons maintenant la transmission de paquets IPv4. Un problème de sécurité avec IPv4 est le protocole de résolution d'adresse (ARP). Pouvez-vous expliquer ce que sont l'usurpation d'ARP (ARP spooding) ou l'empoisonnement d'ARP (ARP poisoning) et montrer comment l'hôte <span class="em">A</span> dans le réseau ci-dessous pourrait intercepter tous les paquets IP envoyés par l'hôte <span class="em">B</span> via le routeur par défaut ? Le routeur <span class="em">R</span> peut-il faire quelque chose pour améliorer la sécurité de l'ARP ?</p>
                                <figure>
                                    <img src="../images/petit_reseau_ethernet.png" alt="">
                                    <figcaption>Figure 5.81 : Un petit réseau Ethernet</figcaption>
                                </figure>
                            </li>
                            <li>
                                <p>Considérons le réseau illustré dans la figure ci-dessous qui utilise uniquement des routes statiques comme indiqué dans la figure. En supposant qu'il n'y a que des liens point-à-point, montrez tous les paquets échangés dans le réseau lorsque <span class="em">S</span> (<span class="em">1.0.0.1/8</span>) effectue un <span class="em">traceroute(8)</span> vers <span class="em">2.0.0.2</span>. <span class="em">S</span> (resp. <span class="em">D</span>) utilise <span class="em">A</span> (resp. <span class="em">E</span>) comme routeur par défaut. Fournissez votre réponse dans un tableau tel que celui illustré ci-dessous où chaque ligne correspond à un paquet IP.</p>
                                <table class="tableBalises" role="presentation">
                                    <thead>
                                        <tr>
                                            <th>Lien</th>
                                            <th>IP source</th>
                                            <th>IP destination</th>
                                            <th>Explication</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr>
                                            <td>...</td>
                                            <td>...</td>
                                            <td>...</td>
                                            <td>...</td>
                                        </tr>
                                    </tbody>
                                </table>
                                <figure>
                                    <img src="../images/petit_reseau.jpg" alt="">
                                    <figcaption>Figure 5.82 : Un petit réseau</figcaption>
                                </figure>
                            </li>
                            <li>
                                <p>Les adresses IPv4 étant rares, les opérateurs de réseau doivent souvent minimiser le nombre d'adresses IPv4 utilisées lorsqu'ils déploient un réseau. Vous êtes responsable du réseau d'entreprise illustré ci-dessous et devez utiliser le préfixe IP <span class="em">172.16.12.128/25</span> pour toutes les adresses IP (hôtes et routeurs) de ce réseau. Comment attribuez-vous des préfixes IP aux différents sous-réseaux pour maximiser le nombre d'adresses disponibles pour les hôtes des réseaux de gauche et de droite ? En tant que contraintre pratique, notez qu'à l'interieur de chaque sous-réseau, les adresses IPv4 dont les bits d'identifcateur d'hôte sont tous définis à <span class="em">0</span> ou tous définis à <span class="em">1</span> ne peuvent pas être utilisées. En pratique, l'adresse où tous les bits d'identificateur d'hôte sont définis à <span class="em">0</span> est utilisée pour représenter le sous-réseau lui-même, tandis que l'adresse où tous les bits d'identificateur d'hôte sont définis à <span class="em">1</span> est réservée pour des raisons de compatibilité ascendante pour l'adresse de diffusion du sous-réseau. Cela implique que sur un lien point-à-point, vous ne pouvez attribuer qu'un préfixe <span class="em">/30</span> et non un préfixe <span class="em">/31</span> bien qu'il n'y ait généralement que deux adresses IPv4 en cours d'utilisation sur un tel lien. Voir RFC 3021 pour une discussion de l'utilisation des préfixes IPv4 <span class="em">/30</span> sur les liens point-à-point.</p>
                                <figure>
                                    <img src="../images/reseau_entreprise_simple.jpg" alt="">
                                    <figcaption>Figure 5.83 : Un réseau simple d'entreprise</figcaption>
                                </figure>
                            </li>
                            <li>
                                <p>Il arrive parfois que les hôtes soient mal configurés sur un sous-réseau. Considérez le réseau illustré ci-dessous où tous les hôtes ont été configurés manuellement. Discutez de l'hôte qui est capable d'envoyer un paquet à quel hôte.</p>
                                <figure>
                                    <img src="../images/hotes_malconfigures_sous-reseau.png" alt="">
                                    <figcaption>Figure 5.83 : Hôtes mal configurés sur un sous-réseau</figcaption>
                                </figure>
                            </li>
                            <li>
                                <p>Les vendeurs expliquent souvent qu'un NAT est équivalent à un pare-feu car il protège les hôtes qui résident derrière le NAT. Quel est votre avis technique sur cette affirmation</p>
                            </li>
                            <li>
                                <p>il existe deux principaux types de NAT. Les NAT les plus simples utilisent une seule adresse IP publique et peuvent desservir de nombreux hôtes utilisant des adresses privées. Les NAT de qualité entreprise et de qualité opérateur utilisent souvent un préfixe IPv4 et peuvent desservir une entreprise entière en utilisant des adresses privées. Comparez ces deux types de NAT.</p>
                            </li>
                            <li>
                                <p>Un étudiant a installé un routeur NAT à la maison et souhaite configurer un serveur web sur son ordinateur portable. Que doit-il faire pour s'assurer que son serveur web est accessible depuis Internet ?</p>
                            </li>
                            <li>
                                <p>Les NAT traduisent les numéros de port et les adresses IP. Parfois, les paquets IPv4 sont fragmentés. Discutez de la manière dont un NAT doit traiter les fragments de paquets IPv4 ? Supposons que seul TCP soit utilisé via le NAT.</p>
                            </li>
                            <li>
                                <p>Même question que ci-dessus pour un pare-feu.</p>
                            </li>
                            <li>
                                <p>Supposons que vous utilisez un ordinateur portable avec une adresse IPv4 privée derrière un NAT pour naviguer sur le Web. Pour réduire les coûts, l'implémenteur de votre NAT a choisi de supprimer tous les messages ICMP envoyés par votre ordinateur portable et tous les ICMP reçus de l'Internet. Quelles pourraient être les conséquences de ce NAT réduit ?</p>
                            </li>
                            <li>
                                <p>Considérez le réseau illustré dans la figure ci-dessous et expliquez le chemin suivi par les paquets pour atteindre <span class="em">194.100.10.0/23</span>.</p>
                                <figure>
                                    <img src="../images/reseau_entreprise_avec_serveurs.png" alt="">
                                    <figcaption>Figure 5.85 : Un réseau d'entreprise avec de nombreux serveurs</figcaption>
                                </figure>
                            </li>
                            <li>
                                <p>Considérons maintenant, comme indiqué dans la figure ci-dessous, que le stub AS est également connecté au fournisseur <span class="em">AS789</span>. Par quel fournisseur les paquets destinés à <span class="em">194.100.10.0/23</span> seront-ils reçus par <span class="em">AS4567</span> ? <span class="em">AS123</span> doit-il changer sa configuration ?</p>
                                <figure>
                                    <img src="../images/stub_connecte_fournisseur.png" alt="">
                                    <figcaption>Figure 5.86 : Un stub connecté à un fournisseur</figcaption>
                                </figure>
                            </li>
                            <li>
                                <p>Considéréez que le stib illsutré dans la figure ci-dessous décide de diffuser deux préfixes <span class="em">/24</span> au lieu de son préfixe alloué <span class="em">/23</span>.</p>
                                <figure>
                                    <img src="../images/stub_connecte_2fournisseurs.png" alt="">
                                    <figcaption>Figure 5.87 : Un stub connecté à deux fournisseurs</figcaption>
                                </figure>
                                <ol>
                                    <li>
                                        <p>Par quel fournisseur <span class="em">AS4567</span> reçoit-il les paquets destinés à <span class="em">194.100.11.99</span> et <span class="Em">194.100.10.1</span> ?</p>
                                    </li>
                                    <li>
                                        <p>Comment la disponibilité de ces adresses est-elle affectée lorsque le lien <span class="em">R1-R3</span> tombe en panne ?</p>
                                    </li>
                                    <li>
                                        <p>Proposez une configuration sur <span class="em">R1</span> qui atteint le même objectif que celui illustré dans la figure, mais qui préserve également la disponibilité de toutes les adresses IP à l'intérieur d'<span class="em">AS4567</span> si l'un des liens interdomaine d'<span class="em">AS4567</span> échoue ?</p>
                                    </li>
                                </ol>
                            </li>
                            <li>
                                <p>Considérez le réseau présenté dans la figure ci-dessous. Dans ce réseau, chaque AS contient un seul routeur BGP. Supposons que <span class="em">R1</span> annonce un seul préfixe. <span class="em">R1</span> reçoit beaucoup de paquets de la part de <span class="em">R9</span>. Sans aucune aide de la part de <span class="em">R2</span>, <span class="em">R9</span> ou <span class="em">R4</span>, comment <span class="em">R1</span> pourrait-il configurer son annonce BGP pour recevoir les paquets de <span class="em">R9</span> via <span class="em">R3</span> ? Que se passe-t-il lorsqu'un lien échoue ?</p>
                                <figure>
                                    <img src="../images/stub_connecte_2fournisseurs.png" alt="">
                                    <figcaption>Figure 5.88 : Un stub connecté à deux fournisseurs</figcaption>
                                </figure>
                            </li>
                            <li>
                                <p>Considérez le réseau présenté dans la figure ci-dessous.</p>
                                <figure>
                                    <img src="../images/inter-reseau_simple.PNG" alt="">
                                    <figcaption>Figure 5.89 : Un inter-réseau simple</figcaption>
                                </figure>
                                <figure>
                                    <img src="../images/inter-reseau_simple_2.png" alt="">
                                    <figcaption>Figure 5.90 : Un inter-réseau simple</figcaption>
                                </figure>
                                <ol>
                                    <li>
                                        <p>Montrez quels messages BGP sont échangés lorsque le routeur <span class="em">R1</span> annonce le préfixe <span class="em">10.0.0.0/8</span>.</p>
                                    </li>
                                    <li>
                                        <p>Combien et quelles sont les routes connues par le routeur <span class="em">R5</span> ? Quelle route annonce-t-il à <span class="em">R6</span> ?</p>
                                    </li>
                                    <li>
                                        <p>Supposons maintenant que le lien entre <span class="em">R1</span> et <span class="em">R2</span> échoue. Montrez les messages échangés suite à cet événement. Quels messages BGP sont envoyés à <span class="em">R6</span> ?</p>
                                    </li>
                                </ol>
                            </li>
                            <li>
                                <p>Considérez le réseau présenté dans la figure ci-dessous où <span class="em">R1</span> annonce un seul préfixe. Dans ce réseau, le lien entre <span class="em">R1</span> et <span class="em">R2</span> est considéré comme un lien de secours. Il ne devrait être utilisé que lorsque le lien principal (<span class="em">R1-R4</span>) échoue. Cela peut être implémenté sur <span class="em">R2</span> en définissant une faible préférence locale pour les routes re!ues sur le lien <span class="em">R2-R1</span>.</p>
                                <figure>
                                    <img src="../images/inter-reseau_simple_avec_lien_sauvegarde.png" alt="">
                                    <figcaption>Figure 5.91 : Un inter-réseau simple avec un lien de sauvegarde</figcaption>
                                </figure>
                                <ol>
                                    <li>
                                        <p>Dans ce réseau, quels sont les chemins utilisés par tous les routeurs pour atteindre <span class="em">R1</span> ?</p>
                                    </li>
                                    <li>
                                        <p>Supposons maintenant que le lien <span class="em">R1-R4</span> échoue. Quels messages BGP sont échangés et quels sont les chemins utilisés pour atteindre <span class="em">R1</span> ?</p>
                                    </li>
                                    <li>
                                        <p>Le lien <span class="em">R1-R4</span> revient. Quels messages BGP sont échangés et quels sont les chemins utilisés pour atteindre <span class="em">R1</span> ?</p>
                                    </li>
                                </ol>
                            </li>
                            <li>
                                <p>Le 22 février 2008, l'Autorité des télécommunications du Pakistan a ordonné aux ISP pakistanais de bloquer l'accès à trois adresses IP appartenant à YouTube : <span class="em">208.65.153.238</span>, <span class="em">208.65.153.253</span>, <span class="em">208.65.153.251</span>. Un opérateur a noté que ces adresses appartenaient au même préfixe <span class="em">/24</span>. Lire <a href="http://www.ripe.net/news/studyyoutube-hijacking.html" target="_blank">http://www.ripe.net/news/studyyoutube-hijacking.html</a> pour comprendre ce qui s'est réellement passé.</p>
                                <ol>
                                    <li>
                                        <p>Que devait faire YouTube pour éviter ce problème ?</p>
                                    </li>
                                    <li>
                                        <p>Quels types de solutions proposeriez-vous pour améliorer la sécurité du routage interdomaine ?</p>
                                    </li>
                                </ol>
                            </li>
                            <li>
                                <p>Il existe actuellement 13 adresses IPv4 associées aux serveurs racines du système de noms de domaine. Cependant, <a href="http://www.root-servers.org/" target="_blank">http://www.root-servers.org/</a> indique qu'il y a plus de 100 serveurs physiques différents qui les prennent en charge. Il s'agit d'un grand service anycast. Comment configureriez-vous les routeurs BGP pour fournir un tel service anycast ?</p>
                            </li>
                            <li>
                                <p>Considérez le réseau représenté dans la figure ci-dessous. Dans ce réseau, <span class="em">R0</span> annonce le préfixe <span class="Em">p</span> et toutes les métriques de lien sont fixées à <span class="em">1</span>.</p>
                                <ul>
                                    <li>
                                        <p>Dessinez les sessions iBGP et eBGP.</p>
                                    </li>
                                    <li>
                                        <p>Supposons que la session <span class="em">R0-R8</span> est en panne lorsque <span class="em">R0</span> annonce <span class="em">p</span> via <span class="em">R0-R7</span>. Quels sont les messages BGP échangés et les routes choisies par chaque routeur dans le réseau ?</p>
                                    </li>
                                    <li>
                                        <p>La session <span class="em">R0-R8</span> est établie et <span class="em">R0</span> annonce également le préfixe <span class="em">p</span> via cette session.</p>
                                    </li>
                                    <li>
                                        <p>Les routes sélectionnées par chaque routeur changent-elles si l'attribut <span class="em">MED</span> est utilisé sur les sessions <span class="em">R7-R6</span> et <span class="em">R3-R10</span>, mais pas sur les sessions <span class="em">R4-R9</span> et <span class="em">R6-R8</span> ?</p>
                                    </li>
                                    <li>
                                        <p>Est-il possible de configurer les routeurs dans le réseau <span class="em">R1-R6</span> de manière à ce que <span class="em">R4</span> atteigne le préfixe <span class="em">p</span> via <span class="em">R6-R8</span> tandis que <span class="em">R2</span> utilise le lien <span class="em">R3-R10</span> ?</p>
                                    </li>
                                </ul>
                                <figure>
                                    <img src="../images/internet_simple.jpg" alt="">
                                    <figcaption>Figure 5.92 : Un Internet simple</figcaption>
                                </figure>
                            </li>
                            <li>
                                <p>L'attribut BGP <span class="em">MED</span> est souvent défini sur le coût IGP pour atteindre le nexthop BGP du préfixe annoncé. Cependant, les routeurs peuvent également être configurés pour utiliser toujours les mêmes valeurs <span class="em">MED</span> pour toutes les routes annoncées sur une session donnée. Comment l'utiliseriez-vous dans la figure ci-dessus pour que le lien <span class="em">R10-R3</span> soit le lien principal tandis que le lien <span class="em">R7-R6</span> soit un lien de secours ? Y a-t-il un avantage ou un inconvénient à utiliser l'attribut <span class="em">MED</span> pour cette application par rapport à la préférence locale (<span class="em">local-pref</span>) ?</p>
                            </li>
                            <li>
                                <p>Dans la figure ci-dessus, supposons que les responsables de <span class="em">R8</span> et <span class="em">R9</span> souhaitent utiliser le lien <span class="em">R8-R6</span> comme lien de secours, mais que les responsables de <span class="em">R4</span> et <span class="em">R6</span> ne sont pas d'accord pour utiliser l'attribut BGP <span class="em">MED</span> ni pour utiliser une préférence locale différente pour les routes apprises de <span class="em">R8</span> et <span class="em">R9</span>. Existe-t-il une alternative à l'utilisation de <span class="em">MED</span> ?</p>
                            </li>
                        </ol>
                        <h5>Pratique :</h5>
                        <ol>
                            <li>
                                <p>Voici les plus petites et les plus grandes adresses IPv4 dans les sous-réseaux suivants :</p>
                                <ul>
                                    <li>
                                        <p class="em">8.0.0.9/8</p>
                                    </li>
                                    <li>
                                        <p class="em">172.12.0.0/16</p>
                                    </li>
                                    <li>
                                        <p class="em">200.123.42.128/25</p>
                                    </li>
                                    <li>
                                        <p class="em">12.1.2.0/13</p>
                                    </li>
                                </ul>
                            </li>
                            <li>
                                <p>Pour les sous-réseaux IPv6 suivants, indiquez la plus petite et la plus grande adresse IPv6 à l'intérieur du sous-réseau :</p>
                                <ul>
                                    <li>
                                        <p class="em">FE80::/64</p>
                                    </li>
                                    <li>
                                        <p class="em">2001:db8::/48</p>
                                    </li>
                                    <li>
                                        <p class="em">2001:6a8:3080::/48</p>
                                    </li>
                                </ul>
                            </li>
                            <li>
                                <p>Les chercheurs et les opérateurs de réseau collectent et exposent beaucoup de données BGP. Pour cela, ils établissent des sessions eBGP entre des routeurs de collecte de données et des routeurs de production situés dans des réseaux opérationnels. Plusieurs routeurs de collecte de données sont disponibles, les plus populaires étant :</p>
                                <ul>
                                    <li>
                                        <p><a href="http://www.routeviews.org/" target="_blank">http://www.routeviews.org/</a></p>
                                    </li>
                                    <li>
                                        <p><a href="http://www.ripe.net/ris" target="_blank">http://www.ripe.net/ris</a></p>
                                    </li>
                                </ul>
                                <p>Pour cet exercice, vous utiliserez l'un des routeurs BGP de <span class="html">routeviews</span>. Vous pouvez accéder à l'un de ces routeurs en utilisant <span class="em">telnet</span>. Une fois connecté au routeur, vous pouvez utiliser l'interface de ligne de commande du routeur pour analyser sa table de routage BGP.</p>
                                <figure>
                                    <img src="../images/routeviews.png" alt="">
                                </figure>
                                <p>Ce routeur a des sessions eBGP avec des routeurs de plusieurs ISP. Consultez <a href="http://www.routeviews.org/peers/route-views.oregon-ix.net.txt" target="_blank">http://www.routeviews.org/peers/route-views.oregon-ix.net.txt</a> pour une liste à jour de toutes les sessions eBGP maintenues par ce routeur.</p>
                                <p>Parmi toutes les commandes prises en charge par ce routeur, la commande <span class="html">show ip bgp</span> est très utile. Cette commande prend un préfixe IPv4 en paramètre et vous permet de récupérer toutes les routes que ce routeur a reçues dans son <span class="em">Adj-RIB-In</span> pour le préfixe spécifié.</p>
                                <ol>
                                    <li>
                                        <p>Utilisez la commande <span class="html">show ip bgp 130.104.0.0/16</span> pour trouver le meilleur chemin utilisé par ce routeur pour atteindre UCLouain.</p>
                                    </li>
                                    <li>
                                        <p>Sachant que <span class="em">130.104.0.0/16</span> est annoncé par belnet (AS2611), quels sont, selon ces tables de routage BGP, les AS qui sont en peering avec belnet ?</p>
                                    </li>
                                    <li>
                                        <p>Faites la même analyse pour l'un des préfixes IPv4 assignés à Skynet (AS5432) : <span class="em">62.4.128.0/17</span>. La sortie de la commande <span class="html">show ip bgp 624128.0/17</span> révèle quelque chose d'étrange car il semble que l'un des chemins vers ce préfixe passe deux fois par AS5432. Pouvez-vous expliquer cela ?</p>
                                    </li>
                                </ol>
                                <figure>
                                    <img src="../images/show_ip_bgp.png" alt="">
                                </figure>
                            </li>
                            <li>
                                <p><span class="html">Netkit</span> permet de réaliser facilement des expériences en utilisant un environnement émulé composé de machines virtuelles exécutant User Model Linux. <span class="em">Netkit</span> permet de configurer un petit réseau en laboratoire comme si vous aviez accès à plusieurs ordinateurs interconnectés par des câbles et des équipements réseau.</p>
                                <p>Un laboratoire <span class="em">Netkit</span> est défini comme quelques fichiers de configuration et scripts :</p>
                                <p><span class="html">lab.conf</span> est un fichier texte qui définit les machines virtuelles et la topologie du réseau. Un fichier <span class="em">lab.conf</span> simple est présenté ci-dessous.</p>
<pre><code>LAB_DESCRIPTION="a string describing the lab"
LAB_VERSION=1.0
LAB_AUTHOR="the author of the lab"
LAB_EMAIL="email address of the author"

h1[0]="lan"
h2[0]="lan"</code></pre>
                                <dl>
                                    <dt>
                                        <p>Ce fichier de configuration demande la création de deux machines virtuelles nommées <span class="em">h1</span> et <span class="em">h2</span>. Chacune de ces machines a un réseau.</p>
                                    </dt>
                                    <dd>
                                        <p>Un fichier <span class="html">host.startup</span> pour chaque machine (<span class="em">h1.startup</span> et <span class="em">H2.startup</span> dans l'exemple ci-dessus). Ce fichier est un script shell qui est exécuté à la fin du démarrage de la machien virtuelle. C'est généralement dans ce script que les interfaces réseau sont configurées et que les démons sont lancés. Un répertoire pour chaque machine (<span class="em">h1</span> et <span class="em">h2</span> dans l'exemple ci-dessus). Ce répertoire est utilisé pour stocker des fichiers de configuration qui doivent être copiés sur les systèmes de fichiers des machines virtuelles lorsqu'elles sont créées pour la première fois.</p>
                                    </dd>
                                </dl>
                                <p><span class="em">Netkit</span> contient plusieurs scripts qui peuvent être utilisés poyr exécuter un labo. <span class="html">lstart</span> permet de lancer un labo et <span class="html">lhalt</span> permet de stopper les machines à la fin du labo. Si vous devez échanger des fichiers entre les machines virtuelles et l'hôte Linux sur lequel <span class="em">Netkit</span> s'exécute, notez que les hôtes virtuels montent le répertoire contenant le labo en cours d'exécution dans <span class="html">/hostlab</span> et votre répertoire personnel dans <span class="html">/hostname</span>.</p>
                                <p>Pour cet exercice, vous utiliserez un labo <span class="em">Netkit</span> contenant 4 hôtes et deux routeurs. Les fichiers de configuration sont disponibles dans <span class="em">exercises/labs/lab-2routers.tar.gz</span>. La topologie de ce labo est présentée dans la figure ci-dessous.</p>
                                <figure>
                                    <img src="../images/laboratoire_2routeurs.png" alt="">
                                    <gigcaption>5.93 : Le laboratoire à deux routeurs</gigcaption>
                                </figure>
                                <p>Le fichier <span class="em">lab.conf</span> de ce labo est montré ci-dessous.</p>
<pre><code>h1[0]="lan1"
h2[0]="lan1"
h3[0]="lan2"
router1[0]="lan1"
router1[1]="lan2"
router2[0]="lan2"
router2[1]="lan3"
h4[0]="lan3"</code></pre>
                                <p>Sur ce réseau, nous utiliserons le sous-réseau <span class="em">172.12.1.0/24</span> pour <span class="em">lan1</span>, <span class="em">172.12.2.0/24</span> pour <span class="em">lan2</span> et <span class="em">172.12.3.0/24</span> pour <span class="em">lan3</span>.</p>
                                <p>Sur Linux, les adresses IP assignées à une interface peut être configurées en utilisant <span class="html">ifconfig(8)</span>.</p>
                                <p>Lorsque <span class="em">ifconfig(8)</span> est utilisé sans paramètres, il répertorie toutes les interfaces existantes de l'hôte avec leur configuration. Un exemple de sortie <span class="em">ifconfig(8)</span> est présenté ci-dessous.</p>
                                <figure>
                                    <img src="../images/ifconfig8.png" alt="">
                                </figure>
                                <p>Ce hôte a deux interfaces : l'interface de loopback (<span class="html">lo</span> avec l'adresse IPv4 <span class="em">127.0.0.1</span> et l'adresse IPv6 <span class="em">::1</span>) et l'interface <span class="html">eth0</span>. L'adresse <span class="em">192.168.1.1/24</span> et une adresse IPv6 locale de liaison (<span class="em">fe80::fc3a:59ff:fecd:59ad/64</span>) ont été attribuées à l'interface <span class="em">eth0</span>. L'adresse de diffusion est utilisée dans certains cas particuliers, ce qui est hors de protée de cet exercice. <span class="em">ifconfig(8)</span> fournit également des statistiques telles que le nombre de paquets envoyés et reçus sur cette interface. Une autre information importante fournie par <span class="em">ifconfig(8)</span> est l'adresse matérielle (HWaddr) utilisée par la couche liaison de données de l'interface. Dans l'exemple ci-desus, l'interface <span class="em">eth0</span> et utilise l'adresse matérielle <span class="em">FE:3A:59:CD:59:AD</span> sur 48 bits.</p>
                                <p>Vous pouvez configurer l'adresse IPv4 assignée à une interface en spécifiant l'adresse et le masque de sous-réseau.</p>
<pre><code>ifconfig eth0 192.168.1.2 netmask 255.255.255.128 up</code></pre>
                                <p>Vous pouvez aussi spécifier la longueur du préfixe :</p>
<pre><code><span class="em">.. code-block:: text</span>

    ifconfig eth0 192.168.1.2/25 up</code></pre>
                                <p>Dans les deux cas, <span class="html">ifconfig eth0</span> vous permet de vérifier que l'interface a été correctement configurée.</p>
                                <figure>
                                    <img src="../images/ifconfig_eth0.png" alt="">
                                </figure>
                                <p>Une autre commande importante sur Linux est <span class="html">route(8)</span> qui permet de voir le contenu de la table de routage stockée dans le noyau Linux et de la modifier. Par exemple, <span class="html">route -n</span> renvoie le contenu de la table de routage IPv4. Voir <span class="em">route(8)</span> pour une description détaillée de la façon dont vous pouvez configurer les routes en utilisant cet outil.</p>
                                <ol>
                                    <li>
                                        <p>Utilisez <span class="em">ifconfig(8)</span> pour configurer les adresses IPv4 suivantes :</p>
                                        <ul>
                                            <li>
                                                <p><span class="em">172.16.1.11/4</span> sur l'interface <span class="em">eth0</span> sur <span class="em">h1</span></p>
                                            </li>
                                            <li>
                                                <p><span class="em">172.16.1.12/4</span> sur l'interface <span class="em">eth0</span> sur <span class="em">h2</span></p>
                                            </li>
                                        </ul>
                                    </li>
                                    <li>
                                        <p>Utilisez <span class="em">route -n</span> pour regarder le contenu de la table de routage sur les deux hôtes.</p>
                                    </li>
                                    <li>
                                        <p>Vérifiez en utilisant <span class="em">ping(8)</span> que <span class="em">h1</span> peut atteindre <span class="em">172.17.1.12</span>.</p>
                                    </li>
                                    <li>
                                        <p>Utilisez <span class="em">ifconfig(8)</span> pour configurer l'adresse IPv4 <span class="em">172.17.1.1/24</span> sur l'interface <span class="em">eth0</span> de <span class="em">router1</span> et <span class="em">172.16.2.1/24</span> sur l'interface <span class="em">eth1</span> de ce routeur.</p>
                                    </li>
                                    <li>
                                        <p>Étant donné que les hôtes <span class="em">h1</span> et <span class="em">h2</span> sont connectés à un réseau local qui ne contient qu'un seul routeur, ce dernier peut agir en tant que routeur par défaut. Ajoutez une route par défaut sur <span class="em">h1</span> et <span class="em">H2</span> pour qu'ils puissent utiliser <span class="em">router1</span> comme routeur par défaut pour atteindre toute adresse IPv4 distante. Vérifiez en utilisant <span class="em">ping(8)</span> que <span class="em">h1</span> peut atteindre l'adresse <span class="em">172.16.2.1</span>.</p>
                                    </li>
                                    <li>
                                        <p>Que devez-vous configurer sur <span class="em">router2</span>, <span class="em">h3</span> et <span class="em">h4</span> pour que tous les hôtes et routeurs puissent atteindre tous les hôtes et routeurs du réseau émulé ? Ajoutez les commandes <span class="em">ifconfig</span> et <span class="em">route</span> dans les fichiers <span class="em">.startup</span> de tous les hôtes de manière à ce que le réseau soit correctemnt configuré lorsqu'il est démarré à l'aide de <span class="em">lstart</span>.</p>
                                    </li>
                                </ol>
                            </li>
                            <li>
                                <p>Utilisez le réseau configuré ci-dessus pour tester la fragmentation des paquets IP. La commande <span class="em">ifconfig</span> vous permet de spécifier l'Unité de Transmission Maximale (MTU), c'est-à-dire la taille maximale des trames autorisées sur une interface donnée. La MTU par défaut sur les interfaces <span class="em">eth?</span> est de 1500 octets.</p>
                                <ol>
                                    <li>
                                        <p>Forcez une MTU de 500 octets sur les trois interfaces connectés à <span class="em">lan2</span>.</p>
                                    </li>
                                    <li>
                                        <p>Utilisez la commande <span class="html">ping -s 1000</span> pour envoyer un paquet ping de 1000 octets depuis <span class="em">h3</span> vers l'un des routeurs connectés à <span class="em">lan2</span> et capturez les paquets sur l'autre routeur en utilisant <span class="html">tcpdump(8)</span>. Dans quel ordre l'hôte émulé envoie-t-il les fragments IP ?</p>
                                    </li>
                                    <li>
                                        <p>Utilisez la commande <span class="html">ping -s 2000</span> pour envoyer un paquet ping de 2000 =octets depuis <span class="em">h1</span> vers <span class="em">h4</span> et capturez les paquets sur <span class="em">lan2</span> et <span class="em">lan3</span> en utilisant <span class="html">tcpdump(8)</span>. Dans quel ordre l'hôte émulé envoie-t-il les fragments IP ?</p>
                                    </li>
                                    <li>
                                        <p>À partir de vos mesures, comment un hôte émulé génère-t-il les identifiants des paquets IP qu'il envoie ?</p>
                                    </li>
                                    <li>
                                        <p>Réinitialisez la MTU sur l'interface <span class="em">eth1</span> du routeur <span class="em">r1</span> à 1500 octets, mais laissez la MTU  sur l'interface <span class="em">eth0</span> du routeur <span class="em">r2</span> à 500 octets. Vérifiez si l'hôte <span class="em">h1</span> peut envoyer un ping à l'hôte <span class="em">h4</span>. Utilisez <span class="html">tcpdump(8)</span> pour analyser ce qui se passe.</p>
                                    </li>
                                    <li>
                                        <p>Le protocole d'information de routage (RIP) est un protocole de vecteur de distance souvent utilisé dans les petits réseaux IP. Pour cet exercice, vous utiliserez <span class="html">quagga</span>, une implémentation open-source de plusieurs protocoles de routage IP qui s'exécute sur Linux et d'autres systèmes d'exploitation compatibles Unix. <span class="html">quagga(8)</span> est en fait un ensemble de démons qui interagissent entre eux et avec la table de routage du noyau Linux. Pour cet exercice, vous utiliserez deux de ces démons : <span class="html">zebra(8)</span> et <span class="html">ripd(8)</span>. <span class="em">zebra(8)</span> est le démon principal qui gère les interactions entre la table de routage du noyau Linux et les protocoles de routage. <span class="em">ripd(8)</span> est l'implémentation du protocole RIP. Il interagit avec les tables de routage Linux via le démon <span class="em">zebra(8)</span>.</p>
                                        <p>Pour utiliser une machine réelle ou virtuelle Linux en tant que routeur, vous devez d'abord configurer les adresses IP des interfaces de la machine. Une fois cette configuration vérifiée, vous pouvez configurer les démons <span class="em">zebra(8)</span> et <span class="em">ripd(8)</span>. Les fichiers de configuration de ces démons se trouvent dans <span class="em">/etc/zebra</span>. Le premier fichier de configuration est <span class="em">/etc/zebra/daemons</span>. Il répertorie les démons qui sont lancés lorsque <span class="em">zebra</span> est démarré par <span class="em">/etc/init.d/zebra</span>. Pour activer <span class="em">ripd(8)</span> et <span class="em">zebra(8)</span>, ce fichier sera configuré comme suit.</p>
<pre><code># This file tells the zebra package
# which daemons to start.
# Entries are in the format: <daemon>=(yes|no|priority)
# where 'yes' is equivalent to infinitely low priority, and
# lower numbers mean higher priority. Read
# /usr/doc/zebra/README.Debian for details.
# Daemons are: bgpd zebra ospfd ospf6d ripd ripngd
zebra=yes
bgpd=no
ospfd=yes
ospf6d=no
ripd=no
ripngd=no</code></pre>
                                        <p>Le deuxième fichier de configuration est le fichier <span class="em">/etc/zebra/zebra.conf</span>. Il définit les règles de configuration globales qui s'appliquent à <span class="em">zebra(8)</span>. Pour cet exercice, nous utilisons le fichier de configuration par défaut illusrré ci-dessous.</p>
<pre><code>! -*- zebra -*-
!
! zebra configuration file
!
hostname zebra
password zebra
enable password zebra
!
! Static default route sample.
!
!ip route 0.0.0.0/0 203.181.89.241
!
log file /var/log/zebra/zebra.log</code></pre> 
                                        <p>Dans le fichier de configuration zebra, les lignes commençant par <span class="em">!</span> sont des commentaires. Cette configuration définit le nom d'hôte en tant que <span class="em">zebra</span> et deux mots de passe. Le mot de passe par défaut (<span class="html">password zebra</span>) doit être fourni lors de la connexion à la console de gestion <span class="em">zebra(8)</span> via une connexion TCP. Cette console de gestion peut être utilisée comme un shell sur un hôte Unix pour spécifier des commandes aux démons <span class="em">zebra(8)</span>. Le deuxième mot de passe (<span class="html">enable password zebra</span>) spécifie le mot de passe à fournir avant de donner des commandes qui modifient la configuration du démon. Il est également possible de spécifier des routes statistiques dans ce fichier de configuration, mais nous n'utilisons pas cette fonctionnalité dans cet exercice. Le dernier paramètre spécifié est le fichier journal où <span class="em">zebra(8)</span> écrit les informations de débogage. Des informations supplémentaires sur <span class="em">quagga</span> sont disponibles sur <a href="http://www.quagga.net/docs/docs-info.php" target="_blank">http://www.quagga.net/docs/docs-info.php</a>.</p>
                                        <p>Le fichier de configuration le plus intéressant pour cet exercice est le fichier <span class="em">/etc/zebra/ripd.conf</span>. Il contient tous les paramètre spécifiques au fonctionnement du protocole RIP. Un exemple de fichier de configuration <span class="em">ripd(8)</span> est présenté ci-dessus.</p>
<pre><code>!
hostname ripd
password zebra
enable password zebra
!
router rip
network 100.1.0.0/16
redistribute connected
!
log file /var/log/zebra/ripd.log</code></pre>
                                        <p>Ces fichiers de configuration montrent les deux différentes façons de configurer <span class="em">ripd(8)</span>. L'instruction <span class="html">router rip</span> indique le début de la configuration du protcole de routage RIP. Les lignes indentées qui suivent font partie de la configuration de ce protocole. La première ligne, <span class="em">network 100.1.0.0/16</span>, est utilisée pour activer RIP sur l'interface dont le sous-réseau OP corrspond à <span class="em">100.1.0.0/16</span>. La deuxième ligne, <span class="em">redistribute connected</span>, indique que tous les sous-réseaux directement connectés au routeur doivent être annoncés. Lorsque cette ligne de configuration est utilisée, <span class="em">ripd(8)</span> interagit avec la table de routage du noyau Linux et annonc tous les sous-réseaux directement connects au routeur. Si une nouvelle interface est activée et configurée sur le routeur, son préfixe de sous-réseau sera automatiquement annoncé. De même, le préfixe de sous-réseau sera automatiquement supprimé si l'interface du sous-réseau est désactivée.</p>
                                        <p>Pour expérimenter avec RIP, vous utiliserez les routeurs émulés illustrés dans la figure ci-dessous. Vous pouvez télécharger l'ensemble du laboratoire à partir du fichier <span class="em">exercises/labs/lab-5routers-rip.tar.gz</span>.</p>
                                        <figure>
                                            <img src="../images/laboratoire_5routeurs.png" alt="">
                                            <figcaption>Figure 5.94 : Le laboratoire à cinq routeurs</figcaption>
                                        </figure>
                                        <p>Le fichier <span class="em">lab.conf</span> décrivant la topologie et les interfaces utilisées sur tous les hôtes est présenté ci-dessous.</p>
<pre><code>r1[0]="A"
r1[1]="B"
r1[2]="F"
r1[3]="V"
r2[0]="A"
r2[1]="C"
r2[2]="W"
r3[0]="B"
r3[1]="C"
r3[2]="D"
r3[3]="X"
r4[0]="D"
r4[1]="E"
r4[2]="Y"
r5[0]="E"
r5[1]="F"
r5[2]="Z"</code></pre>
                                        <p>Il existe deux types de sous-réseaux dans cette topologie. Les sous-réseaux du préfixe <span class="em">172.16.0.0/16</span> sont utilisés sur les liaisons entre les routeurs, tandis que les sous-réseaux du préfixe <span class="em">192.168.0.0/16</span> sont utilisés sur les réseaux locaux qui sont connectés à un seul routeur.</p>
                                        <p>Un routeur peut être configuré de deux manières différentes : en spécifiant des fichiers de configuration ou en tapant les commandes directment sur le routeur en utilisant <span class="html">telnet(1)</span>. Les quatre premiers routeurs ont été configurés dans les fichiers de configuration fournis. Consultez le fichier <span class="em">r1.startup</span> et les fichiers de configuration dans le répertoire <span class="em">r1/tmp/zebra</span> du laboratoire pour le routeur <span class="em">r1</span>. Les fichiers <span class="em">r?.startup</span> contiennent les commandes <span class="em">ifconfig(8)</span> qui sont utilisées pour configurer les interfaces de chaque routeur virtuel. Les fichiers de configuration situés dans <span class="em">r?/tmp/zebra</span> sont également copiés automatiquement sur le routeur virtuel de son démarrage.</p>
                                        <ol>
                                            <li>
                                                <p>Lancez le laboratoire en utilisant la commande <span class="em">lstart</span> et vérifiez si le routeur <span class="em">r1</span> peut atteindre les adresses IP <span class="em">192.168.2.2</span>, <span class="em">192.168.3.3</span> et <span class="em">192.168.4.4</span>. Vous pouvez également utiliser la commande <span class="html">traceroute(8)</span> pour déterminer le chemin emprunté par vos paquets.</p>
                                            </li>
                                            <li>
                                                <p>Le démon <span class="em">ripd(8)</span> peut également être configuré en tapant des commandes via une connexion TCP. <span class="em">ripd(8)</span> écoute sur le port <span class="em">2602</span>. Sur le routeur <span class="em">r1</span>, utilisez la commande <span class="html">telnet 127.0.0.1 2602</span> pour vous connecter au démon <span class="em">ripd(8)</span>. Le mot de passe par défault est <span class="html">zebra</span>. Une fois connecté au démon <span class="em">ripd(8)</span>, vous accédez à l'invite <span class="em">&gt;</span> où vous pouvez interroger l'état du routeur. En tapant <span class="html">?</span> à l'invite, vous obtiendrez la liste des commandes prises en charge. La commande <span class="html">show</span> est particulièrement utile. Tapez <span class="html">show ?</span> pour obtenir la liste de ses sous-options. Par exemple, la commande <span class="html">show ip rip</span> affichera la table de routage maintenue par le démon <span class="em">ripd(8)</span>.</p>
                                            </li>
                                            <li>
                                                <p>Désactivez l'interface <span class="em">eth3</span> sur le routeur <span class="em">r1</span> en tapant la commande <span class="html">ifconfig eth3 down</span> sur ce routeur. Vérifiez l'impact de cette commande sur les tables de routage des autres routeurs du réseau. Réactivez cette interface en tapant la commande <span class="html">ifconfig eth3 up</span>.</p>
                                            </li>
                                            <li>
                                                <p>Faites de même avec l'interface <span class="em">eth1</span> sur le routeur <span class="em">r3</span>.</p>
                                            </li>
                                            <li>
                                                <p>Modifiez le fichier de configuration <span class="em">/etc/zebra/ripd.conf</span> sur le routeur <span class="em">r5</span> pour que ce routeur fasse partie du réseau. Vérifiez que <span class="em">192.168.5.5</span> est accessible depuis tous les routeurs à l'intérieur du réseau.</p>
                                            </li>
                                        </ol>
                                    </li>
                                    <li>
                                        <p>L'Open Shortest Path First Protocol (OSPF) est un protocole de type d'état de lien souvent utilisé dans les réseaux IP d'entreprise. OSPF est implémenté dans le démon <span class="html">ospfd(8)</span> qui fait partie de <span class="em">quagga</span>. Nous utilsons la même topologie que dans l'exercice précédent. Le laboratoire <span class="em">Netkit</span> peut être téléchargé à partir de <span class="em">exercises/labs/lab-5routers-ospf.tar.gz</span>.</p>
                                        <p>Le démon <span class="em">ospfd(8)</span> prend en charge une configuration plus complexe que le démon <span class="em">ripd(8)</span>. Un exemple de configuration est présenté ci-dessous.</p>
<pre><code>!
hostname ospfd
password zebra
enable password zebra
!
interface eth0
ip ospf cost 1
interface eth1
ip ospf cost 1
interface eth2
ip ospf cost 1
interface eth3
ip ospf cost 1
!
router ospf
router-id 192.168.1.1
network 172.16.1.0/24 area 0.0.0.0
network 172.16.2.0/24 area 0.0.0.0
network 172.16.3.0/24 area 0.0.0.0
network 192.168.1.0/24 area 0.0.0.0
passive-interface eth3
!
log file /var/log/zebra/ospfd.log</code></pre>
                                        <p>Dans ce fichier de configuration, la commande <span class="html">ip ospf cost 1</span> spécifie une métrique de <span class="em">1</span> pour chaque interface. La configuration de <span class="em">ospfd(8)</span> est composée de trois parties. Tout d'abord, chaque routeur doit avoir un identifiant unique à l'intérieur du réseau. Habituellement, cet identifiant est l'une des adresses IP attribuées au routeur. Ensuite, chaque sous-réseau sur le routeur est associé à une zone. Dans cet exemple, nous utilisons uniquement la zone principale (c'est-à-dire <span class="em">0.0.0.0</span>). La dernière commande spécifie que les messages Hello OSPF ne doivent pas être envoyés via l'interface <span class="em">eth3</span>, bien que son sous-réseau soit annoncé par le routeur. Une telle commande est souvent utilisée sur des interfaces qui sont attachées à des hôtes finaux pour éviter tout problème si un étudiant configure un routeur OSPF logiciel sur son ordinateur portable connecté à cette interface.</p>
                                        <p>Le laboratoire <span class="em">Netkit contient déjà la configuration des routeurs </span><span class="em">r1</span> à <span class="em">r4</span>.</p>
                                        <p>Le démon <span class="em">ospfd(8)</span> écoute sur le port TCP <span class="em">2604</span>. Vous pouvez suivre l'évolution du protocole OSPF en utilisant les commandes <span class="html">show ip ospf ?</span></p>
                                        <ol>
                                            <li>
                                                <p>Lancez le laboratoire en utilisant la commande <span class="em">lstart</span> et vérfiez que les adresses <span class="em">192.168.1.1</span>, <span class="em">192.168.2.2</span>, <span class="em">192.168.3.3</span> et <span class="em">192.168.4.4</span> sont accessibles depuis n'importe quel routeur à l'intérieur du réseau.</p>
                                            </li>
                                            <li>
                                                <p>Configurez le routeur <span class="em">r5</span> en modifiant le fichier <span class="em">/etc/zebra/ospfd.conf</span> et redémarrez le démon. Vérifiez que l'adresse <span class="em">192.168.5.5</span> est accessible depuis n'importe quel routeur à l'intérieur du réseau.</p>
                                            </li>
                                            <li>
                                                <p>Comment pouvez-vous mettre à jour la configuration réseau pour que les paquets envoyés par le routeur <span class="em">r1</span> vers le routeur <span class="em">r5</span> utilisent la liaison directe entre les deux routeurs tandis que les paquets envoyés par <span class="em">r5</span> sont acheminés via <span class="em">r4</span> ?</p>
                                            </li>
                                            <li>
                                                <p>Désactivez l'interface <span class="em">eth3</span> sur le routeur <span class="em">r1</span> et observez la rapidité avec laquelle le réseau converge. Vous pouvez suivre l'évolution de la table de routage sur un routeur en tapant <span class="html">netstat -rnc</span>. Réactivez l'interface <span class="em">eth3</span> sur le routeur <span class="em">r1</span>.</p>
                                            </li>
                                            <li>
                                                <p>Modifiez le MTU de l'interface span.em{eth0} sur le routeur <span class="em">r1</span>, mais laissez-le inchangé sur l'interface <span class="em">eth0</span> du routeur <span class="em">r2</span>. Quel est l'impact de cette modification ? Pouvez-vous l'expliquer ?</p>
                                            </li>
                                            <li>
                                                <p>Désactivez l'interface <span class="em">eth1</span> sur le routeur <span class="em">r3</span> et observez la rapidité avec laquelle le réseau converge. Réactivez cette interface.</p>
                                            </li>
                                            <li>
                                                <p>Arrêtez le routeur <span class="em">r2</span> en utilisant la commande <span class="html">vcrash r2</span>. À quelle vitesse le réseau réagit-il à cette défaillance ?</p>
                                            </li>
                                        </ol>
                                    </li>
                                </ol>
                            </li>
                        </ol>
                </article>
                <article>
                    <h2 id="couche_liaison_donnees_LAN">Partie 6 : La couche de liaison de données et les LAN :</h2>
                    <h3>6.1 La couche de liaison de données et les LAN :</h3>
                    <p>La couche de liaison de données est la plus basse couche du modèle de référence que nous étudions en détail. Comme mentionné précédemment, il existe deux types de couches de liaison de données. Les premières couches de liaison de données qui sont apparues sont celles utilisées sur des liens point-à-point entre des appareils connectés directement par une liaison physique. Nous discuterons brièvement de l'une de ces couches de liaison de données dans ce chapitre. Le deuxième type de couches de liaison de données est celui utilisé dans les réseaux locaux (LAN). La principale différence entre les couches de liaison de données point-à-point et les couches de liaison de données LAN est que ces dernières doivent réguler l'accès au réseau local, qui est généralement un support partagé.</p>
                    <p>Ce chapitre est organisé comme suit. Nous commenàons par discuter des principes de la couche de liaison de données ainsi que des services qu'elle utilise de la couche physique. Nous décrivons ensuite plus en détail plusieurs algorithmes de contrôle d'accès au support utilisés dans les réseaux locaux pour réguler l'accès au support partgé. Enfin, nous examinons en détail les technologies importantes de la couche de liaison de données, en mettant l'accent sur les réseaux Ethernet et WiFi.</p>
                    <h4>5.1.1 Principes :</h4>
                    <p>La couche de liaison de données utilise le service fourni par la couche physique. Bien qu'il existe de nombreuses implémentations différentes de la couche physique d'un point de vue technologique, elles fournissent toutes un service qui permet à la couche de liaison de données d'envoyer et de recevoir des bits entre des appareils directement connectés. La couche de liaison de données reçoit des paquets de la couche réseau. Deux entités de la couche de liaison de données échangent des <span class="html">trames (frames en anglais)</span>. Comme expliqué dans le chapitre précédent, la plupart des technologies de la couche de liaison de données imposent des limitations sur la taille des trames. Certaines technologies imposent uniquement une taille maximale de trame, d'autres imposent à la des fois des tailles minimales et maximales de trame, et enfin, certaines technologies ne prennent en charge qu'une seule taille de trame. Dans ce dernier cas, la couche de liaison de données inclura généralement une sous-couche d'adaptation pour permettre à la couche réseau d'envoyer et de recevoir des paquets de longueur variable. Cette couche d'adaptation peut inclure des mécanismes de fragmentation et de réassemblage.</p>
                    <figure>
                        <img src="../images/couche_liaison_donnees_modele_reference.png" alt="">
                        <figcaption>Figure 6.1 : La couche de liaison de données et le modèle de référence</figcaption>
                    </figure>
                    <p>Le service de la couche physique facilite l'envoi et la réception des bits. De plus, il est généralement loin d'être parfait, comme expliqué dans l'introduction :</p>
                    <ul>
                        <li>
                            <p>La couche physique peut subir des changements, par exemple en raison d'interférences électromagnétiques, affectant ainsi la valeur d'un bit transmis.</p>
                        </li>
                        <li>
                            <p>La couche physique peut livrer <span class="em">plus</span> de bits au récepteur que ceux envoyés par l'émetteur.</p>
                        </li>
                        <li>
                            <p>La couche physique peut livrer <span class="em">moins</span> de bits au récepteur que ceux envoyés par l'émetteur.</p>
                        </li>
                    </ul>
                    <p>La couche de liaison de données doit permettre aux systèmes finaux d'échanger des trames contenant des paquets malgré toutes ces limitations. Sur les liens point-à-point et les LAN, le premier problème à résoudre est comment coder une trame en tant que séquence de bits, de sorte que le récepteur puisse facilement récupérer la trame re!ue malgré les limitations de la couche physique.</p>
                    <p>Si la couche physique était parfaite, le problème serait très simple. La couche de liaison de données aurait simplement besoin de définir comment encoder chaque trame en tant que séquence de bits consécutifs. Le récepteur pourrait alors facilement extraire les trames des bits reçus. Malheureusement, les imperfections de la couche physique rendent ce problème de découpage légèrement plus complexe. Plusieurs solutions ont été proposées et sont utilisées en pratique dans différentes technologies de la couche de liaison de données.</p>
                    <h5>Le découpage en trames :</h5>
                    <p>Ceci est le problème du <span class="html">découpage en trames</span>. On peut le définir ainsi : "<span class="em">Comment un émetteur encode-t-il des trames de sorte que le récepteur puisse les extraire efficacement du flux de bits qu'il reçoit de la couche physique</span>".</p>
                    <p>Une première solution pour résoudre le problème de découpage en trames consiste à demander à la couche physique de rester inactive pendant un certain temps après la transmission de chaque trame. Ces périodes d'inactivité peuvent ^^etre détectées par le récepteur et servir de repère pour délimiter les limites des trames. Malheureusement, cette solution n'est pas suffisante pour deux riasons. Premièrement, certaines couches physiques ne peuvent pas rester inactives et doivent toujours transmettre des bits. Deuxièmement, l'insertion d'une période d'inactivité entre les trames réduit la bande passante maximale pouvant être atteinte par la couche de liaison de données.</p>
                    <p>Certaines couches physiques proposent une alternative à cette période d'inactivité. Toutes les couches physiques sont capables d'envoyer et de recevoir des symboles physiques qui représentent les valeurs <span class="em">0</span> et <span class="em">1</span>. Cpendant, pour diverses raisons qui dépassent le cadre de ce chapitre, plusieurs couches physiques sont capables d'échanger d'autres symboles physiques. Par exemple, le codage Manchester utilisé dans plusieurs couches physiques peut envoyer quatre symboles différents. Le codage Manchester est un schéma de codage différentiel dans lequel le temps est divisé en périodes de longueur fixe. Chaque période est divisée en deux moitiés et deux niveaux de tension différents peuvent être appliqués. Pour envoyer un symbole, l'émetteur doit définir l'un de ces deux niveaux de tension pendant chaque moitié de période. Pour envoyer un <span class="em">1</span> (resp. <span class="em">0</span>, l'émetteur doit définir une tension élevée (resp. faible) pendant la première moitié de la période et une tensions faible (resp. élevée) pendant la deuxième moitié. Ce codage garantit qu'il y aura une transition au milieu de chaque période et permet au récepteur de synchroniser son horloge sur celle de l'émetteur. En dehors des codages pour <span class="em">0</span> et <span class="em">1</span>, le codage Manchester prend également en charge deux symboles supplémentaires : <span class="html">InvH</span> et <span class="html">InvB</span>, où le même niveau de tension est utilisé pour les deux demi-périodes. Par définition, ces deux symboles ne peuvent pas apparaître à l'intérieur d'une trame qui est composée uniquement de <span class="em">0</span> et de <span class="em">1</span>. Certaines technologies utilisent ces symboles spéciaux comme marqueurs pour le début ou la fin des trames.</p>
                    <figure>
                        <img src="../images/codage_Manchester.png" alt="">
                        <figcaption>Figure 6.2 : Codage Manchester</figcaption>
                    </figure>
                    <p>Malheureusement, les encodages à plusieurs symboles ne peuvent pas être utilisés par tous les couches physiques et une solution générique qui peut être utilisée avec n'importe quelle cocuhe physique capable de transmettre et de recevoir uniquement des <span class="em">0</span> et des <span class="em">1</span> est nécessaire. Cette solution générique s'appelle le "<span class="html">stuffing (remplissage)</span>" et deux variantes existent : le <span class="html">bit stuffing (remplissage de bits)</span> et le <span class="html">character stuffing (remplissage de caractères)</span>. Pour permettre à un récepteur de délimiter facilement les limites des trames, ces deux techniques réservent des chaînes de bits spéciales en tant que marqueurs de délimitation de trame et encodent les trames de manière à ce que ces chaînes de bits spéciales n'apparaissent pas à l'intérieur des trames.</p>
                    <p>Le <span class="em">bit stuffing</span> réserve la séquence de bits <span class="em">01111110</span> comme marqueur de délimitation de trame et garantit qu'ik n'y aura jamais six symboles <span class="em">1</span> consécutifs transmis par la couche physique à l'intérieur d'une trame. Avec le <span class="em">bit stuffing</span>, une trame est envoyée de la manière suivante : tout d'abord, l'émetteur transmet le marqueur, c'est-à-dire <span class="em">01111110</span>. Ensuite, il envoie tous les bits de la trame et insère un bit supplémentaire à <span class="em">0</span> après chaque séquence de cinq bits consécutifs à <span class="em">1</span>. Cela garantit que la trame envoyée ne contient jamais une séquence de six bits consécutifs à <span class="em">1</span>. Par conséquent, le motif du marqueur ne peut pas apparaître à l'intérieur de la trame envoyée. Le marqueur est également envoyé pour marquer la fin de la trame. Le récepteur effectue l'opération inverse pour décoder une trame reçue. Il détecte d'abord le début de la trame grâce au marqueur <span class="em">01111110</span>. Ensuite, il traite les bits reçus et compye le nombre de bits consécutifs à <span class="em">1</span>. Si un <span class="em">0</span> suit cinq bits consécutifs à <span class="em">1</span>, ce bit est supprimé car il a été inséré par l'émetteur. Si un <span class="em">1</span> suit cinq bits bits consécutifs à <span class="em">1</span>, cela indique un marqueur s'il est suivi d'un bit à <span class="em">0</span>. Le tableau ci-dessous illustre l'application du <span class="em">bit stuffing</span> à quelques trames.</p>
                    <table class="tableBalises" role="presentation">
                        <thead>
                            <tr>
                                <th>Trame originale</th>
                                <th>Trame transmise</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>0001001001001001001000011</td>
                                <td>01111110000100100100100100100001101111110</td>
                            </tr>
                            <tr>
                                <td>0110111111111111111110010</td>
                                <td>01111110011011111011111011111011001001111110</td>
                            </tr>
                            <tr>
                                <td>01111110</td>
                                <td>0111111001111101001111110</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>Par exemple, considérons la transmission de <span class="em">0110111111111111111110010</span>. L'émetteur commencera par envoyer le marqueur <span class="em">01111110</span>, suivi de <span class="em"> 011011111</span>. Après ces cinq bits consécutifs à <span class="em">1</span>, il insère un bit à <span class="em">0</span>suivi de <span class="em">11111</span>. Un nouveau <span class="em">0</span> est inséré, suivi de <span class="em">11111</span>. Un nouvzau <span class="em">0</span> est inséré, suivi de la fin de la fin de la trame <span class="em">110010</span> et du marqueur <span class="em">01111110</span>.</p>
                    <p>Le <span class="em">bit stuffing</span> augmente le nombre de bits nécessaires pour transmettre chaque trame. Le pire cas pour le <span class="em">bit stuffing</span> est bien sûr une longue séquence de bits à <span class="em">1</span> à l'intérieur de la trame. En cas d'erreurs de transmission, les bits ou marqueurs insérés peuvent être erronés. Dans ces cas, la trame affectée par l'erreur, et éventuellement la trame suivante, ne seront pas correctement décodées par le récepteur, mais il sera capable de se resynchroniser au prochain marqueur valide.</p>
                    <p>Le <span class="em">bit stuffing</span> peut être facilement implémenté en matériel. Cependant, son implémentation en logiciel est difficile en raison de la surcharge plus élevée des manipulations de bits en logiciel. Les implémentations logicielles préfèrent traiter les caractères plutôt que les bits, c'est pourquoi les couches de liaison de données basées sur des logiciels utilisent généralement le <span class="em">character stufffing</span>. Cette technique opère sur des trames contenant un nombre entier de caractères de 8 bits. Certains caractères sont utilisés comme marqueurs pour délimiter les limites de la trame. De nombreuses techniques de <span class="em">character stuffing</span> les caractères <span class="html">DLE</span><span class="html">STX</span> et <span class="html">ETX</span> de l'ensemble de caractères ASCII. <span class="html">DLE STX</span> (resp. <span class="html">DLE ETX</span>) est utilisé pour marquer le début (la fin) d'une trame. Lors de la transmission d'une trame, l'émetteur ajoute un caractère <span class="html">DLE</span> après chaque caractère <span class="html">DLE</span> transmis. Cela garantit que aucun des marqueurs ne peut apparaître à l'intérieur de la trame transmise. Le récepteur détecte les limites de la trame et supprime le deuxième <span class="html">DLE</span> lorsqu'il reçoit deux caractères <span class="html">DLE</span> consécutifs. Par exemple, pour transmettre la trame <span class="em">1 2 3 DLE STX 4</span>, un émetteur enverra d'abord <span class="html">DLE STX</span> en tant que marqueur, suivi de <span class="Em">1 2 3 DLE</span>. Ensuite, l'émetteur transmet un caractère <span class="html">DLE</span> supplémentaire suivi de <span class="em">STX 4</span> et du marqueur <span class="html">DLE ETX</span>.</p>
                    <table class="tableBalises" role="presentation">
                        <thead>
                            <tr>
                                <th>Trame originale</th>
                                <th>Trame transmise</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>1 2 3 4</td>
                                <td>DLE STX 1 2 3 4 DLE ETX</td>
                            </tr>
                            <tr>
                                <td>1 2 3 DLE STX 4</td>
                                <td>DLE STX 1 2 3 DLE DLE STX 4 DLE ETX</td>
                            </tr>
                            <tr>
                                <td>DLE STX DLE ETX</td>
                                <td>DLE STX DLE DLE STX DLE DLE ETX DLE ETX</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>Le <span class="em">character stuffing</span>, tout comme le <span class="em">bit stuffing</span>, augmente la longuer des trames transmises. Pour le <span class="em">character stuffing</span>, la pire trame est une trame contenant de nombreux caractères <span class="html">DLE</span>. Lorsque des erreurs de transmission se produisent, le récepteur peut décoder de manière incorrecte une ou deux trames (par exemple, si les erreurs se produisent dans les marqueurs). Cependant, il sera capable de se resynchroniser avec les marqueurs correctement reçus suivants.</p>
                    <h5>Détection d'erreurs :</h5>
                    <p>En plus du framing (délimitation des trames), les couches de liaison de données incluent également des mécanismes pour détecter et parfois même récupérer des erreurs de transmission. Pour permettre à un récepteur de détecter les erreurs de transmission, l'émetteur doit ajouter des informations redondantes sous la forme d'un code de <span class="em">détection d'erreur</span> à la trame transmise. Ce code de <span class="em">détection d'erreur</span> est calculé par l'émetteur sur la trame qu'il transmet. Lorsque le récepteur reçoit une trame avec un code de <span class="em">détection d'erreur</span>, il le recalcul et vérifie le code de <span class="em">détection d'erreur</span> reçu corrspond au code de <span class="em">détection d'erreur</span> calculé. S'ils corrspondent, la trame est considérée comme valide. De nombreux schémas de <span class="em">détection d'erreurs</span> existent et des livres entiers ont été écrits sur le sujet. Une discussion détaillée de ces techniques dépasse le cadre de ce livre, et nous ne discuterons que de quelques exemples pour illustrer les principes clés.</p>
                    <p>Pour comprendre les codes de <span class="em">détection d'erreurs</span>, considérons deux dispositifs qui échangent des chaînes de bits contenant <span class="em">N</span> bits. Pour permettre au récepteur de détecter une erreur de transmission, l'émetteur convertit chaque chaîne de <span class="em">N</span> en une chaîne de <span class="em">N+r</span> bits. Généralement, les <span class="em">r</span> bits redondants sont ajoutés au début ou à la fin de la chaîne de bits transmise, mais certaines techniques entrelacent les bits redondants avec les bots d'origine. Un code de <span class="em">détection d'erreurs</span> peut être défini comme une fonction qui calcule les <span class="em">r</span> bits redondants correspondant à chaque chaîne de <span class="em">N</span> bits. Le code de <span class="em">détection d'erreurs</span> le plus simple est le <span class="html">bit de parité</span>. Il existe deux types de schémas de parité : la <span class="html">parité paire</span> et la <span class="html">parité impaire</span>. Avec le schéma de <span class="em">parité paire</span> (resp. <span class="em">impaire</span>), le bit redondant est choisi de telle sorte qu'un nombre pair (resp. impair) de bits soit défini à <span class="em">1</span> dans la chaîne de bits transmise de <span class="em">N+r</span> bits. Le récepteur peut facilement recalculer la parité de chaque chaîne de bits reçue et rejeter les chaînes avec une parité invalide. Le schéma de parité est souvent utilisé lors de l'échange de caractères de 7 bits. Dans ce cas, le huitième bit est souvent un <span class="html">bit de parité</span>. Le tableau ci-dessous montre les bits de parité qui sont calculés pour les chaînes de bits contenant trois bits.</p>
                    <table class="tableBalises" role="presentation">
                        <thead>
                            <tr>
                                <th>Chaîne de 3 bits</th>
                                <th>Parité impaire</th>
                                <th>Parité paire</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>000</td>
                                <td>1</td>
                                <td>0</td>
                            </tr>
                            <tr>
                                <td>001</td>
                                <td>0</td>
                                <td>1</td>
                            </tr>
                            <tr>
                                <td>010</td>
                                <td>0</td>
                                <td>1</td>
                            </tr>
                            <tr>
                                <td>100</td>
                                <td>0</td>
                                <td>1</td>
                            </tr>
                            <tr>
                                <td>111</td>
                                <td>0</td>
                                <td>1</td>
                            </tr>
                            <tr>
                                <td>110</td>
                                <td>1</td>
                                <td>0</td>
                            </tr>
                            <tr>
                                <td>101</td>
                                <td>1</td>
                                <td>0</td>
                            </tr>
                            <tr>
                                <td>011</td>
                                <td>1</td>
                                <td>0</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>Le bit de parité permet au récepteur de détecter les erreurs de transmission qui ont affecté un seul bit parmi les <span class="em">N+r</span> bits transmis. Si deux bits ou plus sont erronés, le récepteur peut ne pas nécessairement être en mesure de détecter l'erreur de transmission. Des schémas de détection d'erreurs plus puissants ont été définis. Les contrôles de redondance cyclique (CRC) sont largement utilisés dans les protocoles de la couche de liaison de données. Un CRC de <span class="em">N</span> bits peut détecter toutes les erreurs de transmission affectant une séquence de moins de <span class="em">N</span> bits dans la trame transmise et toutes les erreurs de transmission affectant un nombre impair de bits. Des détails supplémentaires sur les CRC peuvent être trouvés dans [Williams1993].</p>
                    <p>Il est également possible de concevoir un code qui permet au récepteur de corriger les erreurs de transmission. Le code de correction d'erreurs le plus simple est la <span class="html">redondance triple modulaire (TMR pour Triple Modular Redundancy)</span>. Pour transmettre un bit défini à <span class="em">1</span> (resp. <span class="em">0</span>), l'émetteur transmet <span class="em">111</span> (resp. <span class="em">000</span>). Lorsqu'il n'y a pas d'erreurs de transmission, le récepteur peut décoder <span class="em">111</span> comme <span class="em">1</span>. Si des erreurs de transmission ont affecté un seul bit, le récepteur effectue un vote majoritaire comme indiqué dans le tableau ci-dessous. Ce schéma permet au récepteur de corriger toutes les erreurs de transmission affectant un seul bit.</p>
                    <table class="tableBalises" role="presentation">
                        <thead>
                            <tr>
                                <th>Bits reçus</th>
                                <th>Bit décodé</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>000</td>
                                <td>0</td>
                            </tr>
                            <tr>
                                <td>001</td>
                                <td>0</td>
                            </tr>
                            <tr>
                                <td>010</td>
                                <td>0</td>
                            </tr>
                            <tr>
                                <td>100</td>
                                <td>0</td>
                            </tr>
                            <tr>
                                <td>111</td>
                                <td>1</td>
                            </tr>
                            <tr>
                                <td>110</td>
                                <td>1</td>
                            </tr>
                            <tr>
                                <td>101</td>
                                <td>1</td>
                            </tr>
                            <tr>
                                <td>011</td>
                                <td>1</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>D'autres codes de correction d'erreurs plus puissants ont été proposés et sont utilisés dans certaines applications. Le <span class="html">code de Hamming</span> est une combinaison astucieuse de bits de parité qui offre des capacités de détection et de correction d'erreurs. En pratique, les protocoles de la couche de liaison de données combinent le <span class="em">bit stuffing</span> ou le <span class="em">character stuffing</span> avec une indication de longueur dans l'en-tête de la trame et un checksum ou un CRC. Le checksum/CRC est calculé par l'émetteur et placé dans la trame avant d'appliquer le <span class="em">bit stuffing</span> ou le <span class="em">character stuffing</span>.</p>
                    <h4>6.1.2 Contrôle d'accès au support (Medium Access Control) :</h4>
                    <p>Les couches de liaison de données point-à-point doivent sélectionner l'une des techniques de cadrage décrites ci-dessus et éventuellement ajouter des algorithmes de retransmission tels que ceux expliqués pour la couche transport afin de fournir un service fiable. Les couches de liaison de données pour les réseaux locaux font face à deux problèmes supplémentaires. Un réseau local est composé de plusieurs hôtes qui sont connectés au même support physique partagé. Du point de vue de la couche physique, un réseau local peut être organisé de quatre manières différentes :</p>
                    <ul>
                        <li>
                            <p>Un réseau en forme de bus où tous les hôtes sont connectés au même câble physique.</p>
                        </li>
                        <li>
                            <p>Un réseau en forme d'anneau où tous les hôtes sont connectés à un noeud amont et à un noeud aval de sorte que l'ensemble du réseau forme un anneau.</p>
                        </li>
                        <li>
                            <p>Un réseau en forme d'étoile où tous les hôtes sont connectés au même périphérique.</p>
                        </li>
                        <li>
                            <p>Un réseau sans fil où tous les hôtes peuvent envoyer et recevoir des trames à l'aide de signaux radio.</p>
                        </li>
                    </ul>
                    <p>Ces quatre organisations physiques de base des réseaux locaux sont représentées graphiquement dans la figure ci-dessous. Nous nous concentrerons d'abord sur une organisation physique à la fois.</p>
                    <figure>
                        <img src="../images/LAN_bus_anneau_etoile.png" alt="">
                        <figcaption>Figure 6.3 : Réseau local en forme de bus, d'anneau et d'étoile</figcaption>
                    </figure>
                    <p>Le problème commun à toutes ces organisations de réseau est de partager efficacement l'accès au réseau local. Si deux dispositifs envoient une trame en même temps, les deux signaux électriques, optiques ou radio correspondant à ces trames apparaîtront en même temps sur le support de transmission et un récepteur ne pourra pas décoder l'une ou l'autre trame. De telles transmissions simultanées sont appelés <span class="html">collisions</span>. Une collision peut impliquer des trames transmises par deux ou plusieurs dispositifs connectés au réseau local. Les collisions sont la principale cause d'erreurs dans les réseaux locaux câblés.</p>
                    <p>Toutes les technologies de réseau local reposent sur un algorithme de <span class="em">contrôle d'accès au support (MAC pour Medium Access Control)</span> pour réguler afin de minimiser ou d'éviter les collisions. Il existe deux grandes familles d'algorithmes de <span class="em">contrôle d'accès au support</span> :</p>
                    <ol>
                        <li>
                            <p>Les algorithmes MAC <span class="html">déterministes</span> ou <span class="html">pessimistes</span> : Ces algorithmes partent du principe que les collisions sont un problème très grave et qu'elles doivent être totalement évitées. Ces algorithmes garantissent qu'à tout moment, au plus un dispositif est autorisé à envoyer une trame sur le réseau local. Cela est généralement réalisé en utilisant un protocole distribué qui sélectionne un dispositif autorisé à transmettre à chaque instant. Un algorithme MAC déterministe garantit l'absence de collision, mais il y a une certaine surcharge liée à la régulation des transmissions de tous les dispositifs connectés au réseau local.</p>
                        </li>
                        <li>
                            <p>Les algorithmes MAC <span class="html">stochastiques</span> ou <span class="html">optimistes</span> : Ces algorithmes considèrent que les collisions font partie du fonctionnement normal d'un réseau local. Ils visent à minimiser le nombre de collisions, mais ils n'essaient pas d'éviter toutes les collisions. Les algorithmes <span class="Em">stochastiques</span> sont généralement plus faciles à implémenter que les algorithmes <span class="em">déterministes</span>.</p>
                        </li>
                    </ol>
                    <p>Nous commencerons par discuter d'un algorithme MAC déterministe simple, puis nous décrirons plusieurs algorithmes optimistes importants avant de revenir à un algorithme MAC distribué et déterministe.</p>
                    <h5>Méthodes d'allocation statique :</h5>
                    <p>Une première solution pour partager les ressources disponibles entre tous les dispositifs connectés à un réseau local consiste à définir, à priori, la répartition des ressources de transmission parmi les différents dispositifs. Si <span class="em">N</span> dispositifs doivent partager les capacités de transmission d'un réseau local fonctionnant à <span class="em">b</span> Mbps, chaque dispositif pourrait se voir attribuer une bande passante de <span class="em">b/N</span> Mbps.</p>
                    <p>Les ressources limitées doivent être partagées dans d'autres environnements que les réseaux locaux. Depuis les première transmissions radio par <span class="em">Marconi</span> il y a plus d'un siècle, de nombreuses applications qui échangent des informations par le biais de signaux radio ont été développées. Chaque signe radio est une onde électromagnétique dont la puissance est centrée autour d'une fréquence donnée. Le spectre radio correspond aux fréquences comprises entre environ 2 kHz et 300 GHz. Des plans d'allocation de fréquences négociés entre les gouvernements réservent la plupart des gammes de fréquences à des applications spécifiques telles que la radio-diffusion, la télévision, les communications mobiles, la navigation radio aéronautique, la radio amateur, les satellites, etc. Chaque plage de fréquences est ensuite subdivisée en canaux et chaque canal peut être réservé pour une application donnée.</p>
                    <p>La <span class="html">multiplexion par répartition en fréquence (FDM, pour Frequency Division Multiplexing)</span> est une méthode d'allocation statique dans laquelle une fréquence est attribuée à chaque appareil au support partagé. Comme chaque appareil utilise une fréquence de transmission différente, les coliisions ne peuvent pas se produire. Dans les réseaux optiques, une variante de la <span class="em">FDM</span> appelée <span class="html">multiplexage par répartition en longueur d'onde (WDM, pour Wavelength Division Multiplexing)</span> peut être utilisée. Une fibre optique peut transporter de la lumière à différentes longueurs d'onde sans interférence. Avec le <span class="em">WDM</span>, une longueur d'onde différente est attribuée à chaque appareil partageant la m^me fibre optique.</p>
                    <p>La <span class="html">multiplexion par répartition dans le temps (TDM, pour Time Division Multiplexing)</span> est une méthode d'allocation statique de bande passante qui a été initialement définie pour le réseau téléphonique. Dans le réseau téléphonique fixe, une conversation vocale est généralment transmise sous forme d'un signal de 64 kbps. Ainsi, une conversation téléphonique génère 8 Ko par seconde, soit un octet toutes les 125 microsecondes. Les conversations téléphoniques doivent souvent être multiplxées sur une seule ligne. Par exemple, en Europe, trente signaux vocaux de 64 Kbps sont multiplexés sur une seule ligne de 2 Mbps (<span class="em">E1</span>). Cela est réalisé en utilisant le <span class="em">TDM</span>. Le TDM divise les intervalles de transmission en <span class="em">crénaux (slots)</span>. Dans le réseau téléphonique, un créeau correspond à 125 microsecondes. Une position à l'intérieur de chaque créneau est réservée à chaque signal vocal. La figure ci-dessous illustre le TDM sur une liaison utilisée pour transporter quatre conversations vocales. Les lignes verticales représentent les limites des créneaux et les lettres représentent les différentes conversations vocales. Un octet de chaque conversation vocale est envoyé pendant chaque créneau de 125 microsecondes. L'octet correspondant à une conversation donnée est toujours envoyé à la même position dans chaque créneau.</p>
                    <figure>
                        <img src="../images/TDM.png" alt="">
                        <figcaption>Figure 6.4 : Multiplexion par répartition dans le temps</figcaption>
                    </figure>
                    <p>Le TDM, tel qu'illustré ci-dessus, peut être entièrement statique, c'est-à-dire que les mêmes conversations partagent toujours la liaison, ou dynamique. Dans ce dernier cas, les deux extrémités de la liaison doivent échanger des messages spécifiant quelle conversation utilise quel octet à l'intérieur de chaque créneau. Grâce à ces messages de signalisation, il est possible d'ajouter et de supprimer dynamiquement des conversations vocales à partir d'une liaison donnée.</p>
                    <p>Le TDM et la FDM sont largement utilisés dans les réseaux téléphoniques pour prendre en charge des conversations à bande passante fixe. Les utiliser dans des réseaux locaux qui prennent en charge des ordinateurs serait probablement inefficace. Les ordinateurs n'envoient généralement pas d'informations à un débit fixe. Au lieu de cela, ils ont souvent un comportement marche-arrêt. Pendant la période d'activité, l'ordinateur essaie d'envoyer à un débit maximal possible, par exemple pour transférer un fichier. Pendant la période d'inactivité, qui est souvent beaucoup plus longue que la période d'activité, l'ordinateur ne transmet aucun paquet. Utiliser un schéma d'allocation statique pour les ordinateurs connectés à un réseau local entraînerait d'énormes inefficacités, car ils ne pourraient transmettre qu'à <span class="em">1/N</span> de la bande passante totale pendant leur période d'activité, malgré le fait que les autres ordinateurs soient en période d'inactivité et n'aient donc pas besoin de transmettre d'informations. Les algorithmes MAC dynamiques discutés dans le reste de ce chapitre visent à résoudre ce problème.</p>
                    <h5>ALOHA :</h5>
                    <p>Dans les années 1960, les ordinateurs étaient principalement des ordinateurs centraux avec quelques dizaines de terminaux qui y étaient connectés. Ces terminaux étaient généralement situés dans le même bâtiment que l'ordinateur central et y étaient directement connectés. Dans certains cas, les terminaux étaient installés dans des endroits éloignés et connectés via un modem attaché à une ligne téléphonique. L'université d'Hawaï a choisi une organisation différente. Au lieu d'utiliser des lignes téléphoniques pour connecter les terminaux distants, ils ont développé la première technologies de radio paquet [Abramson1970]. Jusque-là, les réseaux informatiques étaient construits sur la base du réseau téléphonique ou de câbles physiques. ALOHANet a démontré qu'il était possible d'utiliser des signaux radio pour interconnecter des ordinateurs.</p>
                    <p>La première version d'ALOHANet, décrite dans [Abraham1970], fonctionnait de la manière suivante ; tout d'abord, les terminaux et l'ordinateur central échangeaient des trames de longueur fixe composées de 704 bits. Chaque trame contenait 80 caractères de 8 bits, ainsi que certains bits de contrôles et des informations de parité pour détecter les erreurs de transmission. Deux canaux dans la plage de 400 MHz étaient réservés pour le fonctionnement d'ALOHANet. Le premier canal était utilisé par l'ordinateur central pour envoyer des trames à tous les terminaux. Le deuxième canal était partagé par tous les terminaux pour envoyer des trames à l'ordinateur central. Étant donné que tous les terminaux partagent le même canal de transmission, il y a un risque de collision. Pour faire face à ce problème ainsi qu'aux erreurs de transmission, l'ordinateur central vérifiait les bits de parité de la trame reçue et envoyait un accusé de réception sur son canal pour chaque trame correctement reçue. Les terminaux, quant à eux, devaient retransmettre les trames non acquittées. Comme pour TCP, retransmettre ces trames immédiatement après l'expiration d'un délai d'attente fixe n'est pas une bonne approche, car plusieurs terminaux peuvent retransmettre leurs trames en même temps, ce qui entraîne un effondrement du réseau. Une meilleure approche, mais toujours loin d'être parfaite, consite à ce que chaque terminal attende un laps de temps aléatoire après l'expiration de son délai de retransmission. Cela évite la synchronisation entre les multiples terminaux en cours de retransmission.</p>
                    <p>Le pseudo-code ci-dessous illustre le fonctionnement d'un terminal ALOHANet. Nous utilisons cette syntaxe Python pour tous les algorithmes MAC décrits dans ce chapitre. L'algorithme est appliqué à chaque nouvelle trame devant être transmise. Il tente de transmettre une trame au maximum <span class="em">mex</span> fois (<span class="em">boucle while</span>). Chaque tentative de transmission est effectuée comme suit ; d'abord, la trame est envoyée. Chaque trame est protégée par un délai d'attente. Ensuite, le terminal attend soit une trame d'acquittement valide, soit l'expiration de son délai d'attente. Si le terminal reàoit un acquittement, la trame a été livrée correctement et l'algorithme se termine. Sinon, le terminal attend un temps aléatoire et tente de retransmettre la trame.</p>
<pre><code># ALOHA
N=1
while N &lt;= max :
    send(frame)
    wait(ack_on_return_channel or timeout)
    if (ack_on_return_channel):
        break # transmission was successful
    else:
        # timeout
        wait(random_time)
        N=N+1
else:
    # Too many transmission attempts</code></pre>
                    <p>[Abramson1970] a analysé les performances de ALOHANet et en supposant certaines conditions particulières et a constaté que ALOHANet fonctionnait bien lorsque le canal était peu chargé. Dans ce cas, les trames sont rarement retransmises et le trafic du canal, c'est-à-dire le nombre total de trames (correctes et retransmises) transmises par unité de temps, est proche de l'utilisation du canal, c'est-à-dire le nombre de trames correctement transmises par unité de temps. Malheureusement, l'analyse révèle également que l'utilisation du canal atteint son maximum à 1/(2 * e) = 0,186 fois la largueur de bande du canal. À une utilisation plus élevée, ALOHANet devient instable et le réseau s'effondre en raison des retransmissions en collision.</p>
                    <hr>
                    <p>Note : Radio amateur en paquets :</p>
                    <p>Les technologies de radio en paquets ont évolué dans différentes directions depuis les premières expériences réalisées à l'Université d'Hawaï. Le service de radio amateur en paquets développé par des opérateurs radio amateurs est l'un des descendants d'ALOHANet. De nombreux opérateurs radio amateurs sont très intéressés par les nouvelles technologies et passent souvent d'innombrables heures à développer de nouvelles antennes ou émetteurs-récepteurs. Lorsque les premiers ordinateurs personnels sont apparus, plus opérateurs radio amateurs ont conçu des modems radio et leurs propres protocoles de couche de liaison de données [KPD1985] [BNT1997]. Ce réseau s'est développé et il était possible de se connecter à des serveurs dans plusieurs payes européens en utilisant uniquement des relais de radio en paquets. Certains opérateurs radio amateurs ont également développé des piles de protocoles TCP/IP qui étaient utilisées via le service de radio en paquets. Certaines parties du réseau de radio amateur en paquets sont connectées à Internet et utilisent le préfixe <span class="em">44.0.0.0/8</span>.</p>
                    <hr>
                    <p>De nombreuses améliorations de ALOHANet ont été proposées depuis la publication de [Abraham1970], et cette technique, ou certaines de ses variantes, se retrouvent encore dans les réseaux sans fil aujourd'hui. La technique à créneaux proposée dans [Roberts1975] est importante car elle montre qu'une simple modification peut améliorer significativement l'utilisation du canal. Au lieu de permettre à tous les terminaux de transmettre à tout moment, [Roberts1975] a proposé de diviser le temps en créneaux et de permettre aux terminaux de transmettre uniquement au début de chaqye créneau. Chaque créneau correspond au temps nécessaire pour transmettre une trame de taille fixe. En pratique, ces créneaux peuvent être imposés par une seule horloge reçue par tous les terminaux. Dans ALOHANet, cela aurait pu être situé sur le mainframe central. L'analyse dans [Roberts1975] révèle que cette simple modification améliore l'utilisation du canal d'un facteur de deux.</p>
                    <h5>Accès multiple par détection de porteuse (CSMA) :</h5>
                    <p>ALOHA et ALOHA à créneaux peuvent être facilement implémenté, mais malheureusement, ils ne peuvent être utilisés que dans des réseaux très peu chargés. Concevoir un réseau pour une utilisation très faible est possible, mais cela augmente clairement le coût du réseau. Pour surmonter les problèmes de ALOHA, de nombreux mécanismes de MAC ont été proposés pour améliorer l'utilisation du canal. L'<span class="html">accès multiple par détection de porteuse (CSMA, pour Carrier Sense Multiple Access)</span> est une amélioreation significative par rapport à ALOHA. <span class="em">CSMA</span> néccessite que tous les noeuds écoutent le canal de transmission pour vérifier s'il est libre avant de transmettre une trame [KT1975]. Lorsqu'un noeud détecte que le canal est occupé, il diffère sa transmission jusqu'à ce que le canal redevienne libre. Le pseudo-code ci-dessous fournit une description plus détaillée du fonctionnement de <span class="em">CSMA</span>.</p>
<pre><code># persistent CSMA
N=1
while N &lt;= max :
    wait(channel_becomes_free)
    send(frame)
    wait(ack or timeout)
    if ack :
        break # transmission was successful
    else :
        # timeout
        N=N+1
# end of while loop
    # Too many transmission attempts</code></pre>
                    <p>Le pseudo-code ci-dessus est souvent appelé <span class="html">CSMA persistant</span> [KT1975], car le terminal écoute en continu le canal et transmet sa trame dès que le canal devient libre. Une autre variante importante de CSMA est le <span class="html">CSMA non persistant</span> [KT1975]. La principale différence entre CSMA persistant et non persistant, décrite dans le pseudo-code ci-dessous, est qu'un noeud CSMA non persistant n'écoute pas en continu le canal pour déterminer quand il devient libre. Lorsqu'un terminal CSMA non persistant détecte que le canal de transmission est occupé, il attend un temps aléatoire avant de vérifier à nouveau l'état du canal. Cela améliore l'utilisation du canal par rapport à CSMA persistant. Avec CSMA persistant, lorsque deux terminaux détectent que le canal est occupé, ils transmettent tous les deux (et provoquent ainsi une collision) dès que le canal devient livre. Avec CSMA non persistant, cette synchronisation ne se produit pas, car les terminaux attendent un temps aléatoire après avoir détecté l'état du canal de transmission. Cependant, l'utilisation du canal plus élevée obtenue par CSMA non persistant s'accompagne d'un léger temps d'attente supplémentaire dans les terminaux lorsque le réseau est peu chargé.</p>
<pre><code># Non persistent CSMA
N=1
while N &lt;= max :
    listen(channel)
    if free(channel):
        send(frame)
        wait(ack or timeout)
        if received(ack) :
            break # transmission was successful
        else :
            # timeout
            N=N+1
    else:
        wait(random_time)
# end of while loop
    # Too many transmission attempts</code></pre>
                    <p>[KT1975] analyse en détail les perdormances de plusieurs variantes de CSMA. Sous certaines hypothèses concernant le canal de transmission et le trafic, l'analyse compora ALOHA, ALOHA à crénaux, CSMA persistant et CSMA non persistant. Selon ces hypothèses, ALOHA atteint une utilisation du canal correspondant à seulement 18,4% de la capacité du canal. ALOHA à créneaux parvient à utiliser 36,6% de cette capacité. CSMA persistant améliore l'utilisation en atteignant 52,9% de la capacité, tandis que CSMA non persistant atteint 81,5% de la capacité du canal.</p>
                    <h5>CSMA avec détection de collision :</h5>
                    <p>CSMA améliore l'utilisation du canal par rapport à ALOHA. Cependant, les performances peuvent encore être améliorées, en particulier dans les réseaux câblés. Considérons la situation de deux terminaux connectés au même câble. Ce câble pourrait, par exemple, être un câble coaxial comme dans les premiers jours d'Ethernet [Metcalfe1976]. Il pourrait également être constitué de paires torsadées. Avant d'étendre CSMA, il est utile de comprendre de manière plus intuitive comment les trames sont transmises dans un tel réseau et comment les collisions peuvent se produire. La figure ci-dessous illustre la transmission physique d'une trame sur un tel câble. Pour transmettre sa trame, l'hôte <span class="em">A</span> doit envoyer un signal étrique sur le support partagé. La première étape consiste donc à commencer la transmission du signal électrique. Cela correspond au point (<span class="em">1</span>) dans la figure ci-dessous. Ce signal électrique se propage le long du câble. Bien que les signaux électriques se déplacent rapidement, nous savons que l'information ne peut pas se déplacer plus rapodement que la vitesse de la lumière (c'est-çà-dire 300000 kilomètres par seconde). Sur un câble coaxial, un signal électrique est légèrement plus lent que la vitesse de la lumière et une estimation raisonnable est de 200000 kilomètres par seconde. Cela implique que si le câble a une longueur d'un kilomètre, le signal électrique mettra 5 microsecondes pour se déplacer d'une extrémité du câble à l'autre. Les exrémités des câbles coaxiaux sont équipées de terminaisons qui veillent à ce que le signal électrique ne soit pas réfléchi vers sa souce. Cela est illustré au point (<span class="em">3</span>) de la figure, où le signal électrique a atteint l'extrémité gauche et l'hôte <span class="em">B</span>. À ce stade, <span class="em">B</span> commence à recevoir la trame transmise par <span class="em">A</span>. Remarquez qu'il y a un délai entre la transmission d'un bit sur l'hôte <span class="em">A</span> et sa réception par l'hôte <span class="em">B</span>. Si d'autres hôtes étaient connectés au câble, ils recevraient le premier bit de la trame à des moments légèrement différents. Comme nous le verrons plus tard, cette différence de synchronisation pose un problème clé pour les algorithmes MAC. Au point (<span class="em">4</span>, le signal électrique a atteint les deux extrémités du câble et l'occupe complètement. L'hôte <span class="em">A</span> continue de transmettre le signal électrique jusqu'à la fin de la trame. Comme indiqué au point (<span class="em">5</span>), lorsque l'hôte émetteur arrête sa transmission, le signal électrique correspondant à la fin de la trame quitte le câble coaxial. Le canal redevient vide une fois que l'ensemble du signal électrique a été retiré du câble.</p>
                    <figure>
                        <img src="../images/transmission_trame_bus_partage.png" alt="">
                        <figcaption>Figure 6.5 : Transmission des trames sur un bus partagé</figcaption>
                    </figure>
                    <p>Maintenant que nous avons examiné comment une trame est réellement transmise sous forme de signal électrique sur un bus partagé, il est intéressant de se pencher plus en détail sur ce qui se passe lorsque deux hôtes transmettent une trame presque simultanément. Cela est illustré dans la figure ci-dessous, où les hôtes <span class="em">A</span> et <span class="em">B</span> commencent leur transmission en même temps (point (<span class="em">1</span>)). À ce moment-là, si l'hôte <span class="em">C</span> détecte le canal, il le considérera comme libre. Cela ne durera pas longtemps et au point (<span class="em">2</span>), les signaux électriques des hôtes <span class="em">A</span> et <span class="em">B</span> atteignent l'hôte <span class="em">C</span>. Le signal électrique combiné (représenté graphiquement par la superposition des deux courbes dans la figure) ne peut pas être décodé par l'hôte <span class="em">C</span>. L'hôte <span class="em">C</span> détecte une collision, car il reçoit un signal qu'il ne peut pas décoder. Étant donné que l'hôte <span class="em">C</span> ne peut pas décoder les trames, il ne peut pas déterminer quels hôtes envoient les trames en collision. Notez que l'hôte <span class="em">A</span> (et l'hôte <span class="em">B</span>) détectera la collision après l'hôte <span class="em">C</span>(point (<span class="em">3</span>) dans la figure ci-dessous).</p>
                    <figure>
                        <img src="../images/collision_trame_bus_partage.jpg" alt="">
                        <figcaption>Figure 6.6 : Collision de trames sur un bus partagé</figcaption>
                    </figure>
                    <p>Comme indiqué ci-dessus, les hôtes détectent les collisions lorsqu'ils reçoivent un signal électrique qu'ils ne peuvent pas décoder. Dans un réseau câblé, un hôte est capable de détecter une telle collision à la fois lorsqu'il est en écoute (par exemple, comme l'hôte <span class="em">C</span> dans la figure ci-dessus) et lorsqu'il envoie sa propre trame. Lorsqu'un hôte émet une trame, il peut comparer le signal électrique qui'il émet avec le signal électrique qu'il détecte sur le câble. Aux points (<span class="em">1</span>) et (<span class="em">2</span>) dans la figure ci-dessus, l'hôte <span class="em">A</span> ne détecte que son propre signal. Au point (<span class="em">3</span>), il détecte un signal électrique différent du sien et peut ainsi détecter la collision. À ce stade, sa trame est corrompue et il peut arrêter sa transmission. La capacité de détecter les collisions pendant la transmission est le point de départ de l'algorithme MAC <span class="html">Carrier Sense Multiple Access with Collision Detection (CSMA/CD)</span>, qui est utilisé dans les réseaux Ethernet [Metcalfe1976] [802.3]. Lorsqu'un hôte Ethernet détecte une collision pendant sa transmission, il arrête immédiatement sa transmission.</p>
                    <p>Par rapport au CSMA pur, le CSMA/CD constitue une amélioration importante car lorsqu'une collision se produit, elle ne dure que jusqu'à ce que les hôtes en collision l'aient détectée et arrêtent leur transmission. En pratique, lorsqu'un hôte détecte une collision, il envoie un signal d'interférence spécial sur le câble pour s'assurer que tous les hôtes ont détecté la collision.</p>
                    <p>Pour mieux comprendre ces collisions, il est utile d'analyser ce qui serait la pire collision sur un réseau à bus partagé. Prenons un câble avec deux hôtes attachés aux deux extrémités, comme illustré dans la figure ci-dessous. L'hôte <span class="em">A</span> commence à transmettre sa trame et son signal électrique se propage sur le câble. Le temps de propagation dépend de la longueur physique du câble et de la vitesse du signal électrique. Utilisons <span class="em">&#964;</span> pour représenter ce délai de propagation en secondes. Légèrement moins de <span class="em">&#964;</span> secondes après le début de la transmission de la trame d'<span class="em">A</span>, <span class="em">B</span> décide de commencer à transmettre sa propre trame. Au bout de <span class="em">e</span> secondes, <span class="em">B</span> détecte la trame d'<span class="em">A</span>, détecte la collision et arrête sa transmission. Le début de la trame de <span class="em">B</span> se propage sur le câble jusqu'à ce qu'elle atteigne l'hôte <span class="em">A</span>. <span class="em">A</span> peut ainsi détecter la collision à l'instant <span class="em">&#964; - e + &#964; &#8776; 2 * &#964;</span>. Un point important à noter est qu'une collision ne peut se produire que pendant les premières <span class="em">2 * &#964;</span> secondes de sa transmission. Si une collision ne s'est pas produite pendant cette période, elle ne peut pas se produire par la suite car le canal de transmission est ocuupé après <span class="em">&#964;</span> secondes et les hôtes CSMA/CD détectent le canal de transmission avant de transmettre leur trame.</p>
                    <figure>
                        <img src="../images/pire_collision_bus_partage.jpg" alt="">
                        <figcaption>Figure 6.7 : La pire collision sur un bus partagé</figcaption>
                    </figure>
                    <p>De plus, sur les réseaux câblés où le CSMA/CD est utilisé, les collisions sont presque la seule cause d'erreurs de transmission qui affectent les trames. Les erreurs de transmission qui n'affectent que quelques bits à l'intérieur d'une trame sont rares dans ces réseaux câblés. Pour cette raison, les concepteurs du CSMA/CD ont choisi de supprimer complètement les trames d'acquittement dans la couche liaison de données. Lorsqu'un hôte transmet une trame, il vérifie si sa transmission a été affectée par une collision. Si ce n'est pas le cas, compte tenu du taux d'erreur de bit négligeable du réseau sous-jacent, il suppose que la trame a été correctement reçue par sa destination. Sinon, la trame est retrransmise après un certain délai.</p>
                    <p>Supprimer les acquittements est une optimisation intéressante car cela réduit le nombre de trames échangées sur le réseau et le nombre de trames qui doivent être traitées par les hôtes. Cependant, pour utiliser cette optimisation, nous devons nous assurer que tous les hôtes seronht en mesure de détecter toutes les collisions qui affectent leurs trames. Le problème est important pour les trames courtes. Considérons deux hôtes, <span class="em">A</span> et <span class="em">B</span>, qui envoient une petite trame à l'hôte <span class="em">C</span> comme illustré dans la figure ci-dessous. Si les trames envoyées par <span class="em">A</span> et <span class="em">B</span> sont très courtes, la situation illustrée ci-dessous peut se produire. Les hôtes <span class="em">A</span> et <span class="em">B</span> envoient leur trame et arrêtent leur transmission (point (<span class="em">1</span>)). Lorsque les deux trames courtes arrivent à l'emplacement de l'hôte <span class="em">C</span>, elles entrent en collision et l'hôte <span class="em">C</span> ne peut pas les décoder (point (<span class="em">2</span>)). Les deux trames sont absorbées par les extrémités du câble. Ni l'hôte <span class="em">A</span> ni l'hôte <span class="em">B</span> n'ont détecté la collision. Ils considèrent tous les deux que leur trame a été reçue correctement par sa destination.</p>
                    <figure>
                        <img src="../images/probleme_collision_trames_courtes.png" alt="">
                        <figcaption>Figure 6.8 : Le problème de collision des trames courtes</figcaption>
                    </figure>
                    <p>Pour résoudre ce problème, les réseaux utilisant CSMA/CD exigent que les hôtes transmettent pendant au moins pendant au moins <span class="em">2 * &#964;</span> secondes. Étant donné que la vitesse de transmission du réseau est fixe pour une technologie de réseau donnée, cela implique qu'une technologie utilidant CSMA/CD impose une taille minimale de trame. Dans la technologie CSMA/CD la plus populaire, Ethernet, <span class="em">2 * &#964;</span> est appelé le <span class="html">temps de fente (slot time)</span>. Ce terme ne doit pas être confondu avec la durée d'une fente de transmission dans slotted ALOHA. Dans les réseaux CSMA/CD, temps de fente est la durée pendnat laquelle une collision peut se produire au début de la transmission d'une trame. Dans slotted ALOHA, la durée d'une fente est le temps de transmission d'une trame de taille fixe dans son intégralité.</p>
                    <p>La dernière innovation introduire par CSMA/CD est le calcul du délai de retransmission. Comme pour ALOHA, ce délai ne peut pas être fixe, sinon les hôtes pourraient se synchroniser et toujours retransmettre en même temps. Le réglage d'un tel déali est toujours un compromis entre le délai d'accès au réseau et le nombre de collisions. Un délai court entraînerait un faible délai d'accès au réseau mais avec un risque plus élevé de collisions. En revanche, un délai long entraînerait un long délai d'accès au réseau mais un risque moindre de collisions. L'algorithme de <span class="html">recul exponentiel binaire (binary exponential back-off)</span> a été introduit dans les réseaux CSMA/CD pour résoudre ce problème.</p>
                    <p>Pour comprendre le <span class="em">recul exponentiel binaire</span>, considérons une collision causée par exactement deux hôtes. Une fois qu'il a détecté la collision, un hôte peut soit retransmettre immédiatement son cadre soit différer sa transmission pendant un certain temps. Si chaque hôte en collision lance une pièce de monnaie pour décider s'il doit retransmettre immédiatement ou différer sa retransmission, quatre cas sont possibles :</p>
                    <ol>
                        <li>
                            <p>Les deux hôtes retransmettent immédiatement et une nouvelle collision se produit.</p>
                        </li>
                        <li>
                            <p>Le premier hôte retransmet immédiatement et le second diffère sa retransmission.</p>
                        </li>
                        <li>
                            <p>Le second hôte retransmet immédiatement et le premier diffère sa retransmission.</p>
                        </li>
                        <li>
                            <p>Les deux hôtes diffèrent leur retransmission et une nouvelle collision se produit.</p>
                        </li>
                    </ol>
                    <p>Dans les deuxième et troisième cas, les deux hôtes ont lancé des pièces de monnaie différentes. Le délai choisi par l'hôte qui diffère sa retransmission doit être suffisamment long pour garantir que sa retransmission ne sera pas en collision avec la retransmission immédiate de l'autre hôte. Cependant, le délai ne doit pas être plus long que le temps nécessaire pour éviter la collision, car si les deux hôtes décident de différer leur transmission, le réseau sera inactif pendant ce temps. Le <span class="em">temps en créneau</span> est le délai optimal car c'est le délai plus court qui garantit que le premier hôte pourra retransmettre complètement son cadre sans collision.</p>
                    <p>Si deux hôtes sont en compétition, l'algorithme ci-dessus évitera une deuxième collision 50% du temps. Cependant, si le réseau est fortement chargé, plusieurs hôtes peuvent être en compétition en même temps. Dans ce cas, les hôtes doivent être capables d'adapter automatiquement leur délai de retransmission. Le <span class="em">recul exponentiel binaire</span> effectue cette adaptation en fonction du nombre de collisions qui ont affecté une trame. Après la première collision, l'hôte lance une pièce de monnaie et attend 0 ou 1 temps de créneau. Après la deuxième collision, il génère un nombre aléatoire et attend 0, 1, 2 ou 3 temps en créneaux. Le pseudo-code complet de l'algorithme CSMA/CD est indiqué ci-dessous.</p>
<pre><code># CSMA/CD pseudo-code
N=1
while N &lt;= max :
    wait(channel_becomes_free)
    send(frame)
    wait_until (end_of_frame) or (collision)
    if collision detected:
        stop transmitting
        send(jamming)
        k = min (10, N)
        r = random(0, 2k - 1) * slotTime
        wait(r*slotTime)
        N=N+1
    else :
        wait(inter-frame_delay)
        break
# end of while loop
    # Too many transmission attempts</code></pre>
                    <p>Le délai intertrame utilisé dans ce pseudo-code est un court délai correspondant au temps nécessaire à un adaptateur réseau pour passer du mode transmission au mode de réception. Il est égalment utilisé pour empêcher un hôte d'envoyer en continu des trames sans laisser d'opportunités de transmission pour d'autres hôtes sur le réseau. Cela contribue à l'quité de CSMA/CD. Malgré ce délai, il existe encore des conditions où CSMA/CD n'est pas complètement équitable [RY1994]. Prenons par exemple un réseau avec deux hôtes : un serveur envoyant des trames longues et un client envoyant des accusés de réception. Des mesures rapportées dans [RY1994] ont montré qu'il existe des situations où le client pourrait souffrir de collisions répétées qui le conduisent à attendre de longues périodes en raison de l'algorithme de rétrogradation exponentielle.</p>
                    <h5>Le CSMA avec évitement de colision (CSMA/CA) :</h5>
                    <p>L'algorithme MAC <span class="html">Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA)</span> a été conçu pour la technologie de réseau sans fil WiFi populaire [802.11]. CSMA/CA détecte également le canal de transmission avant d'envoyer une trame. De plus, CSMA/CA essaie d'éviter les collisions en ajustant soigneusement les timers utilisés par les dispositifs CSMA/CA.</p>
                    <p>CSMA/CA utilise des accusés de réception, tout comme CSMA. Chaque trame contient un numéro de séquence et un CRC. Le CRC est utilisé pour détecter les erreurs de transmission, tandos que le numéro de séquence est utilisé pour éviter la duplication des trames. Lorsqu'un appareil reçoit une trame correcte, il renvoie un accusé de réception spécial à l'expéditeur. CSMA/CA introduit un léger délai, appelé <span class="html">intervalle court entre trames (Short Inter Frame Spacing - SIFS)</span>, entre la réception d'une trame et la transmission de l'accusé de réception. Ce délai correspond au temps nécessaire pour passer la radio d'un appareil entre les modes de réception et de transmission.</p>
                    <p>Comparé à CSMA, CSMA/CA définit de manière plus précise quand un appareil est autorisé à envoyer une trame. Tout d'abord, CSMA/CA définit deux délais : <span class="html">DIFS</span> et <span class="html">EIFS</span>. Pour envoyer une trame, un appareil doit d'abord attendre que le canal soit inactif pendant au moins le <span class="html">Distributed Coordination Function Inter Frame Space (DIFS)</span> si la trame précédente a été reçue correctement. Cependant, si la trame précédemment reçue était corrompue, cela indique qu'il y a eu des collisions et l'appareil doit détecter un canal inactif pendant au moins l'<span class="html">Extended Inter Frame Space (EIFS)</span>, avec <span class="em">SIFS &lt; DIFS &lt; EIFS</span>. Les valeurs exactes de SIFS, DIFS et EIFS dépendent de la couche physique sous-jacente [802.11].</p>
                    <p>La figure ci-dessous montre le fonctionnement de base des appareils CSMA/CA. Avant de transmettre, l'hôte <span class="em">A</span> vérifie que le canal est vide pendant une période suffisamment longue. Ensuite, il envoie sa trame de données. Après avoir vérifié la validité de la trame reçue, le destinataire envoie une trame d'accusé de réception après un court délai SIFS. L'hôte <span class="em">C</span>, qui ne participe pas à l'échange de trames, détecte que le canal est occupé au début de la trame de données. L'hôte <span class="em">C</span> peut utiliser cette information pour déterminer la durée pendant laquelle le canal sera occupé. Notez que puisque <span class="em">SIFS &lt; DIFS &lt; EIFS</span>, même un appareil qui commencerait à détecter le canal immédiatement après le dernier bit de la trame de données ne pourrait pas décider de transmettre sa propre trame pendant la transmission de la trame d'accusé de réception.</p>
                    <figure>
                        <img src="../images/operation_appareil_CSMA_CA.png" alt="">
                        <figcaption>Figure 6.9 : Fonctionnement d'un appareil CSMA/CA</figcaption>
                    </figure>
                    <p>La principale difficulté avec CSMA/CA survient lorsque deux ou plusieurs appareils transmettent en même temps et provoquent des collisions. Cela est illustré dans la figure ci-dessous, en supposant un délai fixe après la transmission d'une trame de données. Avec CSMA/CA, le délai après la transmission d'une trame de données est très court, car il correspond au SIFS plus le temps nécessaire pour transmettre la trame d'accusé de réception.</p>
                    <figure>
                        <img src="../images/collisions_avec_CSMA_CA.png" alt="">
                        <figcaption>Figure 6.10 : Collisions avec CSMA/CA</figcaption>
                    </figure>
                    <p>Pour résoudre ce problème, CSMA/CA utilise un timer de recul (backoff timer). Ce timer de recul est un délai aléatoire choisi par chaque appareil dans une plage qui dépend du nombre de retransmissions pour la trame en cours. La plage augmente de manière exponentielle avec les retransmissions, tout comme CSMA/CD. La plage minimale pour le timer de recul est <span class="em">[0,7 * slotTime]</span>, pù le <span class="em">slotTime</span> est un paramètre qui dépend de la couche physique sous-jacente. Comparé au recul exponentiel de CSMA/CD, il y a deux différences importantes à noter. Premièrement, la plage initiale pour le timer de recul est sept fois plus grande. Cela est dû au fait qu'il est impossible de détecter les collisions avec CSMA/CA au moment où elles se produisent. Avec CSMA/CD, elle ne peut affter que le début de la trame. Deuxièmement, un appareil CSMA/CA doit régulièrement détecter le canal de transmission pendant son timer de recul. Si le canal devient occupé (c'est-à-dire qu'un autre appareil est en train de transmettre), alors le timer de recul doit être gelé jusqu'à ce que le canal redevienne libre. Une fois que le canal redevient libre, le timer de recul est redémarré. Cela contraste avec CSMA/CD où le recul est recalculé après chaque collision. Cela est illustré dans la figure ci-dessous. L'hôte <span class="em">A</span> choisit un recul plus court que l'hôte <span class="em">C</span>. Lorsque <span class="em">C</span> détecte que le canal est occupé, il gèle son timer de recul et ne le redémarre que lorsque le canal redevient libre.</p>
                    <figure>
                        <img src="../images/exemple_detaille_CSMA_CA.png" alt="">
                        <figcaption>Figure 6.11 : Exemple détaillé avec CSMA/CA</figcaption>
                    </figure>
                    <p>Le pseudo-code ci-dessous résume le fonctionnement d'un appareil CSMA/CA. Les valeurs de SIFS, DIFS, EIFS et slotTime dépendent de la technologie de la couche physique sous-jacente [802.11].</p>
<pre><code># CSMA/CA simplified pseudo-code
N=1
while N &lt;= max :
    waitUntil(free(channel))
    
    if correct(last_frame) :
        wait(channel_free_during_t >=DIFS)
    else:
        wait(channel_free_during_t >=EIFS)
    
    back-off_time = int(random[0,min(255,7*(2^(N-1)))])*slotTime
    wait(channel free during backoff_time)
    # backoff timer is frozen while channel is sensed to be busy
    send(frame)
    wait(ack or timeout)
    if received(ack)
        # frame received correctly
        break
    else:
        # retransmission required
        N=N+1</code></pre>
                    <p>Un autre problème rencontré par les réseaux sans fil est souvent appelé le <span class="html">problème de la station cachée</span>. Dans un réseau sans fil, les signaux radio ne se propagent pas toujours de la même manière dans toutes les directions. Par exemple, deux appareils sépar&s par un mur peuvent ne pas être en mesure de recevoir le signal émis par un troisième hôte. Cela est illustré dans figure ci-dessous, mais cela peut se produire dans d'autres environnements. Par exemple, deux appareils situés de part et d'autre d'une colline peuvent ne pas être en mesure de recevoir le signal envoyé par une station au sommet de la colline. De plus, les confitions de propagation radio peuvent changer avec le temps. Par exemple, un camion peut bloquer temporairement la communication entre deux appareils proches.</p>
                    <figure>
                        <img src="../images/probleme_station_cachee.png" alt="">
                        <figcaption>Figure 6.12 : Le problème de la station cachée</figcaption>
                    </figure>
                    <p>Pour éviter les collisions dans ces situations, CSMA/CA permet aux appareils de réserver le canal de transmission pendnat un certain temps. Cela est réalisé en ytilisant deux trames de contrôle : <span class="html">request To Send (RTS, demande d'envoi)</span> et <span class="html">Clear To Send (RTS, autorisation d'envoi)</span>. Les deux trames sont très courtes pour minimiser le risque de collisions. Pour réserver le canal de transmission, un appareil envoie une trame RTS au destinataire prévu du cadre de données. La trame RTS contient la durée de la réservation demandée. Le destinataire répond, après un délai SIFS, avec une trame CTS qui contient également la durée de la réservation. Comme la durée de la réservation a été envoyée à la fois dans les trames RTS et CTS, tous les hôtes susceptibles de provoquer une collision avec l'émetteur ou la réception de la frame de données sont informés de la réservation. Ils peuvent calculer la durée totale de la transmission et différer leur accès au canl de transmission jusqu'à cette durée. Cela est illustré dans la figure ci-dessous où l'hôte <span class="em">A</span> réserve le canal de transmission pour envoyer un cadre de données à l'hôte <span class="em">B</span>. L'hôte <span class="em">C</span> remarque la réservation et diffère sa transmission.</p>
                    <figure>
                        <img src="../images/reservations_CSMA_CA.png" alt="">
                        <figcaption>Figure 6.13 : Réservations avec CSMA/CA</figcaption>
                    </figure>
                    <p>L'utilisation des réservations avec CSMA/CA est une optimisation utile lorsque les collisions sont fréquentes. S'il y a peu de collisions, le temps nécessaire pour transmettre les trames RTS et CTS peut devenir significatif, en particulier lors de l'échange de trames courtes. Certains appareils n'activent RTS/CTS qu'après des erreurs de transmission.</p>
                    <h5>Algrithmes de contrôle d'accès déterministes au support de transmission :</h5>
                    <p>Au cours des années 1970 et 1980, il y a eu d'énormes débats au sein de la communauté de réseautage sur les algorithmes MAC les mieux adaptés aux LAN. Les algorithmes optimistes que nous avons décrits jusqu'à présent étaient relativement faciles à implémenter lorsqu'ils ont été conçus. Du point de vue des performances, les modèles mathématiques et les simulations ont montré la capacité de ces techniques optimistes n'est capable de garantir qu'une trame sera livrée dans un délai donné, et certaines applications nécessitent des délais de transmission prévisibles. Les algorithmes MAC déterministes ont été considérés par une fraction de la communauté de réseautage comme la meilleure solution pour répondre aux besoins des LAN.</p>
                    <p>Les partisans des techniques déterministes et oportunistes ont fait pression pour développer des normes pour les LAN qui intègreraient leur solution respective. Au lieu de chercher à trouver un compromis impossible entre ces points de vue divergents, le comité IEEE 802, chargé de développer des normes pour les LAN, a choisi de travailler en parallèle sur trois technologies LAN différentes et a créé trois groupes de travail. Le groupe de travail IEEE 802.3 est devenu responsable de CSMA/CD. Les partisans des algorithmes MAC déterministes se sont mis d'accord sur le principe de base consistant à échanger des trames spéciales appelés <span class="html">jetons (tokens en anglais)</span> entre les appareils pour réguler l'accès au support de transmision. Cependnat, ils n'étaient pas d'accord sur la configuration physique la plus appropriée pour le réseau. IBM soutenait les réseaux en forme d'anneau, tandiq que l'industrie manufacturière, dirigée par General Motors, soutenait les réseaux en forme de bus. Cela a conduit à la création du groupe de travail IEEE 802.4 pour normaleiser les réseaux <span class="html">Token Bus</span> et du groupe de travail IEEE 802.5 pour normaliser les réseaux <span class="html">Token Ring</span>. Bien que ces technologies ne soient plus largeemnt utilisées aujourd'hui d'un protocole basé sur les <span class="em">jetons</span> sont toujours importants.</p>
                    <p>La technologie Token Ring IEEE 802.5 est définie dans [802.5]. Nous utilsons Token Ring comme exemple pour expliquer les principes des algorithmes MAC basés sur les jetons dans les réseaux en forme d'anneau. D'autres réseaux en forme d'anneau incluent le FDDI presque obsolète [Ross1989] ou le plus récent Risilient Pack Ring [DYGU2004]. Une bonne étude des réseaux en forme d'anneau basés sur les jetons peut être trouvéee dans [Bux1989].</p>
                    <p>Un réseau Token Ring est composé d'un ensemble de stations qui sont connectées à une boucle unidirectionnelle. Le principe de base de l'algorithme MAC Token Ring est que deux types de trames circulent sur la boucle : les jetons (tokens) et les trames de données. Lorsque le Token Ring démarre, l'une des stations envoie le jeton. Le jeton est une petite trame qui représente l'autorisation de transmettre des données sur la boucle. Pour transmettre une trame de données sur la boucle, une station doit d'abord capturer le jeton en le retirant de la boucle. Comme seule une station peut capturer le jeton à la fois, la station qui possède le jeton peut transmettre en toute sécurité une trame de données sur la boucle sans risque de collisions. Après avoir transmis sa trame, la station doit la retirer de la boucle et renvoyer le jeton afin que d'autres stations puissent transmettre leurs propres trames.</p>
                    <p>Bien que les principes de base du Token Ring soient simples, il existe plusieurs détails d'implémentation subtils qui ajoutent de la complexité aux réseaux Token Ring. Pour comprendre ces détails, analysons le fonctionnement d'une interface Token Ring sur une station. Une interface Token Ring remplit trois fonctions différentes. Comme les autres interfaces LAN, elle doit être capable d'envoyer et de recevoir des trames. De plus, une interface Token Ring fait partie de la boucle et, en tant que telle, elle doit être capable de transmettre le signal électrique qui circule sur la boucle même lorsque sa station est éteinte.</p>
                    <p>Lorsqu'elle est sous tension, les interfaces Token Ring fonctionnent en deux modes différents : <span class="html">@coute</span> et <span class="html">transmission</span>. En mode <span class="em">écoute</span>, une interface Token Ring reçoit un signal électrique de son voisin amont sur la boucle, introduit un retard égal au temps de transmission d'un bit sur la boucle et régénère le signal avant de l'envoyer à son voisin aval sur la boucle.</p>
                    <figure>
                        <img src="../images/reseau_token_ring.png" alt="">
                        <figcaption>Figure 6.14 : Un réseau Token Ring</figcaption>
                    </figure>
                    <p>Le premier problème auquel est confronté un réseau Token Ring est que, comme le jeton représente l'autorisation de transmettre, il doit circuler en continu sur la boucle lorsque aucune trame de données n'est en cours de transmission. Supposons qu'un jeton ait été produit et envoyé sur la boucle par une station. Dans les réseaux Token Ring, le jeton est une trame de 24 bits dont la structure est indiqué ci-dessus.</p>
                    <figure>
                        <img src="../images/format_token_802.5.png" alt="">
                        <figcaption>Figure 6.15 : Format du jeton 802.5</figcaption>
                    </figure>
                    <p>Le jeton est composé de trois champs. Tout d'abord, le <span class="html">délimiteur de début (Starting Delimiter)</span> est le marqueur qui indique le début d'une trame. Les premiers réseaux Token Ring utilisaient le codage Manchester et le <span class="em">délimiteur de début</span> contenait à la fois des symboles représentant <span class="em">0</span> et des symboles qui ne représentent pas de bits. Le dernier champ est le <span class="html">délimiteur de fin (Ending Delimiter)</span> qui marque la fin du jeton. Le champ de <span class="html">ontrôle d'accès (Access Control)</span> est présent dans toutes les trames et contient plusieurs indicateurs. Le plus important est le bit de <span class="em">jeton</span> qui est défini dans les tames de jeton et réinitialisé dans les autres trames. Prenons en considération le réseau à cinq stations représenté dans la figure du réseau Token Ring ci-dessus et supposons que la station <span class="em">S1</span> envoie un jeton. Si nous négligeons le délai de propagation sur les liaisons inter-stations, chaque station introduisant un délai d'un bit, le premier bit de la trame reviendrait à pendant qu'elle envoie le cinquième bit du jeton. Si la station <span class="em">S1</span> est éteinte à ce moment-là, seuls les cinq premiers bits du jeton circuleront sur la boucle. Pour éviter ce problème, il y a une station spéciale appelée le <span class="html">Moniteur</span> sur chaque réseau Token Ring. Pour garantir que le jeton puisse circuler en continu sur la boucle, ce <span class="em">Moniteur</span> insère un délai égal au moins 24 fois le temps de transmission d'un bit. Si la station <span class="em">S3</span> était le <span class="em">Moniteur</span> dans la figure du réseau Token Ring, <span class="em">S1</span> aurait pu transmettre l'intégralité du jeton avant de recevoir le premier bit du jeton de son voisin amont.</p>
                    <p>Maintenant que nous avons expliqué comment le jeton peut être transmis sur la boucle, analysons comment une station peut capturer un jeton pour transmettre une trame de données. Pour cela, nous avons besoin d'informations sur le format des trames de données. Une trame de données 802.5 commence par le <span class="em">délimiteur de début</span>, suivi du champ de <span class="em">contrôle d'accès</span> dont le bit de jeton est réinitialisé, un champ de <span class="html">contrôle de trame (Frame Control)</span> qui permet la définition de plusieurs types de trames, une adresse de destination et une adresse source, un payload, un CRC, le <span class="em">délimiteur de fin</span> et un champ d'<span class="html">état de trame (Frame Status)</span>. Le format des trames de données du Token Ring est illustré ci-dessous.</p>
                    <figure>
                        <img src="../images/format_trame_donnees.png" alt="">
                        <figcaption>Figure 6.16 : Format de trame de données 802.5</figcaption>
                    </figure>
                    <p>Pour capturer un jeton, une station doit fonctionner en mode <span class="em">écoute</span>. Dans ce mode, la station reçoit les bits de sa station amont. Si les bits correspondent à une trame de données, ils doivent être transmis à la station aval. S'ils correspondent à un jeton, la station peut le capturer et transmettre sa trame de données. Tant la trame de données que le jeton sont encodées sous forme d'une chaîne de bits commençant par le <span class="em">délimiteur de début</span>, suivi du champ de <span class="em">contrôle d'accès</span>. Lorsque la station reçoit le premier bit d'un <span class="em">délimiteur de début</span>, elle ne peut pas savoir s'il s'agit d'une trame de données ou d'un jeton et doit transmettre l'intégralité du délimiteur à sa station aval. Ce n'est qu'à la réception du quatrième bit du champ de <span class="em">contrôle d'accès</span> (c'est-à-dire le bit de <span class="em">jeton</span>) que la station sait s'il s'agit d'une trame de données ou d'un jeton. Si le bit de <span class="em">jeton</span> est réinitialisé, cela indique une trame de données et les bits restants de la trame de données doivent être transmis à la station aval. SInon (le bit de <span class="em">jeton</span> est activé), il s'agit d'un jeton et la station peut le capturer en réinitialisant le bit qui se trouve actuellement dans son tampn. Grâce à cette modification, le début du jeton devient le début d'une trame de données et la station peut passer en mode <span class="em">transmission</span> et envoyer sa trame de données à partir du cinquième bit du champ de <span class="em">contrôle d'accès</span>. Ainsi, le retard d'un bit introduit par chaque station Token Ring joue un rôle clé en permettant aux stations de capturer efficacement le jeton.</p>
                    <p>Une fois qu'elle a transmis sa trame de données, la station doit rester en mode <span class="em">transmission</span> jusqu'à ce qu'elle ait reçu le dernier bit de sa propre trame de données. Cela garantit que les bits envoyés par une station ne restent pas indéfiniment dans le réseau. Une trame de données envoyée par une station dans un réseau Token Ring passe devant toutes les stations connectées au réseau. Chaque station peit détecter la trame de données et analyser l'adresse de destination pour éventuellement la capturer.</p>
                    <p>Le texte ci-dessus décrit le fonctionnement de base d'un réseau Token Ring lorsque toutes les stations fonctionnent correctement. <span class="html">LAPB (Link Access Procedure, Balanced)</span> et <span class="html">HDLC (High-Level Data Link Control)</span> étaient des protocoles largement utilisés de la couche de liaison de données. Malheureusement, le véritable réseau Token Ring doit être capable de gérer différents types d'anomalies, ce qui augmente la complexité des stations Token Ring. Nous énumérons brièvement les problèmes et esquissons leurs solutions ci-dessous. Une description détaillée du fonctionnement des stations Token Ring peut être trouvée dans [802.5]. Le premier problème survient lorsque toutes les stations attachées au réseau démarrent. L'une d'entre elles doit initialiser le réseau en envoyant le premier jeton. Pour cela, toutes les stations implémentent un mécanisme d'élection distribué qui est utilisé pour sélectionner le <span class="em">Moniteur</span>. N'importe quelle station peut devenir un <span class="em">Moniteur</span>. Le <span class="em">Moniteur</span> gère le réseau Token Ring et s'assure qu'il fonctionne correctement. Son premier rôle est d'introduire un délai de 24 fois le temps de transmission d'un bit pour garantir que le jeton peut circuler en douceur sur l'anneau. Deuxièmement, le <span class="em">Moniteur</span> envoie le premier jeton sur l'anneau. Il doit également vérifier que le jeton circule régulièrement. Selon la norme Token Ring [802.5], une station ne peut pas conserver le jeton pour transmettre des trames de données pendant une durée au <span class="html">temps de détention du jeton (THT, pour Token Holding Time)</span> (légèrement inférieur à 10 millisecondes). Sur un réseau contenant <span class="em">N</span> stations, le <span class="em">Moniteur</span> doit recevoir le jeton au moins toutes les <span class="em">N * THT</span> secondes. Si le <span class="em">Moniteur</span> ne reçoit pas de jeton pendnat une telle période, il coupe l'anneau pendant un certain temps, puis réinitiliase l'anneau et envoie un jeton.</p>
                    <p>Plusieurs autres anomalies peuvent se produire dans un réseau Token Ring. Par exemple, une station pourrait capturer un jeton et s'éteindre avant d'avoir renvoyé le jeton. Une autre station pourrait avoir capturé le jeton, envoyé sa trame de données et s'éteindre avant de recevoir l'intégralité de sa trame de données. Dans ce cas, la chaîne de bits correspondant à la fin d'une trme resterait sur l'anneau sans être supprimée par son expéditeur. Plusieurs techniques sont définies dans [802.5] pour permettre au <span class="em">Moniteur</span> de gérer tous ces problèmes. Si malheureusement, le <span class="em">Moniteur</span> échoue, une autre station sera élue pour devenir le nouveau <span class="em">Moniteur</span>.</p>
                    <h4>6.1.3 Technologies de la couche de liaison de données :</h4>
                    <p>Dans cette section, nous examinons les caractéristiques clés de plusieurs technologies de la couche de liaison de données. Nous disutons plus en détail des technologies largement utilisées aujourd'hui. Une étude détaillée de toutes les technologies de la couche de liaison de données serait en dehors du cadre de ce livre.</p>
                    <h5>Le protocole point-à-point :</h5>
                    <p>De nombreuses couches de liaison de données point-à-point ont été développées, à partir des années 1960 [McFadyen1976]. Dans cette section, nous nous concentrons sur les protocoles qui sont souvent utilisés pour transporter des paquets IP entre des hôtes ou des routeurs directement connectés par une liaison point-à-point. Cette liaison peut être un câble physique dédié, une ligne louée à travers le réseau téléphonique ou une connexion à composition automatique avec des modems sur les deux hôtes communicants.</p>
                    <p>La première solution pour transporter des paquets IP sur une ligne série a été proposée dans la RFC 1055 et est connue sous le nom de <span class="html">Serial Line IP (SLIP)</span>. SLIP est une technique simple de bourrage de caractères appliquée aux paquets IP. SLIP définit deux caractères spéciaux : <span class="html">END</span> (décimal 192) et <span class="html">ESC</span> (décimal 219). <span class="em">END</span> apparaît au début et à la fin de chaque paquet IP transmis, et l'émetteur ajoute <span class="em">ESC</span> avant chaque caractère <span class="em">END</span> à l'intérieur de chaque paquet IP transmis. SLIP ne prend en charge que la transmission de paquets IP et suppose que les deux hôtes/routeurs communiquants ont été configurés manuellement avec les adresses IP de l'autre. SLIP était principalement utilisé sur des liens offrant une bande passante souvent inférieure à 20 Kbps. Sur un lien de si faible bande passante, l'envoi de 20 octets d'en-tête IP suivi de 20 octets d'en-tête TCP pour chaque segment TCP prend beaucoup de temps. Cela a initié le développement d'une famille de techniques de compression pour compresser efficacement les en-têtes TCP/IP. La première technique de compression d'en-tête proposée dans la RFC 1144 a été conçue pour exploiter la redondance entre plusieurs segments consécutifs appartenant à la même connexion TCP. Dans tous ces segments, les adresses IP et les numéros de port sont toujours les mêmes. De plus, des champs tels que les numéros de séquence et d'accusé de réception ne changent pas de manière aléatoire. La RFC 1144 a défini des techniques simples pour réduire la redondance trouvée dans les segments successifs. Le développement des techniques de compression des en-têtes a continué et des améliorations sont encore en cours de développement aujourd'hui (RFC 5795).</p>
                    <p>Alors que SLIP a été implémenté et utilisé dans certains environnements, il présentait plusieurs limitations discutées dans la RFC 1055. Le <span class="html">protocole point-à-point (PPP)</span> a été conçu peu de temps après et est spécifié dans la RFC 1548. PPP vise à prendre en charge IP et d'autres protocoles de couche réseau sur différents types de lignes série. PPP est en fait une famille de de trois protocoles qui sont utilisés ensemble :</p>
                    <ol>
                        <li>
                            <p>Le <span class="html">protocole point-à-point (PPP, pour Point-to-Point Protocol)</span> définit la technique d'encadrement pour transporter les paquets de la couche réseau.</p>
                        </li>
                        <li>
                            <p>Le <span class="html">protocole de contrôle de liaison (LCP, pour Link Control Protocol)</span> est utilisé pour négocier les options et authentifier la session en utilisant un nom d'utilisateur et un mot de passe ou d'autres types d'informations d'identification.</p>
                        </li>
                        <li>
                            <p>Le <span class="html">protocole de contrôle de réseau (NCP, pour Network Control Protocol)</span> est spécifique à chaque protocole de couche réseau. Il est utilisé pour négocier des options spécifiques à chaque protocole. Par exemple, le protocole NCP IPv4 (RFC 1548) peut négocier l'adresse IPv4 à utiliser et l'adresse IPv4 du serveur DNS. Le NCP IPv6 est défini dans la RFC 5072.</p>
                        </li>
                    </ol>
                    <p>Le protcole d'encadrement PPP (RFC 1662) s'est inspiré des protocoles de couche de liaison de donénes normalisés par l'ITU-T et l'ISO. Une trame PPP typique est composée des champs illustrée dans la figure ci-dessous. Une trame PPP commence par un octet de flag contenant <span class="em">01111110</span>. PPP peut utiliser le bit stuffing ou le character stuffing selon l'environnement dans lequel le protocole est utilisé. Les champs d'adresse et de contrôle sont présents pour des raisons de compatibilité ascendante. Le champ de protocole de 16 bits contient l'identifiant du protocole de couche réseau transporté dans la trame PPP. L'IANA gère le registre de tous les champs de protocole PPP attribués à l'adresse suivante <a href="http://www.iana.org/assignments/ppp-numbers" target="_blank">http://www.iana.org/assignments/ppp-numbers</a>. <span class="em">0x002d</span> est utilisé pour un paquet IPv4 compressé avec RFC 1144, tandis que <span class="em">0x002f</span> est utilisé pour un paquet IPv4 non compressé. <span class="em">0xc021</span> est utilisé par le <span class="em">protocole de contrôle de liaison (LCP)</span>, <span class="em">0xc023</span> est utilisé par le <span class="html">protocole d'authentification par mot de passe (PAP, pour Password Authentication Protocol)</span>. <span class="em">0x0057</span> est utilisé pour les paquets IPv6. PPP prend en charge des paquets de longueur variable, mais LCP peut négocier une longueur maximale de paquet. La trame PPP se termine par une <span class="html">séquence de contrôle de trame (FCS, pour Frame Check Sequence)</span>. Par défaut, il s'agit d'un CRC sur 16 bits, mais certaines implémentations peuvent négocier un CRC sur 32 bits. La trame se termine par le flag <span class="em">01111110</span>.</p>
                    <figure>
                        <img src="../images/format_trame_PPP.png" alt="">
                        <figcaption>Figure 6.17 : format de trame PPP</figcaption>
                    </figure>
                    <p>PPP a joué un rôle essentiel en permettant aux fournisseurs de services Internet de fournir un accès à distance via des modems à la fin des années 1990 et au début des années 2000. Les ISP exploitaient des banques de modems connectées au réseau téléphonique. Pour ces ISP, un problème clé était d'authentifier chaque utilisateur connecté via le réseau téléphonique. Cette authentification était effectuée en utilisant le <span class="html">protocole d'authentification extensible (EAP, pour Extensible Authentication Protocol)</span> défini dans le RFC 3748. EAP est un protocole simple, mais extensible, qui était initialement utilisé par les routeurs d'accès pour authentifier les utilisateurs connectés via des lignes de composition. Plusieurs méthodes d'authentification, allant des simples paires nom d'utilisateur/mot de passe à des schémas plus complexes, ont été définies et implémentées. Lorsque les ISP ont commencé à mettre à niveau leur infrastructure physique pour fournir un accès Internet via des <span class="html">lignes d'abonné numérique asymétriques (ADSL, Asymetric Digital Subscriber Lines)</span>, ils ont tenté de réutiliser leurs systèmes d'authentification (et de facturation) existants. Pour répondre à ces exigences, l'IETF a développé des spécifications permettant de transporter des trames PPP sur d'autres réseaux que les liaisons point-à-point pour lesquelles PPP a été conçu. De nos jours, la plupart des déploiements ADSL utilisent PPP sur ATM (RFC 2364) ou sur Ethernet (RFC 2516).</p>
                    <h5>Ethernet :</h5>
                    <p>Ethernet a été conçu dans les années 1970 au Palo Alto Research Center [Metcalfe1976]. Le premier prototype utilisait un câble coaxial en tant que support partagé et offrait une bande passante de 3 Mbps. Des informations supplémentaires sur l'histoire de la technologie Ethernet peuvent être trouvées sur <a href="http://ethernethistory.typepad.com/" target="_blank">http://ethernethistory.typepad.com/</a>. Ethernet a été amélioré au cours des années 1970 et 1980, et Digital Equipement, Intel et Xerox ont publié la première spécification officielle d'Ethernet [DIX]. Cette spécification définit plusieurs paramètres importants pour les réseaux Ethernet. La première décision a été de standardiser Ethernet commercial à 10 Mbps. La deuxième décision a été la durée du slot. Dans Ethernet, un long slot permet aux réseaux de couvrir une longue distance mais oblige l'hôte à utiliser une taille minimale de trame plus grande. Le compromis a été une durée de slot de 51,2 microsecondes, ce qui correspond à une taille minimale de trame de 64 octets.</p>
                    <p>La troisième décision concernait le format des trames. Le réseau Ethernet expérimental de 3 Mbps construit chez Xerox utilisait des trames courtes contenant des champs d'adresses source et destination de 8 bits, une indication de type de 16 bits, jusqu'à 554 octets de payload et un CRC de 16 bits. L'utilisation d'adresses de 8 bits convenait à un réseau expérimental, mais il était clairement trop petit pour des déploiements commerciaux. Bien que la spécification initiale d'Ethernet [DIX] ne permettait que jusqu'à 1024 hôtes sur un réseau Ethernet, elle recommandait également trois changements importants par rapport aux technologie de mise en réseau disponibles à l'époque. Le premier changement consistait à exiger que chaque hôte connecté à un réseau Ethernet ait une adresse de couche de liaison de données globalement unique. Jusque-là, les adresses de couche de liaison de données étaient configurées manuellement sur chaque hôte. [BP1981] allait à l'encontre de cet état de l'art et notait : <q class="em">Des procédures administratives spécifiques à l'installation sont également nécessaires pour attribuer des numéros aux hôtes sur un réseau. Si un hôte est déplacé d'un réseau à un autre, il peut être nécessaire de modifier son numéro d'hôte si son ancien numéro est utilisé sur le nouvel réseau. Cela est plus facile à dire qu'à faire, car chaque réseau doit avoir un administrateur qui doit enregistrer l'état en constante évolution du système (souvent sur un morceau de papier accroché au mir !). On prévoit qu'à l'avenir, dans les environnements de bureau, les emplacements des hôtes changeront aussi souvent ques les téléphones sont changés dans les bureaux d'aujourd'hui.</q> Le deuxième changement introduit par Ethernet consistait à coder chaque adresse comme un champ de 48 bits [DP1981]. Les adresses de 48 bits étaient énormes par rapport aux technologies de mise en réseau disponibles dans les années 1980, mais l'espace d'adressage étendu avait plusieurs avantages [DP1981], notamment la possibilité d'allouer de grands blocs d'adresses aux fabricants. Finalement, d'autres technologies LAN ont également opté pour des adresses de 48 bits [802]. Le troisième changement introduit par Ethernet était la définition des adresses de diffusion et de multidiffusion. Le besoin de multidiffusion Ethernet était prévu dans [DP1981] et grâce à la taille de l'espace d'adressage possible de réserver un grand bloc d'adresses de multidiffusion pour chaque fabricant.</p>
                    <p>Les adresses de couche de liaison de données utilisées dans les réseaux Ethernet sont souvent appelées <span class="html">adresses MAC</span>. Elles sont structurées comme indiqué dans la figure ci-dessous. Le premier bit de l'adresse indique si l'adresse identifie un adaptateur réseau ou un groupe multidiffusion. Les 24 bits supérieurs sont utilisés pour coder un <span class="html">identifiant unique d'organisation (OUI, pour Organisation Unique Identifier)</span>. Ce OUI identifie un bloc d'adresses qui a été attribué par le secrétariat responsable de l'unicité des adresses Ethernet à un fabricant. Initialement, les OUI étaient attribués par Xerox [BP1981]. Cependant, une fois qu'Ethernet est devenu une norme IEEE puis ISO, l'attribution des OUI a été transférée à l'IEEE. La liste de toutes les allocations OUI peut être consultée sur le site <a href="http://standards.ieee.org/regauth/oui/index.shtml" target="_blank">http://standards.ieee.org/regauth/oui/index.shtml</a>. Une fois qu'un fabricant a reçu un OUI, il peut concevoir et vendre des produits avec l'une des 16 millions d'adresses de ce bloc.</p>
                    <figure>
                        <img src="../images/format_adresse_ethernet_48_bits.jpg" alt="">
                        <figcaption>Figure 6.18 : format d'adresse Ethernet de 48 bits</figcaption>
                    </figure>
                    <p>La spécification originale d'Ethernet à 10 Mbps [DIX] définissait un format de trame simple où chaque trame est composée de cinq champs. La trame Ethernet commence par un préambule (non représenté dans la figure ci-dessous) utilisé par la couche physique du récepteur pour synchorniser son horloge avec celle de l'émetteur. Le premier champ de la trame est l'adressse de destination. Comme cette adresse est placée au début de la trame, une interface Ethernet peut rapidement vérifier si elle est le destinataire de la trame et, le cas échéant, annuler le traitement de la trame arrivante. Le deuxième champ est l'adresse source. Alors que l'adresse de destination peut être soit une adresse unicast, soit une adresse multidiffusion/broadcast, l'adresse source doit toujours être une adresse unicast. Le troisième champ est un entier de 16 bits qui indique le type de paquet de la couche réseau transporté dans la trame. Ce champ est souvent appelé <span class="html">Ether Type</span>. Les valeurs <span class="em">Ether Type</span> fréquemment utilisées comprennent <span class="em">0x0800</span> pour IPv4, <span class="em">0x86DD</span> pour IPv6 et <span class="em">0x806</span> pour le protocole de résolution d'adresse (ARP). La liste officielle de toutes les valeurs de type Ethernet assignées est disponible sur <a href="http://standards.ieee.org/regauth/ethertype/eth.txt" target="_blank">http://standards.ieee.org/regauth/ethertype/eth.txt</a>. Le lecteur attentif peut se demander pourquoi différents <span class="em">Ether Types</span> sont nécessaires pour IPv4 et IPv6 alors que l'en-tête IP contient déjà un champ de version qui peut être utilisé pour distinguer les paquets IPv4 et IPv6. Théoriquement, IPv4 et IPv6 pourraient utiliser le même <span class="em">Ether Type</span>. Malheureusement, les développeurs des premières implémentations IPv6 ont constaté que certains appareils ne vérifiaient pas le champ de version des paquets IPv4 qu'ils recevaient et analysaient les trames dont l'<span class="em">Ether Type</span> était défini  sur <span class="em">0x0800</span> comme des paquets IPv4. Envoyer des paquets IPv6 à de tels appareils aurait provoqué des perturbations. Pour éviter ce problème, l'IETF a décidé de demander une valeur <span class="em">Ether Type</span> distincte pour IPv6. Un tel choix est maintenant obligatoire selon la RFC 6274 (section 3.1), bien que nous puissions trouver un contre-exemple amusant dans la RFC 6214.</p>
                    <p>La quatrième partie de la trame Ethernet est le payload. La longueur minimale du payload est de 46 octets pour assurer une taille minimale de trame, y compris l'en-tête de 512 bits. Le payload Ethernet ne peut pas dépasser 1500 octets. Cette taille a été jugée raisonnable lorsque la première spécification Ethernet a été rédigée. À l'époque, Xerox utilisait son Ethernet expérimental à 3 Mbps qui offrait 554 octets de payload et la RFC 1122 exigeait un MTU minimum de 572 octets pour IPv4. 1500 octets étaient suffisamment grands pour répondre à ces besoins sans contraindre les adaptateurs réseau à contenir des mémoires excessivement grandes. De plus, des simulations et des études de mesure dans des réseaux Ethernet ont révélé que CSMA/CD était capable d'atteindre une utilisation très élevée. Cela est illsutré dans la figure ci-dessous, basée sur [SH1980], qui montre l'utilisation du canal obtenue dans des réseaux Ethernet contenant différents nombres d'hôtes envoyant des trames de différentes tailles.</p>
                    <figure>
                        <img src="../images/impact_longueur_trame_utilisation_chaine_maximum.png" alt="">
                        <figcaption>Figure 6.19 : Impact de la longueur de trame sur l'utilisation maximale du canal[SH1980]</figcaption>
                    </figure>
                    <p>Le dernier champ de la trame Ethernet est un code de redondance cyclique (CRC) de 32 bits. Ce CRC est capable de détecter un nombre beaucoup plus important d'erreurs de transmission que la somme de contrôle Internet utilisé par IP, UDP et TCP [SGP98]. Le format de la trame Ethernet est illustré ci-dessous.</p>
                    <figure>
                        <img src="../images/format_trame_DIX_ethernet.png" alt="">
                        <figcaption>Figure 6.20 : Format de trame Ethernet DIX</figcaption>
                    </figure>
                    <hr>
                    <p>Note : Où doit être situé le CRC dans une trame ?</p>
                    <p>Les couches de transport et de liaison de données choisissent généralement des stratégies différentes pour placer leurs CRC ou sommes de contrôle (checksums). Les protocoles de la couche de transport placent généralement leurs CRC ou sommes de contrôle dans l'en-tête du segment. Les protocoles de la couche de liaison de données placent parfois leur CRC dans l'en-tête de la trame, mais souvent dans une partie additionnelle à la fin de la trame, appelée <span class="html">trailer</span>. Ce choix reflète des hypothèses d'implémentation, mais influence également les performances [RFC 893]. Lorsque le CRC est placé dans le <span class="em">trailer</span>, comme dans Ethernet, la couche de liaison de données peut le calculer pendant la transmission de la trame et l'insérer à la fin de la transmission. Toutes les interfaces Ethernet utilisent cette optimisation aujourd'hui. Lorsque la somme de contrôle est placée dans l'en-tête, comme dans un segment TCP, il est impossible pour l'interface réseau de le calculer pendant la transmission du segment. Certaines interfaces réseau fournissent une assistance matérielle pour calculer la somme de contrôle TCP, mais cela est plus complexe que si la somme de contrôle TCP était placée dans le <span class="em">trailer</span>. Ces interfaces réseau calculent la somme de contrôle TCP pendnat le transfert d'un segment de la mémoire de l'hôte vers l'interface réseau [SH2004].</p>
                    <hr>
                    <p>Le format de trame Ethernet indiqué ci-dessus est spécifié dans [DIX]. C'est le format utilisé pour envoyer à la fois les paquets IPv4 [RFC 894] et IPv6 [RFC 2464]. Après la publication de [DIX], l'Institute of Electrical and Electronic Engineers (IEEE) a commencé à standardiser plusieurs technologies de réseau local (LAN). L'IEEE a travaillé sur plusieurs technologies LAN, commençant par Ethernet, Token Ring et Token Bus. Ces trois technologies étaient complètement différentes, mais elles ont toutes convenu d'utiliser les adresses MAC de 48 bits spécifiées initialement pour Ethernet [802]_. Lors du développement de sa norme Ethernet [802.3], le groupe de travail IEEE 802.3 a été confronté à un problème. Ethernet imposait une taille minimale de payload de 46 octets, tandis que certaines entreprises recherchaient une technologie LAN pouvant transporter de manière transparente des trames courtes ne contenant que quelques octets de payload. Une telle trame peut être envoyée par un hôte Ethernet en la remplissant pour s'assurer que le payload fait au moins 46 octets. Cependant, étant donné que l'en-tête Ethernet [DIX] ne contient pas de champ de longuueur, il est impossible pour le récepteur de déterminer combien d'octets utiles ont été placés dans le champ de payload. Pour résoudre ce problème, l'IEEE a décidé de remplacer le champ <span class="em">Type</span> de l'en-tête Ethernet [DIX] par un champ <span class="em">Length</span>. Heuresuement, l'IEEE a pu définir le format de trame [802.3] tout en assurant la compatibilité descendante avec le format de trame Ethernet [DIX]. L'astuce a été d'attribuer uniquement des valeurs supérieures à 1500 en tant que valeurs <span class="em">Ether Type</span>. Lorsqu'un hôte reçoit une trame, il peut déterminer son format en vérifiant le champ <span class="em">Ether Type/Length</span>. Une valeur inférieure à 1501 est clairement un indicateur de longueur et donc une trame [802.3]. Une valeur supérieure à 1501 ne peut être qu'un type et donc une trame [DIX]. Ce champ <span class="em">Length</span> contient le nombre d'octets utiles dans le payload de la trame. Le payload doit encore contenir au moins 48 octets, mais des octets de bourrage (padding bytes) sont ajouté par l'émetteur et supprimés par le récepteur. Afin d'ajouter le champ <span class="em">Length</span> sans modifier significativement le format de la trame, l'IEEE a dû supprimer le champ <span class="em">Type</span>. Sans ce champ, il est impossible pour un hôte récepteur d'identifier le type de paquet de la couche réseau à l'intérieur d'une trame reçue. Pour résoudre ce nouveau problème, L'IEEE a développé une nouvelle sous-couche appelée <span class="html">Logical Link Control</span> [802.2]. Plusieurs protocoles ont été dénis dans cette sous-couche. L'un d'entre eux fournissait une version légèrement différente du champ <span class="em">Type</span> du format de trame Ethernet original. Un autre contenait des accusés de réception et des retransmissions pour fournir un service fiable... En pratique, [802.2] n'est jamais utilisé pour prendre en charge IP dans les réseaux Ethernet. La figure ci-dessous montre le format officiel de trame [802.3].</p>
                    <figure>
                        <img src="../images/format_trame_ethernet_802.3.png" alt="">
                        <figcaption>Figure 6.21 : Format de trame Ethernet 802.3</figcaption>
                    </figure>
                    <hr>
                    <p>Note : Quel est le service Ethernet ?</p>
                    <p>Un réseau Ethernet offre un service non fiable et sans connexion. Il prend en charge trois modes de transmission : <span class="html">unicast</span>, <span class="html">multicast</span> et <span class="html">broadcast</span>. Bien que le service Ethernet soit théoriquement non fiable, un bon réseau Ethernet devrait, en pratique, fournir un service qui :</p>
                    <ul>
                        <li>
                            <p>Livre les trames à leur destination avec une très forte probabilité de réussite.</p>
                        </li>
                        <li>
                            <p>Ne réorganise pas les trames transmises.</p>
                        </li>
                    </ul>
                    <p>La première porpriété est une conséquence de l'utilisation du CSMA/CD. La deuxième propriété est une conséquence de l'organisation physique du réseau Ethernet en tant que bus partagé. Ces deux propriétés sont importantes et toutes les évolutions de la technologie Ethernet les ont préservées.</p>
                    <hr>
                    <p>Plusieurs couches physiques ont été définies pour les réseaux Ethernet. La première couche physique, généralement appelée 10Base5, offrait une vitresse de 10 Mbps sur un câble coaxial épais. Les caractéristiques du câble et des émetteurs-récepteurs utilisés à l'époque permettaient l'utilisation de segments d'une longueur de 500 mètres. Un réseau 10Base5 peut également inclure des répéteurs entre les segments.</p>
                    <p>La deuxième couche physique était 10Base2. Cette couche physique utilisait un câble coaxial fin qui était plus facile à installer que le câble 10Base5, mais sa longueur maximale ne pouvait pas dépasser 185 mètres. Une couche physique 10BaseF a égalemennt été définie pour transporter Ethernet sur des liaisons optiques point à point. Le principal changement apporté à la couche physique était la prise en charge des paires torsadées dans la spécification 10BaseT. Les câbles à paires torsadées sont traditionnellement utilisés pour prendre en charge le service téléphonique dans les immeubles de bureaux. La plupart des immeubles de bureaux sont aujourd'hui équipés d'un câblage structuré. Plusieurs câbles à paires torsadées sont installés entre chaque pièce et un placard de télécommunication central par bâtiment ou par étage dans les grands bâtiments. Ces placards de télécommunication servent de points de concentration pour le service téléphonique, mais aussi pour les LAN.</p>
                    <p>L'introduction des paires torsadées a entraîné deux changements majeurs dans Ethernet. Le premier changement concerne la topologie physique du réseau. Les réseaux 10Base2 et 10Base5 sont des bus partagés, où le câble à paires coaximal passe généralment par chaque pièce contenant un ordinateur connecté. Un réseau 10BaseT est un réseau en étoile. Tous les appareils connectés au réseau sont reliés à câble à paires torsadées qui se termine dans le placard de télécommunication. Du point de vue de la maintenance, il s'agit d'une amélioration majeure. Le câble est un point faible dans les réseaux 10Base2 et 10Base5. Tout dommage physique sur le câble rompt l'ensemble du réseau et lorsque cela se produit, l'administrateur réseau devait vérifier manuellement tout le câble pour détecter où il était endommagé. Avec 10BaseT, lorsqu'une paire torsadée est endommagée, seul l'appareil connecté à cette paire torsadée est affecté et cele n'affecte pas les autres appareils. Le deuxième changement majeur introduit par 10BaseT &tait qu'il était impossible de construire un réseau 10BaseT en connectant simplement toutes les paires torsadées ensemble. Toutes les paires torsad&es doivent connectées à un relais qui fonctionne dans la couche physique. Ce relais s'appelle un <span class="html">concentrateur Ethernet (ou Ethernet hub en anglais)</span>. Un concentrateur est donc un relais de couche physique qui reçoit un signal éléctrique sur l'une de ses interfaces, régénère le signal et le transmet sur toutes ses autres interfaces. Certains concentrateurs sont également capables de convertir le signal électrique d'une couche physique à un autre (par exemple, conversion de 10BaseT à 10Base2).</p>
                    <figure>
                        <img src="../images/hubs_ethernet_modele_reference.png" alt="">
                        <figcaption>Figure 6.22 : Concentrateurs Ethernet dans le modèle de référence</figcaption>
                    </figure>
                    <p>Les ordinateurs peuvent être directement connectés aux concentrateurs Ethernet. Les concentrateurs Ethernet eux-mêmes peuvent être connectés d'autres concentrateurs Ethernet eux-mêmes peuvent être connectés d'autres concentrateurs Ethernet pour construire un réseau plus large. Cependant, certaines directives importantes doivent être suivies lors de la construction d'un réseau complexe avec des concentrateurs. Premi!rement, la topologie du réseau doit être un arbre. Comme les concentrateurs sont des relais dans la couche physique, l'ajout d'un lien entre le <span class="html">Hub2</span> et le <span class="html">Hub3</span> dans le réseau ci-dessous créerait un raccourci électrique qui perturberait complètement le réseau. Cela implique qu'il ne peut y avoir aucune redondance dans un réseau basé sur des concentrateurs. Une défaillance d'un concentrateur ou d'un lien entre deux concentrateurs diviserait le réseau en deux réseaux isolés. Deuxièmement, comme les concentrateurs sont des relais dans la couche physique, des collisions peuvent se produire et doivent être gérées par CSMA/CD comme dans un réseau 10Base5. Cela implique que le délai maximum entre n'importe quelle paire d'appareils dans le réseau ne peut pas être supérieur à la durée d'une fente de 51,2 microsecondes. Si le délai est plus long, les collisions entre les trames courtes peuvent être correctement détectées. Cette contrainte limite la portée géographique des réseaux 10BaseT contenant des concentrateurs.</p>
                    <figure>
                        <img src="../images/reseau_ethernet_hierarchique_compose_hubs.png" alt="">
                        <figcaption>Figure 6.23 : Un réseau Ethernet hiérarchique composé de concentrateurs</figcaption>
                    </figure>
                    <p>À la fin des années 1980, 10 Mbps est devenu trop lent pour certaines applications et les fabricants de réseaux ont développé plusieurs technologies LAN offrant une bande passante plus élevée, telles que le LAN FDDI 100 Mbps utilisant des fibres optiques. Comme le développement de 10Base5, 10Base2 et 10BaseT avait montré qu'Ethernet pouvait être adapté à différentes couches physiques, plusieurs fabricants ont commencé à travailler sur Ethernet 100 Mbps et ont convaincu l'IEEE de standardiser cette nouvelle technologie initialement appelée <span class="html">Fast Ethernet</span>. <span class="em">Fast Ethernet</span> a été conçu en respectant deux contraintes. Premièrement, <span class="em">Fast Ethernet</span> devait prendre en charge les paires torsadées. Bien qu'il soit plus facile du point de vue de la couche physique de prendre en charge une bande passante plus élevée sur des câbles coaxiaux que sur des paires torsadées, les câbles coaxiaux étaient un cauchemar du point de vue du déploiement et de la maintenance. Deuxièmement, <span class="em">Fast Ethernet</span> devait être parfaitement compatible avec les Ethernet 10 Mbps existants pour permettre à la technologie <span class="em">Fast Ethernet</span> d'être utilisée initialement comme une technologie de backbone pour interconnecter des réseaux Ethernet 10 Mbps. Cela a obligé <span class="em">Fast Ethernet</span> à utiliser exactement le même format de trame que l'Ethernet 10 Mbps. Cela impliuait que la taille minimale de trame <span class="em">Fast Ethernet</span> reste à 512 bits. Pour préserver CSMA/CD avec cette taille minimale de trame et une vitesse de 100 Mbps au lieu de 10 Mbps, la durée de la fente temporelle a été réduite à 5,12 microsecondes.</p>
                    <p>L'évolution d'Ethernet ne s'est pas arrêtée. En 1998, l'IEEE a publié une première norme pour fournir Ethernet Gigabit sur fibres optiques. Plusieurs autres types de couches physiques ont été ajoutés par la suite. La norme Ethernet 10 Gigabit est apaprue en 2002. Les travaux sont en cours pour développer des normes Ethernet 40 Gigabit et 100 Gigabit, et certains réfléchissent déjà à Ethernet Terabit. Le tableau ci-dessous répertorie les principales normes Ethernet. Une liste plus détaillée peut être consultée sur <a href="http://en.wikipedia.org/wiki/Ethernet_physical_layer" target="_blank">http://en.wikipedia.org/wiki/Ethernet_physical_layer</a>.</p>
                    <table class="tableBalises" role="presentation">
                        <thead>
                            <tr>
                                <th>Standard</th>
                                <th>Commentaires</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>10Base5</td>
                                <td>Câble coaxial épais, 500 m</td>
                            </tr>
                            <tr>
                                <td>10Base2</td>
                                <td>Câble coaxial mince, 185 m</td>
                            </tr>
                            <tr>
                                <td>10BaseT</td>
                                <td>Deux paires de câble UTP de catégirue 3+, 10 Mbit/s</td>
                            </tr>
                            <tr>
                                <td>10Base-F</td>
                                <td>10 Mbit/s sur fibre optique</td>
                            </tr>
                            <tr>
                                <td>100Base-Tx</td>
                                <td>Câble UTP ou STP de catégorie 5, 100 m maximum</td>
                            </tr>
                            <tr>
                                <td>100Base-FX</td>
                                <td>Deux fibres optiques multimode, 2 km maximum</td>
                            </tr>
                            <tr>
                                <td>1000Base-CX</td>
                                <td>Deux paires de câble torsadé blindé, 25 m maximum</td>
                            </tr>
                            <tr>
                                <td>1000Base-SX</td>
                                <td>Deux fibres optiques multimode ou monomode avec lasers</td>
                            </tr>
                            <tr>
                                <td>10 Gbps</td>
                                <td>Fibre optique mais aussi câble UTP de catégorie 6</td>
                            </tr>
                            <tr>
                                <td>40-100 Gbps</td>
                                <td>Fibre optique (des expériences sont réalisées avec du cuivre)</td>
                            </tr>
                        </tbody>
                    </table>
                    <h5>Commutateurs Ethernet :</h5>
                    <p>Augmenter la bande passante de la couche physique, comme avec le <span class="em">Fast Ethernet</span>n'était qu'une des solutions pour améliorer les performances des LAN Ethernet. Une deuxième solution était de remplacer les hubs par des dispositifs plus intelligents. Comme les <span class="em">hubs Ethernet</span> fonctionnent au niveau physique et ne peuvent que régénrer le signal électrique pour étéendre la portée géographique du réseau. Du point de vue des performances, il serait plus intéressant d'avoir des dispositifs qui fonctionnent au niveau de la couche liaison de données, qui peuvent analyser l'adresse de destination de chaque trame et transférer les trames sélectivement sur le lien menant à la destination. Ces dispositifs sont généralement appelés <span class="html">commutateurs Ethernet (Ethernet switches en anglais)</span>. Les premiers relais Ethernet qui fonctionnaient au niveau de la couche de liaison de données étaient appelés des <span class="html">ponts (bridges)</span>. En pratique, la principale différence entre les commutateurs et les ponts est que les ponts étaient généralment implémentés en logiciel, tandis que les commutateurs sont des dispositifs matériels. Tout au long de ce texte, nous utilisons toujours le terme "commutateur" pour désigner un relais au niveau de la couche de liaison de données, mais vous pouvez encore rencontrer le mot "pont". Un commutateur Ethernet est un relais qui fonctionne au niveau de la couche de liaison de données, comme illustré dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/switchs_ethernet_modele_reference.png" alt="">
                        <figcaption>Figure 6.24 : Commutateurs Ethernet et modèle de référence</figcaption>
                    </figure>
                    <p>Un <span class="em">commutateur Ethernet</span> comprend le format des trames Ethernet et peut acheminer sélectivement les trames sur chaque interface. Pour cela, chaque <span class="em">commutateur Ethernet</span> maintient une <span class="html">table d'adresses MAC</span>. Cette table contient, pour chaque adresse MAC connue par le commutateur, l'identifiant du port du commutateur par lequel une trame envoyée vers cette adresse doit être transmise pour atteindre sa destination. Cela est illustré ci-dessous avec la <span class="em">table d'adresses MAC</span> du commutateur inférieur. Lorsque le commutateur reçoit une trame destinée à l'adresse <span class="em">B</span>, il la transmet sur son port <span class="em">Sud</span>. S'il reçoit une trame destinée à l'adresse <span class="em">D</span>, il la transmet uniquement sur son port <span class="em">Nord</span>.</p>
                    <figure>
                        <img src="../images/fonctionnement_switchs_ethernet.png" alt="">
                        <figcaption>Figure 6.25 : Fonctionnement des commutateurs Ethernet</figcaption>
                    </figure>
                    <p>L'un des points forts des réseaux Ethernet est que, grâce à l'utilisation des adresses MAC de 48 bits, un LAN Ethernet est prêt à l'emploi au niveau de la couche de liaison de données. Lorsque deux hôtes sont connectés au même segment ou hub, ils peuvent immédiatement échanger des trames Ethernet sans nécessiter de configuration. Il est important de conserver cette capacité plug and play également pour les commutateurs Ethernet. Cela implique que les commutateurs EThernet doivent être capables de construire leur table d'adresses MAC automatiquement, sans nécessiter de configuration manuelle. Cette configuration automatique est réalisée par l'algorithme d'<span class="html">apprentissage des adresses MAC</span> qui s'exécute sur chaque commutateur Ethernet. Cet algorithme extrait l'adresse source des trames reçues et mémorise le port par lequel une trame provennant de chaque adresse Ethernet source a été reçue. Ces informations sont insérées dans la table d'adresses MAC que le commutateur utilise pour acheminer les trames. Cela permet au commutateur d'apprendre automatiquement les ports qu'il peut utiliser pour atteindre chaque adresse de destination, à condition que cet hôte ait précédemment envoyé au moins une trame. Cela ne pose pas de problème car la plupart des protocoles de couche supérieure utilisent des acquittements à un certain niveau, de sorte même une imprimante Ethernet envoie également des trames Ethernet.</p>
                    <p>Le pseudo-code ci-dessous détaille la manière dont un commutateur Ethernet transfère les trames Ethernet. Il met d'abord à jour sa <span class="em">table d'adresses MAC</span> avec l'adresse source de la trame. La <span class="em">table d'adresses MAC</span> utilisée par certains commutateurs contient également un horodatage qui est mis à jour à chaque réception d'une trame de chaque adresse source connue. Cet horodatage est utilisé pour supprimer de la <span class="em">table d'adresses MAC</span> les entrées qui n'ont pas été actives au cours des dernières <span class="em">n</span> minutes. Cela limite la croissance de la <span class="em">table d'adresses MAC</span>, mais permet également aux hôtes de passer d'un port à un autre. Le commutateur utilise sa <span class="em">table d'adresses MAC</span> pour transférer la trame unicast reçue. S'il existe une entrée pour l'adresse de destination de la trame dans la <span class="em">table d'adresses MAC</span>, la trame est transférée sélectivement sur le port indiqué dans cette entrée. Sinon, le commutateur ne sait pas comment atteindre l'adresse de destination et doit transférer la trame sur tous ses ports sauf celui à partir duquel la trame a été reçue. Cela garantit que la trame atteindra sa destination, au prix de quelques transmissions inutiles. Ces transmissions inutiles ne dureront que jusqu'à ce que la destination ait enovyé sa première trame. Les trames multicast et de brodcast sont également transférées de manière similaire.</p>
<pre><code># Arrival of frame F on port P
# Table : MAC address table dictionary : addr->port
# Ports : list of all ports on the switch
src=F.SourceAddress
dst=F.DestinationAddress
Table[src]=P #src heard on port P
if isUnicast(dst) :
    if dst in Table:
        ForwardFrame(F,Table[dst])
    else:
        for o in Ports :
        if o!= P : ForwardFrame(F,o)
else:
    # multicast or broadcast destination
    for o in Ports :
        if o!= P : ForwardFrame(F,o)</code></pre>
                    <hr>
                    <p>Note : Problèmes de sécurité avec les concentrateurs (hubs) et les commutateurs (switchs) Ethernet :</p>
                    <p>Du point de vue de la sécurité, les concentrateurs Ethernet présentent les mêmes inconvénients que les anciens câbles coaxiaux. Un hôte connecté à un concentrateur peut capturer toutes les trames échangées entre deux hôtes connectés au même concentrateur. Les commutateurs Ethernet sont beaucoup plus sûrs de ce point de vue grâce au transfert sélectif : un hôte ne recevra généralement que les trames qui lui sont destinées, ainsi que les trames multicast, de broadcast et inconnues. Cependant, cela ne signifie pas que les commutateurs sont totalement sécurisés. Malheureusement, il existe des attaques contre les commutateurs Ethernet. Du point de vue de la sécurité, la <span class="em">table d'adresses MAC</span> est l'un des éléments fragiles d'un commutateur Ethernet. Cette table a une taille fixe. Certains commutateurs bas de gamme peuvent stocker quelques dizaines ou quelques centaines d'adresses, tandis que les commutateur haut de gamme peuvent stocker des dizaines de milliers d'adresses ou plus. Du point de vue de la sécurité, une ressource limitée peut être la cible d'attaques par déni de service. Malheureusement, de telles attaques sont également possibles sur les commutateurs Ethernet. Un hôte malveillant pourrait saturer la <span class="em">table d'adresses MAC</span> du commutateur en générant des milliers de trames avec des adresses source aléatoires. Une fois la table d'adresses MAC pleine, le commutateur doit diffuser toutes les trames qu'il reçoit. À ce stade, un attaquant recevra des trames unicast qui ne sont pas destinées à son adresse. L'attaque ARP discutée dans le chapitre précédent peut également se produire avec des commutateurs Ethernet [Vynecke2007]. Les commutateurs récents implémentent plusieurs types de défenses contre ces attaques, mais ils doivent être configurés avec soin par l'administrateur réseau. Voir [Vynecke2007] pour une discussion détaillée sur les problèmes de sécurité avec les commutateurs Ethernet.</p>
                    <hr>
                    <p>L'algorithme d'<span class="html">apprentissage des adresses MAC</span> combiné à l'algorithme de transfert fonctionnent bien dans un réseau en forme d'arbre comme celui illustré ci-dessus. Cependant, pour faire face aux pannes de liens et de commutateurs, les administrateurs réseau ajoutent souvent des liens redondants pour garantir que leur réseau reste connecté même après une défaillance. Prenons en compte ce qui se passe dans le réseau Ethernet illustré dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/swicths_ethernet_boucle.png" alt="">
                        <figcaption>Figure 6.26 : Commutateurs Ethernet en boucle</figcaption>
                    </figure>
                    <p>Lorsque tous les commutateurs démarrent, leur <span class="em">table d'adresses MAC</span> est vide. Supposons que l'hôte <span class="em">A</span> envoie une trame à destination de l'hôte <span class="em">C</span>. À la réception de cette trame, le switch1 met à jour sa <span class="em">table d'adresses MAC</span> pour se souvenir que l'adresse <span class="em">A</span> est accessible via son port <span class="em">Ouest</span>. Comme il n'y a pas d'entrée pour l'adresse <span class="em">C</span> dans la <span class="em">table d'adresses MAC</span> ou switch1, la trame est transférée à la fois vers le switch2 et le switch3. Lorsque le switch2 reçoit la trame, il met à jour sa <span class="em">table d'adresses MAC</span> pour l'adresse <span class="em">A</span> et transfère la trame vers l'hôte <span class="em">C</span> ainsi que vers le switch3. Le switch3 a donc reçu deux copies de la même trame. Comme le switch3 ne sait pas comment atteindre l'adresse de destination, il transfère la trame reçue du switch1 vers le switch2 et la trame reçue du switch2 vers le switch1. La trame unique envoyée par l'hôte <span class="em">A</span> sera continuellement dupliquée par les commutateurs jusqu'à ce que leur <span class="em">table d'adresses MAC</span> contienne une entrée pour l'adresse <span class="em">C</span>. Rapidement, toute la bande passante disponible des liens sera utilisée pour transférer toutes les copies de cette trame. Comme Ethernet ne contient pas de TTL ou de HopLimit, cette boucle ne s'arrêtera jamais.</p>
                    <p>L'algorithme d'<span class="em">apprentissage des adresses MAC</span> permet aux commutateurs d'être plug-and-play. Malheureusement, les boucles qui se produisent lorsque la topologie du réseau n'est pas un arbre sont un problème grave. Forcer les commutateurs à n'être utilisés que dans des réseaux en forme d'arbre comme les concentrateurs serait une limitation importante. Pour résoudre ce problème, les inventeurs des commutateurs Ethernet ont développé le <span class="html">Spanning Tree Protocol</span>. Ce protocole permet aux commutateurs de désactiver automatiquement les ports sur les commutateurs Ethernet pour s'assurer que le réseau ne contient aucun cycle qui pourrait provoquer une boucle infinie de trames.</p>
                    <h5>Le Spanning Tree Protocol (802.1d) :</h5>
                    <p>Le <span class="em">Spanning Tree Protocol (STP)</span>, proposé dans [Perlman1985], est un protocole distribué utilisé par les commutateurs pour réduire la topologie du réseau à un arbre couvrant, de soirt qu'il n'y ait aucun cycle dans la topologie. Par exemple, considérez le réseau illustré dans la figure ci-dessous. Dans cette figure, chaque ligne en gras correspond à un Ethernet auquel deux commutateurs Ethernet sont connectés. Ce réseau contient plusieurs cycles qui doivent être supprimés pour permettre aux commutateurs Ethernet utilisant l'algorithme d'apprentissage des adresses MAC d'échanger des trames.</p>
                    <figure>
                        <img src="../images/arbre_recouvrement_calcule_reseau_ethernet_commute.png" alt="">
                        <figcaption>Figure 6.27 : Arbre couvrant dans un réseau Ethernet commuté</figcaption>
                    </figure>
                    <p>Dans ce réseau, le protocole STP calculera l'arbre couvrant suivant. <span class="em">Switch1</span> sera la racine de l'arbre. Toutes les interfaces de <span class="em">Switch1</span>, <span class="em">Switch2</span> et <span class="Em">Switch7</span> font partie de l'arbre couvrant. Seule l'interface connecté à <span class="em">LANB</span> sera active sur <span class="em">Switch9</span>. <span class="em">LANH</span> ne sera desservi que par <span class="em">Switch7</span> et le port de <span class="em">Switch44</span> sur <span class="em">LANG</span> sera désactivé. Une trame émise depuis <span class="em">LANB</span> et destinée à <span class="em">LANA</span> sera transmise par <span class="em">Switch7</span> sur <span class="em">LANC</span>, puis par <span class="em">Switch1</span> sur <span class="em">LANE</span>, puis par <span class="em">Sitch44</span> sur <span class="em">LANF</span> et enfin par <span class="em">Switch2</span> sur <span class="em">LANA</span>.</p>
                    <p>Les commutateurs exécutant le <span class="em">Spanning Tree Protocol</span> échangent des <span class="html">BPDU (Bridge Protocol Data Units)</span>. Ces <span class="em">BPDU</span> sont toujours envoyés sous forme de trames avec une adresse MAC de destination réservée à l'adresse multicast <span class="HTML">ALL_BRIDGES</span>. Pour garantir l'unicité, les 48 bits inférieurs de l'identifiant sont définis sur l'adresse MAC unique allouée au commutateur par son fabricant. Les 16 bits supérieurs de l'identifiant du commutateur peuvent être configurés par l'administrateur réseau pour influencer la topologie de l'arbre couvrant. La valeur par défaut de ces bits supérieurs est 32768.</p>
                    <p>Les commutateurs échangent des <span class="em">BPU</span> pour construire l'arbre couvrant. De manière intuitive, l'arbre couvrant est construit en sélectionnant d'abord le commutateur avec le plus petit identifiant comme racine de l'arbre. Les branches de l'abre couvrant sont ensuite composées des chemins les plus courts permettant d'atteindre tous les commutateurs du réseau. Les <span class="em">BPDU</span> échangés par les commutateurs contiennent les informations suivantes :</p>
                    <ul>
                        <li>
                            <p>l'<span class="em">indentifiant</span> du commutateur racine (<span class="em">R</span>).</p>
                        </li>
                        <li>
                            <p>le <span class="em">coût</span> du plus court chemin entre le commutateur qui a envoyé le <span class="em">BPDU</span> et le commutateur racine (<span class="em">c</span>).</p>
                        </li>
                        <li>
                            <p>l'<span class="em">identifiant</span> du commutateur qui a envoyé le <span class="em">BPDU</span> (<span class="em">T</span>).</p>
                        </li>
                        <li>
                            <p>le <span class="em">numéro du port</span> du commutateur qui a envoyé le <span class="em">BPDU</span> (<span class="em">T</span>).</p>
                        </li>
                    </ul>
                    <p>Nous utiliserons la notation <span class="em">&lt;R,c,T,p&gt;</span> pour représenter un <span class="em">BDPU</span> dont l'<span class="em">identifiant racine</span> est <span class="em">R</span>, <span class="em">coût</span> est <span class="em">c</span> et qui a été envoyé sur le port <span class="em">p</span> du commutateur <span class="em">T</span>. La construction de l'arbre couvrant dépend d'une relation d'ordre entre les <span class="em">BPDU</span>. Cette relation d'ordre pourrait être implémentée par la fonction Python ci-dessous.</p>
<pre><code># returns True if bpdu b1 is better than bpdu b2
def better( b1, b2) :
    return ( (b1.R &lt; b2.R) or
            ( (b1.R==b2.R) and (b1.c&lt;b2.c) ) or
            ( (b1.R==b2.R) and (b1.c==b2.c) and (b1.T&lt;b2.T) ) or
            ( (b1.R==b2.R) and (b1.c==b2.c) and (b1.T==b2.T) and (b1.p&lt;b2.p) ) )
        </code></pre>
                    <p>En plus de l'<span class="em">identifiant</span> mentionné ci-dessus, l'administrateur réseau peut également configurer un <span class="em">coût</span> à associer à chaque port du commutateur. Habituellement, le <span class="em">coût</span> d'un port dépend de sa bande passante et la norme [802.1d] recommande les valeurs suivantes. Bien sûr, l'administrateur réseau peut choisir d'autres valeurs. Nous utiliserons la notation <span class="html">cost[p]</span> pour indiquer le <span class="em">coût</span> associé au port <span class="em">p</span> dans cette section.</p>
                    <table class="tableBalises" role="presentation">
                        <thead>
                            <tr>
                                <th>Bande passante</th>
                                <th>Coût</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>10 Mbps</td>
                                <td>2000000</td>
                            </tr>
                            <tr>
                                <td>100 Mbps</td>
                                <td>200000</td>
                            </tr>
                            <tr>
                                <td>1 Gbps</td>
                                <td>20000</td>
                            </tr>
                            <tr>
                                <td>10 Gbps</td>
                                <td>2000</td>
                            </tr>
                            <tr>
                                <td>100 Gbps</td>
                                <td>200</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>Le <span class="em">Spanning Tree Protocol</span> utilise sa propre terminologie que nous illustrons dans la figure ci-dessus. Un port de commutateur peut se trouver dans trois états différents : <span class="html">Racine (Root en anglais)</span>, <span class="html">Désigné (Designated en anglais)</span> et <span class="html">Bloqué (Blocked en anglais)</span>. Tous les ports du commutateur <span class="em">racine</span> se trouvent dans l'état <span class="em">Désigné</span>. L'état des ports des autres commutateurs est déterminé en fonction des <span class="em">BPDU</span> reçus sur chaque port.</p>
                    <p>Le <span class="em">Spanning Tree Protocol</span> utilise la relation d'ordre pour construire l'arbre de recouvrement. Chaque commutateur écoute les <span class="em">BPDU</span> sur ses ports. Lorsque le <span class="html">BPDU=&lt;R,c,T,p</span> est reçu sur le port <span class="em">q</span>, le commutateur calcule le <span class="em">vecteur de priorité</span> du port : <span class="html">V[q]=&lt;R,c+cost[q],T,p,q&gt;</span>, où <span class="em">cost[q]</span> est le <span class="em">coût</span> associé au port sur lequel le <span class="em">BPDU</span> a été reçu. Le commutateur stocke dans une table le dernier <span class="em">vecteur de priorité</span> reçu sur chaque port. Ensuite, le commutateur compare son propre <span class="em">identifiant</span> avec le plus petit <span class="em">identifant racine</span> stocké dans cette table. Si son propre <span class="em">identifiant</span> est plus petit, alors le commutateur est la racine de l'arbre de recouvrement et se trouve, par définition, à une distance <span class="em">0</span> de la racine. Le <span class="em">BPDU</span> du commutateur est alors <span class="em">&lt;R,0,R,p&gt;</span>, où <span class="em">R</span> est l'<span class="em">identifiant</span> du commutateur et <span class="em">p</span> sera défini comme le numéro du port sur lequel le <span class="em">BPDU</span> est envoyé. Sinon, le commutateur choisit le meilleur <span class="em">vecteur de priorité</span> de sa table, <span class="em">bv&lt;R,c,T,p&gt;</span>. Le port sur lequel ce meilleur vecteur de priorité a été appris est le port du commutateur le plus proche du commutateur <span class="em">acine</span>. Ce port devient le port <span class="em">Racine</span> du commutateur. Il n'y a qu'un seul port <span class="em">Racine</span> par commutateur. Le commutateur peut alors calculer son <span class="em">BPDU</span> comme <span class="em">BDPU=&lt;R,c,S,p&gt;</span>, où <span class="em">R</span> est l'<span class="em">identifiant racine</span>, <span class="em">c</span> est le <span class="em">coût</span> du meilleur vecteur de priorité, <span class="em">S</span> est l'identifiant du commutateur et <span class="em">p</span> sera remplacé par le numero du port sur lequel le <span class="em">BPDU</span> sera envoyé. Le commutateur peut ensuite déterminer l'état de tous ses ports en compareant son propre <span class="em">BPDU</span> avec le vecteur de priorité reçu sur chaaque port. Si le <span class="em">BPDU</span> du commutateur est meilleure que le vecteur de priorité de ce port, le port devient un port <span class="em">Désigné</span>. Sinon, le port devient un port <span class="em">Bloqué</span>.</p>
                    <p>L'état de chaque port est important lorsqu'on considère la transmission des <span class="em">BPDU</span>. Le switch racine envoie régulièrement son propre <span class="em">BPDU</span> sur tous ses ports (<span class="em">Désigné</span>). Ce <span class="em">BPDU</span> est reçu sur le port <span class="em">Racine</span> de tous les switchs qui sont directement connectés au <span class="em">switch racine</span>. Chacun de ces switchs calcule son propre <span class="em">BPDU</span> et envoie ce <span class="em">BPDU</span> sur tous ses ports <span class="em">Désigné</span>. Ces <span class="em">BPDU</span> sont ensuire reçus sur le port <span class="em">Racine</span> des switchs en aval, qui calculent alors leur propre <span class="em">BPDU</span>, etc. Lorsque la topologie du réseau est stable, les switchs envoient leur propre <span class="em">BPDU</span> sur tous leurs ports <span class="em">Désigné</span>, une fois qu'ils ont reçu un <span class="em">BPDU</span> sur leur port <span class="em">Racine</span>. Aucun <span class="em">BPDU</span> n'est envoyé sur un port <span class="em">Bloqué</span>. Les switchs écoutent les <span class="em">BPDU</span> sur leurs ports <span class="em">Bloqué</span> et <span class="em">Désigné</span>, mais aucun <span class="em">BPDU</span> ne devrait être reçu sur ces ports lorsque la topologie est stable. L'utilisation des ports pour les <span class="em">BPDU</span> et les trames de données est résumée dans le tableau ci-dessous.</p>
                    <table class="tableBalises" role="presentation">
                        <thead>
                            <tr>
                                <th>État du port</th>
                                <th>Reçoit les BPDU</th>
                                <th>Envoie des BPDU</th>
                                <th>Traite les trames de données</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Bloqué</td>
                                <td>oui</td>
                                <td>non</td>
                                <td>non</td>
                            </tr>
                            <tr>
                                <td>Racine</td>
                                <td>oui</td>
                                <td>non</td>
                                <td>oui</td>
                            </tr>
                            <tr>
                                <td>Désigné</td>
                                <td>oui</td>
                                <td>oui</td>
                                <td>oui</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>Pour illustrer le fonctionnement du Spanning Tree Protocol, considérons la topologie réseau simple dans la figure ci-dessous.</p>
                    <figure>
                        <img src="../images/arbre_recouvrement_calcule_reseau_ethernet_commute.png" alt="">
                        <figcaption>Figure 6.28 : Un arbre de recouvrement simple calculé dans un réseau Ethernet commuté</figcaption>
                    </figure>
                    <p>Supposons que <span class="em">Switch4</span> soit le premier à démarrer. Il envoie son propre <span class="em">BPDU</span> <span class="em">&lt;4,0,4,?&gt;</span> sur ses deux ports. Lorsque <span class="em">Switch1</span> démarre, il envoie le <span class="em">BPDU=&lt;1,0,1,1&gt;</span>. Ce <span class="em">BDPU</span> est reçu par <span class="em">Switch4</span>, qui met à jour sa table et calcule un nouveau <span class="em">&lt;1,3,4,?&gt;</span>. Le port 1 de <span class="em">Switch4</span> devient le port <span class="em">Racine</span> tandis que son deuxième port reste dans l'état <span class="em">Désigné</span>.</p>
                    <p>Supposons maintenant que <span class="em">Switch9</span> démarre et reçoive immédiatement le<span class="em">BPDU</span> de <span class="em">Switch1</span> sur le port 1. Ce <span class="em">BPDU</span> est envoyé sur le port 2 de <span class="em">Switch9</span> et atteint <span class="em">Switch4</span>. <span class="Em">Switch4</span> compare le vecteur de priorité construit à partir de ce <span class="em">BPDU</span> (c'est-à-dire <span class="em">&lt;1,2,8,2&gt;</span>) et remarque qu'il est meilleur que le <span class="em">BPDU</span> de <span class="em">Switch4</span> = <span class="em">&lt;1,3,4,2&gt;</span>. Ainsi, le port 2 devient un port <span class="em">Bloqué</span> sur <span class="em">Switch4</span>.</p>
                    <p>Pendant le calcul de l'arbre de recouvrement, les commutateurs rejettent tous les trames de données reçues, car à ce moment-là, la topologie du réseau n'est pas garantie sans boucle. Une fois que la topologie est stable pendant un certain temps, les commutateurs recommencent à utiliser l'algorithme d'<span class="em">apprentissage des adresses MAC</span> pour transférer les trames de données. Seuls les ports <span class="em">Racine</span> et <span class="em">Désigné</span> sont utilisés pour transférer les trames de données. Les commutateurs rejettent toutes les trames de données reçues sur leurs ports <span class="em">Bloqué</span> et ne transmettent jamais de trames sur ces ports.</p>
                    <p>Les commutateurs, les ports et les liaisons peuvent échouer dans un réseau Ethernet commuté. Lorsqu'une panne se produit, les commutateurs doivent être enmesure de calculer l'arbre de recouvrement pour récupérer de la panne. Le <span class="em">Spanning Tree Protocol</span> s'appuie sur des transmissions régulières des <span class="em">BPDU</span> pour détecter ces pannes. Un <span class="em">BPDU</span> contient deux champs supllémentaires : l'<span class="em">âge</span> du <span class="em">BPDU</span> et l'<span class="em">âge maximum</span>. L'<span class="Em">âge</span> contient la durée écoulée depuis que le commutateur racine a initialement émis le <span class="em">BPDU</span>. Le commutateur racine envoie son <span class="em">BPDU</span> avec un <span class="em">âge</span> de zéro et chaque commutateur qui calcule son propre <span class="em">BPDU</span> incrémente son <span class="em">âge</span> de un. L'<span class="em">âge</span> des <span class="em">BPDU</span> stockés dans la table d'un commutateur est également incrémenté toutes les secondes. Un <span class="em">BPDU</span> expire lorsque son <span class="em">âge</span> atteint l'<span class="em">âge maximum</span>. Lorsque le réseau est stable, cela ne se produit pas car les <span class="em">BPDU</span> sont régulièrement envoyés par le commutateur <span class="em">racine</span> et les commutateurs en aval. Cependant, si le commutateur <span class="em">racine</span> échoue ou si le réseau est partitionné, les BPDU expireront et les commutateurs recalculeront leur propre <span class="em">BPDU</span> et redémarreront le <span class="em">Spanning Tree Protocol</span>. Une fois qu'un changement de topologie a été détecté, la transmission des trames de données s'arrête car la topologie n'est pas garantie sans boucle. Des détails supplémentaires sur la réaction aux pannes peuvent être trouvés dans [802.1d].</p>
                    <h5>LAN virtuels :</h5>
                    <p>Un autre avantage important des commutateurs Ethernet est la possibilité de créer des réseaux locaux virtuels (VLAN, pour Virtual Local Networks). Un VLAN peut être défini comme un <span class="em">ensemble de ports connectés à un ou plusieurs commutateurs Ethernet</span>. Un commutateur peut prendre en charge plusieurs VLAN et exécuter un algorithme d'apprentissage des adresses MAC pour chaque VLAN. Lorsqu'un commutateur reçoit une trame avec une destination inconnue ou muticast, il la transmet sur tous les ports appartenant au même VLAN, mais pas sur les ports apaprtenant à d'autres VLAN. De même, lorsqu'un commutateur apprend une adresse source sur un port, il l'associe au VLAN de ce port et utilise cette information uniquement lors de la transmission de trames sur ce VLAN.</p>
                    <p>La figure ci-dessous illustre un réseau Ethernet commuté avec trois VLAN. <span class="em">VLAN2</span> et <span class="em">VLAN3</span> ne nécessitent qu'une configuration locale du commutateur <span class="em">S1</span>. L'hôte <span class="em">C</span> peut échanger des trames avec l'hôte <span class="em">D</span>, mais pas avec des hôtes qui se trouvent en dehors de son VLAN. <span class="em">VLAN1</span> est plus complexe car il y a des ports de ce VLAN sur plusieurs commutateurs. Pour prendre en charge de tels VLAN, la configuration locale n'est plus suffisante. Lorsqu'un commutateur reçoit une trame d'un autre commutateur, il doit être en mesure de déterminer le VLAN à partir duquel la trame a été émise afin d'utiliser la table d'adresses MAC correcte pour transmettre la trame. Cela est réalisé en attribuant un identifiant à chaque VLAN et en plaçant cet identifiant dans les en-têtes des trames échangées entre les commutateurs.</p>
                    <figure>
                        <img src="../images/VLAN_reseau_ethernet_commute.png" alt="">
                        <figcaption>Figure 6.29 : Réseaux locaux virtuels dans un réseau Ethernet commuté</figcaption>
                    </figure>
                    <p>L'IEEE a défini dans la norme [802.1q] un en-tête spécial pour coder les identifiants VLAN. Cet en-tête de 32 bits comprend un champ VLAN de 20 bits qui contient l'identifiant VLAN de chaque trame. Le format de l'en-tête [802.1q] est décrit ci-dessous.</p>
                    <figure>
                        <img src="../images/format_entete_802.1q.png" alt="">
                        <figcaption>Figure 6.30 : Format de l'en-tête 802.1q</figcaption>
                    </figure>
                    <p>L'en-tête [802.1q] est inséré immédiatement après l'adresse MAC source dans la trame Ethernet (c'est-à-dire avant le champ Ether Type). La taille maximale de la trame est augmentée de 4 octets. Il est encodé sur 32 bits et contient quatre champs. L'identifiant du protocole de balisage (TPI, pour Tag Protocol Identifier) est réglé sur <span class="em">0x8100</span> pour permettre au récepteur de détecter la présence de cet en-tête suppélmentaire. Le <span class="html">point de code de priorité (PCP, pour Priority Code Point)</span> est un champ de trois bits qui est utilisé pour prendre en charge différentes priorités de transmission pour la trame. La valeur <span class="em">0</span> est la plus basee priorité et la valeur <span class="em">7</span> la plus élevée. Les trames avec une priorité plus élevée peuvent s'attendre à être transmises plus tôt que les trames avant une priorité plus basse. Le bit <span class="em">C</span> est utilisé pour la compatibilité entre les réseaux Ethernet et Token Ring. Les 12 derniers bits de l'en-tête 802.1q contiennent l'identifiant VLAN. La valeur <span class="em">0</span> indique que la trame n'appartient à aucun VLAN tandis que la valeur <span class="em">0xEEE</span> est réservée. Cela implique que 4094 identifiants VLAN différents peuvent être utilisés dans un réseau Ethernet.</p>
                    <h5>Réseaux sans fil 802.11 :</h5>
                    <p>Le spectre radio est une ressource limitée qui doit être partagée par tous. Pendant la majeure partie du XXe siècle, les gouvernements et les organisations internationales ont réglementé la majeure partie du spectre radio. Cette réglementation contrôle l'utilisation du specre radio afin de garantir l'absence d'interférences entre différents utilisateurs. Une entreprise qui souhaite utiliser une plage de fréquences dans une région donnée doit demander une licence auprès de l'organisme de régulation. La plupart des régulateurs facturent des frais d'utilisation du spectre radio et certains gouvernements ont encouragé la concurrence entre les entreprises qui soumissionnent pour la même fréquence afin d'augmenter les redevances de licence.</p>
                    <p>Dans les années 1970, après les premières expériences avec ALOHANet, l'intérêt pour les réseaux sans fil a augmenté. De nombreuses expérimentations ont été réalisées sur l'ARPANet et en dehors. L'une de ces expériences a été le premier téléphone mobile, qui a été développé et testé en 1973. Ce téléphone mobile expérimental a été le point de dépat des premiers téléphones mobiles analogiques de première génération. Étant donné la demande croissante de téléphones mobiles, il était clair que la technologie de téléphone analogique n'était pas suffisante pour prendre en charge un grand nombre d'utilisateurs. Pour prendre en charge davantage d'utilisateurs et de nouveaux services, des chercheurs de plusieurs pays ont travaillé sur le développement de téléphones mobile numériques. En 1987, plusieurs pays européens ont décidé de développer les normes d'un système de téléphonie cellulaire commun à travers l'Europe : le <span class="html">Global System for Mobile Communications (GSM)</span>. Depuis lors, les normes ont évolué et plus de trois milliards d'utilisateurs sont connectés aux réseaux GSM aujourd'hui.</p>
                    <p>Alors que la plupart des plages de fréquences du spectre radio sont réservées à des applications spécifiques et nécessitent une licence spéciale, il existe quelques exceptions. Ces exceptions sont connues sous le nom de bandes radio <span class="html">industrielles, scientifiques et médicales (ISM, pour Industrial, Scientific and Medical)</span>. Ces bandes peuvent être utilisées pour des applications industrielles, scientifiques et médicales sans nécessiter de licence de la part de l'organisme de régulation. Par exemple, certains modèles radiocommandés utilisent la bande ISM de 27 MHz et certains téléphones sans fil fonctionnent dans la bande ISM de 915 MHz. En 1985, la bande de 2400-2500 GHz a été ajoutée à la liste des bandes ISM. Cette plage de fréquences correspond aux fréquences émises par les fours à mico-ondes. Le partage de cette bande avec des applications sous licence aurait probablement causé des interférences, étant donné le grand nombre de fours à micro-ondes utilés. Malgré le rsique d'interférences avec les fours à micro-ondes, l'ouverture de la plage de 2400-2500 GHz a permis à l'industrie des réseaux de développer plusieurs techniques de réseaux sans fil pour permettre aux ordinateurs d'échanger des données sans utiliser de câbles. Dans cette section, nous discutons plus en détail la plus populaire d'entre elles, à savoir la famille de réseaux sans fil WiFi [802.11]. D'autres techniques de réseaux sans fil telles que <span class="em">Bluetooth</span> ou <span class="em">HiperLAN</span> utilisent la même plage de fréquences.</p>
                    <p>Aujourd'hui, le WiFi est une technologie de réseau sans fil très populaire. Il existe plusieurs centaines de millions d'appareils WiFi. Le développement de cette technologie a commencé à la fin des années 1980 avec le réseau sans fil propriétaire <span class="em">WaveLAN</span>. <span class="em">WaveLAN</span> fonctionnait à 2 Mbps et utilisait différentes bandes de fréquences dans différentes régions du monde. Au début des années 1990, l'IEEE a créé le groupe de travail 802.11 pour standardiser une famille de technologies de réseau sans fil. Ce groupe de travail a été très prolifique et a produit plusieurs normes de réseaux sans fil qui utilisent différentes plages de fréquences et différentes couches physiques. Le tableau ci-dessous résume les principales normes 802.11.</p>
                    <table role="presentation" class="tableBalises">
                        <thead>
                            <tr>
                                <th>Norme</th>
                                <th>Fréquence</th>
                                <th>Débit typique</th>
                                <th>Bande passante maximale</th>
                                <th>Portée (m) intérieur/extérieur</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>802.11</td>
                                <td>2.4 GHz</td>
                                <td>0.9 Mbps</td>
                                <td>2 Mbps</td>
                                <td>20/100</td>
                            </tr>
                            <tr>
                                <td>802.11a</td>
                                <td>5 GHz</td>
                                <td>23 Mbps</td>
                                <td>54 Mbps</td>
                                <td>35/120</td>
                            </tr>
                            <tr>
                                <td>802.11b</td>
                                <td>2.4 GHz</td>
                                <td>4.3 Mbps</td>
                                <td>11 Mbps</td>
                                <td>38/140</td>
                            </tr>
                            <tr>
                                <td>802.1g</td>
                                <td>2.4 GHz</td>
                                <td>19 Mbps</td>
                                <td>54 Mbps</td>
                                <td>38/140</td>
                            </tr>
                            <tr>
                                <td>802.11n</td>
                                <td>2.4/5 GHz</td>
                                <td>74 Mbps</td>
                                <td>150 Mbps</td>
                                <td>70/250</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>Lors du développement de sa famille de normes, le groupe de travail IEEE 802.11 a adopté une approche similaire au groupe de travail IEEE 802.3 qui a développé divers types de couches physiques pour les réseaux Ethernet. Les réseaux 802.11 utilisent la technique d'accès au support CSMA/CA décrite précédemment et ils supposent tous la même architecture et utilisent le même format de trame.</p>
                    <p>L'architecture des réseaux WiFi est légèrement différente de celle des réseaux locaux que nous avons discutée jusqu'à présent. En pratique, il existe deux types principaux de réseaux WiFi : les réseaux <span class="em">indépendants</span> ou <span class="em">ad hoc</span> et les réseaux d'<span class="em">infrastructure</span>. Le groupe de tavail 802.11 a défini l'<span class="html">ensemble de services de base (BSS, pour Basic Service Set)</span> comme un groupe d'appareils qui communiquent entre eux. Nous continuons à utiliser le terme de réseau pour désigner un ensemble d'appareils qui communiquent. Un réseau <span class="em">indépendant</span> ou <span class="em">ad hoc</span> est composé d'un ensemble d'appareils qui communiquent entre eux. Ces appareils jouent le même rôle et le réseau <span class="em">ad hoc</span> n'est généralement pas connécté à Internet. Les réseaux <span class="em">ad hoc</span> sont utilisés, par exemple, lorsque quelques ordinateurs portables ont besoin d'échanger des informations ou pour connecter un ordinateur à une imprimante WiFi.</p>
                    <figure>
                        <img src="../images/reseau_independant_adhoc_802.11.png" alt="">
                        <figcaption>Figure 6.31 : Un réseau indépendant ou ad hoc 802.11</figcaption>
                    </figure>
                    <p>La plupart des réseaux WiFi sont des réseaux d'<span class="em">infrastructure</span>. Un réseau d'<span class="em">infrastructure</span> contient un ou plusieurs poinst d'accès qui sont rattachés à un réseau local fixe (généralement un réseau Ethernet) qui est connecté à d'autres réseaux tels que l'Internet. La figure ci-dessous montre un tel réseau avec deux points d"accès et quatre appareils WiFi. Chaque appareil WiFi est associé à un point d'accès et utilise ce point d'accès comme relais pour échanger des trames avec les appareils associés à un autre point d'accès ou accessibles via le LAN.</p>
                    <figure>
                        <img src="../images/reseau_infrastructure_802.11.png" alt="">
                        <figcaption>Figure 6.32 : Un réseau d'infrastructure 802.11</figcaption>
                    </figure>
                    <p>Un point d'accès 802.11 est un relais qui opère dans la couche de liaison de données, tout comme les commutateurs. La figure ci-dessous représente les couches du modèle de référence qui sont impliquées lorsqu'un hôte WiFi communique avec un hôte connecté à un réseau Ethernet via un point d'accès.</p>
                    <figure>
                        <img src="../images/point_acces_802.11.png" alt="">
                        <figcaption>Figure 6.33 : Un point d'accès 802.11</figcaption>
                    </figure>
                    <p>Les appareils 802.11 échangent des trames de longueur variable, qui ont une structure légèrement différente de la simple structure de trame dans les réseaux Ethernet. Nous examinons les parties clés des trames 802.11. Des détails supplémentaires peuvent être trouvés dans [802.11] et [Gast2002]. Une trame 802.11 contient un en-tête de longueur fixe, un payload de longueur variable qui peut contenir jusqu'à 2324 octets de données utilisateur et un CRC de 32 bits. Bien que le payload puisse contenir jusqu'à 2324 octets, la plupart des déploiements 802.11 utilisent une taille de payload maximale de 1500 octets car ils sont utilisés dans des réseaux d'<span class="em">infrastructure</span> rattachés à des réseaux Ethernet. Une trame de données 802.11 est présentée ci-dessous.</p>
                    <figure>
                        <img src="../images/format_trame_donnees_802.11.png" alt="">
                        <figcaption>Figure 6.34 : Format de la trame de données 802.11</figcaption>
                    </figure>
                    <p>La première partie de l'en-tête 802.11 est le champ de 16 bits <span class="html">Frame Control</span>. Ce champ contient des indicateurs qui indiquent le type de trame (trame de données, RTS/CTS, accusé de réception, trames de gestion, etc.), si la trame est envoyée vers ou depuis un LAN fixe, etc. [802.11]. La <span class="html">Duration</span> est un champ de 16 bits qui est utilisé pour réserver le canal de transmission. Dans les trames de données, le champ <span class="html">Duration</span> est généralement réglé sur le temps nécessaire pour transmettre une trame d'accusé de réception après un délai SIFS. Notez que le champ <span class="em">Duration</span> doit être réglé sur zéro dans les trames de multidiffusion et de diffusion. Comme ces trames ne sont pas acquittées, il n'est pas nécessaire de réserver le canal de transmission après leur transmission. Le champ <span class="html">Sequence Control</span> contient un numéro de séquence de 12 bits qui est incrémenté pour chaque trame de données.</p>
                    <p>Le lecteur attentif aura peut-être remarqué que les trames de données 802.11 contiennent trois champs d'adresse de 48 bits. En réalité, le format de trame [802.11] contient un quatrième champ d'adresse facultatif. Cette quatrième adresse est utilisée uniquement lorsque un réseau sans fil 802.11 est utilisé pour interconnecter des ponts rattachés à deux réseaux locaux classiques. Cela est surprenant par rapport à d'autres protocoles des couches réseau et liaison de données dont les en-têtes ne contiennent qu'une adresse source et une adresse de destination. La nécessité d'une troisième adresse dans l'en-tête 802.11 vient des réseaux d'<span class="em">infrastructure</span>. Dans un tel réseau, les trames sont généralemnt échangées entre les routeurs et les serveurs rattachés au LAN et les appareils WiFi rattachés à l'un des points d'accès. Le rôle des trois champs d'adresse est spécifié par des indicateurs dans le champ <span class="em">Frame Control</span>.</p>
                    <p>Lorsqu'une trame est envoyée depuis un appareil WiFi vers un serveur rattaché au même LAN que le point d'accès, la première adresse de la trame est réglée sur l'adresse MAC du point d'accès, la deuxième adresse est réglée sur l'adresse MAC de l'appareil WiFi source et la troisième adresse est l'adresse de destination finale sur le LAN. Lorsque le serveur répond, il envoie une trame Ethernet dont l'adresse source est son adresse MAC et l'adresse de destination est l'appareil MAC de l'appareil WiFi. Cette trame est capturée par le point d'accès qui convertit l'en-tête Ethernet en en-tête de trame 802.11. La trame 802.11 envoyée par le point d'accès contient trois adresses : la première adresse est l'adresse MAC de l'appareil WiFi de destination, la deuxième adresse est l'adresse MAC du point d'accès et la troisième adresse MAC du serveur qui a envoyé la trame.</p>
                    <p>Les trames de contrôle 802.11 sont plus simples que les trames de données. Elles contiennent un champ <span class="em">Frame Control</span>, un champ <span class="em">Duration</span> et une ou deux adresses. Les trames d'accusé de réception sont très petites. Elles ne contiennent que l'adresse de destination de l'accusé de réception. Il n'y a pas d'adresse source ni de champ <span class="em">Sequence Control</span> dans les trames d'accusé de réception. Cela s'explique par le fait que la trame d'accusé de réception peut facilement être associée à la trame précédente qu'elle accuse réception. En effet, chaque trame de données unicast contient un champ <span class="em">Duration</span> qui est utilisé pour réserver le canal de transmission afin de garantir qu'aucune collision n'affectera la trame d'accusé de réception. Le champ <span class="em">Sequence Control</span> est principalement utilisé par le récepteur pour supprimer les trames en double. Les trames en double sont détectées de la manière suivante. Chaque trame de données contient un champ <span class="em">Sequence Control</span> de séquence de 12 bits et le champ <span class="em">Frame Control</span> contient le drapeau <span class="html">Retry</span> qui est activé lorsqu'une trame est transmise. Chaque récepteur 802.11 stocke le numéro de séquence le plus récent reçu de chaque adresse source dans les trames dont le drapeau <span class="em">Retry</span> est réinitialisé. Lors de la réception d'une trame avec le drapeau <span class="em">Retry</span> est réinitialisé. Lors de la réception d'une trame avec le drapeau <span class="em">Retry</span> activé, le récepteur vérifie son numéro de séquence pour déterminer s'il s'agit d'une trame en double ou non.</p>
                    <figure>
                        <img src="../images/trames_ACK_CTS_IEEE_802.11.png" alt="">
                        <figcaption>Figure 6.35 : Trames ACK et CTS IEEE 802.11</figcaption>
                    </figure>
                    <p>Les trames RTS/CTS de 802.11 sont utilisées pour réserver le canal de transmission afin de transmettre une trame de données et son accusé de réception. Les trames RTS contiennent une durée, ainsi que les adresses de l'émetteur et du récepteur. Le champ <span class="em">Duration</span> de la trame RTS indique la durée de la réservation complète (c'est-à-dire le temps nécessaire pour transmettre la trame CTS, la trame de données, les accusés de réception et les délais SIFS requis). La trame CTS a le même format que la trame d'accusé de réception.</p>
                    <figure>
                        <img src="../images/format_trame_RTS_IEEE_802.11.png" alt="">
                        <figcaption>Figure 6.36 : Format de la trame RTS IEEE 802.11</figcaption>
                    </figure>
                    <hr>
                    <p>Note : Le service 802.11 :</p>
                    <p>Malgré l'utilisation des accusés de réception, la couche 802.11 ne fournit qu'un service sans connexion et non fiable, similaire aux réseaux Ethernet qui n'utilisent pas les accusés de réception. Les accusés de réception de 802.11 sont utilisés pour minimiser la probabilité de duplication de trames. Ils ne garantissent pas que toutes les trames seront correctement reçues par leurs destinataires. Tout comme Ethernet, les réseaux 802.11 offrent une probabilité élevée de livraison réussie des trames, et non une garantie. De plus, il convient de noter que les réseaux 802.11 n'utilisent pas d'accusés de réception poyr les trames de multidiffusion et de diffusion. Cela implique qu'en pratique, de telles trames sont plus susceptibles de subir des erreurs de transmission que les trames de diffusion unicast.</p>
                    <hr>
                    <p>En plus des trames de données et de contrôle que nous avons brièvement décrites ci-dessus, les réseaux 802.11 utilisent plusieurs types de trames de gestion. Ces trames de gestion sont utilisées à des fins diverses. Nous décrivons brièvement certains de ces types de trames ci-dessous. Une discussion détaillée peut être trouvée dans [802.11] et [Gast2002].</p>
                    <p>Un premier type de trames de gestion est constitué des <span class="html">trames balises (beacon frames en anglais)</span>. Ces trames sont diffusées régulièrement par les points d'accès. Chaque <span class="em">trame balise</span> contient des informations sur les capacités du point d'accès (par exemple, les débits de transmission 802.11 pris en charge) et une <span class="html">identité de l'ensemble de services (Service Set Identity, SSID)</span>. Le SSID est une chaîne ASCII terminée par un caractère nul pouvant contenir jusqu'à 32 caractères. Un point d'accès peut prendre en charge plusieurs SSID et les annoncer dans les <span class="em">trames balises</span>. Un point d'accès peut également choisir de rester silencieux et de ne pas diffuser de <span class="em">trames balises</span>. Dans ce cas, les stations WiFi peuvent envoyer des <span class="html">trames de requête de sonde (Probe request frames en anglais)</span> pour obliger les points d'accès disponibles à renvoyer une <span class="html">trame de répojnse à la sonde (Probe response frame)</span>.</p>
                    <hr>
                    <p>Note : IP sur 802.11 :</p>
                    <p>Deux types de schémas d'encapsulation ont été définis pour prendre en charge IP dans les réseaux Ethernet : le schéma d'encapsulation d'origine, construit au-dessus du format Ethernet DIX, est défini dans la RFC 894, et un deuxième schéma d'encapsulation, construit au-dessus du protocole LLC/SNAP [802.2], est défini dans la RFC 1042. Dans les réseaux 802.11, la situation est plus simple et seule l'encapsulation RFC 1042 esr utilisée. En pratique, cette encapsulation ajoute 6 octets à l'en-tête 8°2.11. Les quatre premiers octets correspondent à l'en-tête LLC/SNAP. Ils sont suivis des deux octets du champ Ethernet Type (<span class="em">0x800</span> pour IP et <span class="em">0x806</span> pour ARP). La figure ci-dessous montre un paquet IP encapsulé dans une trame 802.11.</p>
                    <figure>
                        <img src="../images/IP_IEEE_802.11.png" alt="">
                        <figcaption>Figure 6.37 : IP sur IEEE 802.11</figcaption>
                    </figure>
                    <hr>
                    <p>La deuxième utilisation importante des trames de gestion est de permettre à une station WiFi de s'asseoir à un point d'accès. Lorsqu'une station WiFi démarre, elle écoute les <span class="em">trames balises</span> pour trouver les SSID disponibles. Pour être autorisée à envoyer et recevoir des trames via un point d'accès, une station WiFi doit être associée à ce point d'accès. Si le point d'accès n'utilise aucun mécanisme de sécurité pour sécuriser la transmission sans fil, la station WiFi envoie simplement une <span class="html">trame de demanque d'association (Association request frame en anglais)</span> à son point d'accès préféré (généralement le point d'accès avec le signal radio le plus fort qu'elle reçoit). Cette trame contient certains paramètres choisis par la station WiFi et le SSID auquel elle souhaite se connecter. Le point d'accès répond avec une <span class="html">trame de réponse d'association (Association response frame en anglais)</span> s'il accepte la station WiFi.</p>
                    <h4>6.1.4 Résumé :</h4>
                    <p>Dans ce chapitre, nous avons d'abord expliqué les principes de la couche de liaison de données. Il existe deux types de couches de liaison de données : celles utilisées sur des liens point à point et celles utilisées sur des LAN. Sur les liens point à point, la couche de liaison de données doit au moins fournir une technique de cadrage, mais certains protocoles de couche de liaison de données incluent également des mécanismes de fiabilité tels que ceux utilisés dans la couche transport. Nous avons décrit le protocole point à point (Point-to-Point Protocol) qui est souvent utilisé sur les liens point à point dans Internet.</p>
                    <p>Les LAN posent un problème différent car plusieurs dispositifs partagent le même canal de transmission. Dans ce cas, un algorithme MAC est nécessaire pour réguler l'accès au canal de transmission, car chaque fois que deux dispositifs transmettent en même temps, une collusion se prodyit et aucun de ces trames peut être décodé par leurs destinataires. Il existe deux familles d'algorithmes MAC. Les algorithmes MAC statistiques ou optimistes réduisent la probabilité de collisions mais ne les évitent pas complètement. Avec de tels algorithmes, lorsq'une collision se produit, les trames en collision doivent être retransmises. Nous avons décrit le fonctionnement des algorithmes MAC ALOHA, CSMA, CMSA/CD et CSMA/CA. Les algorithmes MAC déterministes ou pessimistes évient toutes les collisions. Nous avons décrit le MAC Token Ring où les stations échangent un jeton pour réguler l'accès au canal de transmission.</p>
                    <p>Enfin, nous avons décrit plus en détail deux technologies de LAN  réussies : Ethernet et WiFi. Ethernet est maintenant la technologie LAN de facto. Nous avons analysé l'évolution d'Ethernet, y compris le fonctionnement des concentrateurs (hubs) et des commutateurs (switchs). Nous avons également décrit le Spanning Tree Protocol qui doit être utilisé lorsque les commutateurs sont interconnectés. Ces dernières années, le WiFi est devenu la technologie sans fil de facto à la maison et dans les entreprises. Nous avons expliqué le fonctionnement des réseaux WiFi et décrit les principales trames 802.11.</p>
                    <h4>6.1.5 Exercices :</h4>
                    <ol>
                        <li>
                            <p>L'hôte H0 effectue un ping vers son homologue H1 dans le réseau représenté dans la figure ci-dessous. Expliquez précisément ce qui s'est passé dans le réseau depuis son démarrage.</p>
                            
                        </li>
                        <li>
                            <p>Considérons le réseau commuté illustré dans la figure ci-dessous. Quel est l'arbre de recouvrement qui sera calculé par le protocole 802.1d dans ce réseau en supposant que tous les liens ont un coût unitaire ? Indiquez l'état de chaque port.</p>
                            <figure>
                                <img src="../images/petit_reseau_compose_switchs_ethernet.png" alt="">
                                <figcaption>Figure 6.38 : Un petit réseau composé de commutateurs Ethernet</figcaption>
                            </figure>
                        </li>
                        <li>
                            <p>Considéez le réseau commuté illustré dans la figure ci-dessus. Dans ce réseau, supposons que le LAN entre les commutateurs <span class="em">3</span> et <span class="em">12</span> échoue. Comment les commutateurs doivent-ils mettre à jour leurs tables de prots/adresses après la défaillance du lien ?</p>
                        </li>
                        <li>
                            <p>Dans le réseau représenté dans la figure ci-dessous, l'hôte H0 effectue un traceroute vers son homologue H1 (désigné par son nom) à travers un réseau composé de commutateurs et de routeurs. Expliquez précisément les trames, paquets et segments échangés depuis le démarrage du réseau. Vous pouvez attribuer des adresses si nécessaire.</p>
                            <figure>
                                <img src="../images/h0_traceroute_h1_reseau_compose_switchs_routeurs.png" alt="">
                                <figcaption>Figure 6.39 : L'hôte H0 effectue un traceroute vers son homologue H1 à travers un réseau composé de commutateurs et de routeurs</figcaption>
                            </figure>
                        </li>
                        <li>
                            <p>De nombreux réseaux d'entreprise sont organisés avec un ensemble d'appareils de backbones interconnectés en utilisant un maillage complet de liens comme indiqué dans la figure ci-dessous. Dans ce réseau, quels osnt les avantages et les inconvénients d'utiliser des commutateurs Ethernet et des routeurs IP exécutant OSPF ?</p>
                            <figure>
                                <img src="../images/reseau_backbone_entreprise_classique.png" alt="">
                                <figcaption>Figure 6.40 : Un réseau de backbone d'entreprise typique</figcaption>
                            </figure>
                        </li>
                        <li>
                            <p>Dans le réseau représenté dans la figure ci-dessous, est-ce que l'hôte H0 peut communiquer avec H1 et vice-versa ? Expliquez. Ajoutez ce dont vous avez besoin dans le réseau pour leur permettre de communiquer.</p>
                            <figure>
                                <img src="../images/h0_communique_h1.png" alt="">
                                <figcaption>Figure 6.41 : Est-ce que H0 et H1 peuvent communiquer ?</figcaption>
                            </figure>
                        </li>
                        <li>
                            <p>Considérez le réseau représenté dans la figure ci-dessous.</p>
                            <figure>
                                <img src="../images/lien_entre_S0_S1_utilise.png" alt="">
                                <figcaption>Figure 6.42 : Est-ce que le lien entre les commutateurs S0 et S1 sera jamais utilisé</figcaption>
                            </figure>
                        </li>
                        <li>
                            <p>La plupart des commutateurs Ethernet commerciaux sont capables d'exécuter le Spanning Tree Protocol indépendamment sur chaque VLAN. Quels sont les avantages d'utiliser des arbres de recouvrement par VLAN ?</p>
                        </li>
                    </ol>
                </article>
                <article>
                    <h2 id="annexes">Partie 7 : Annexes :</h2>
                    <h3>7.1 Glossaire :</h3>
                    <dl>
                        <dt>AIMD (Additive Increase, Multiplicative Decrease)</dt>
                        <dd>Addition additive, diminution multiplicative. Un algorithme d'adaptation de débit utilisé notamment par TCP où un hôte augmente de manière additive son débit de transmission lorsque le réseau n'est pas congestionné et diminue de manière multiplicative lorsqu'une congestion est détectée.</dd>
                        <dt>anycast</dt>
                        <dd>un mode de transmission où une information est envoyée depuis une source vers un récepteur qui appartient à un groupe spécifié.</dd>
                        <dt>API (Application Programming Interface)</dt>
                        <dd>Interface de programmation d'application.</dd>
                        <dt>ARP (Address Resolution Protocol)</dt>
                        <dd>Le protocole de résolution d'adresse est un protocole utilisé par les dispositifs IPv4 pour obtenir l'adresse de la couche de liaison de données correspondant à une adresse IPv4 dans le LAN. L'ARP est défni dans la RFC 826.</dd>
                        <dt>ARPANET</dt>
                        <dd>Le réseau Advanced Research Project Agency (ARPA) est un réseau construit par des scientifiques des réseaux aux États-Unis avec un financement de l'ARPA du ministère de la Défense américain. L'ARPANET est considéré comme le précurseur de l'Internet d'aujourd'hui.</dd>
                        <dt>ascii (American Standard Code for Information Interchange)</dt>
                        <dd>Le code américain normalisé pour l'échnage d'information est un schéma de codage des caractères qui définit une représentation binaire pour les caractères. La table ASCII contient à la fois des caractères imprimables et des caractères de contrôle. Les caractères ASCII étaient codés sur 7 bits et ne contenaient que les caractères nécessaires à l'écriture du texte en anglais. D'autres jeux de caractères tels qu'Unicode ont été développés ultérieursement pour prendre en charge toutes les langues écrites.</dd>
                        <dt>ASN.1 (Abstract Syntax Notation One)</dt>
                        <dd>La notation de syntaxe abstraite unifiée a été conçue par l'ISO et l'ITU-T. Il s'agit d'une notation standard et flexible qui peut être utilisée pour décrire des structures de données permettant de représenter, d'encoder, de transmettre et de décoder des données entre des applications. Elle a été conçue pour être utilisée dans la couche de présentation du modèle de référence OSI, mais elle est maintenant utilisée dans d'autres protocoles tels que <span class="em">SNMP</span>.</dd>
                        <dt>ATM (Asynchronous Transfer Mode)</dt>
                        <dd>Mode transfert asynchrone.</dd>
                        <dt>BGP (Border Gateway Protocol)</dt>
                        <dd>Le protocole de passerelle frontière est le protocole de routage inter-domaines utilisé dans l'Internet mondial.</dd>
                        <dt>BNF (Backus-Naur Form)</dt>
                        <dd>Une forme de Backus-Naur est une méthode formelle pour décrire un langage en utilisant des règles syntaxiques et lexicales. Les BNF sont fréquemment utilisées pour définir des langages de programmation, mais aussi pour définir les messages échangés entre les applications en réseau. La RFC 5234 explique comment une BNF doit être écrite pour spécifier un protocole Internet.</dd>
                        <dt>broadcast</dt>
                        <dd>un mode de transmission où une même information est envoyée à tous les noeuds du réseau.</dd>
                        <dt>CIDR (Classless Inter-Domain Routing)</dt>
                        <dd>Routage inter-domaines sans classe est l'architecture actuelle d'allocation d'adresses pour IPv4. Elle a été définie dans les RFC 1518 et RFC 4632.</dd>
                        <dt>ligne commutée (dial-up line en anglais)</dt>
                        <dd>Un synonyme d'une ligne téléphonique régulière, c'est-à-dire une ligne qui peut être utilisée pour composer n'importe quel numéro de téléphone.</dd>
                        <dt>DNS (Domain Name System)</dt>
                        <dd>
                            <p>Le système de noms de domaine est une base de données distribuée qui permet de faire correspondre des noms de domaine à des adresses IP.</p>
                            <p>Le système de noms de domaine est défini dans RFC 1035.</p>
                            <p>Le système de noms de domaine est une base de données distribuée qui peut être interrogée par les hôtes pour faire correspondre des noms à des noms à des adresses IP.</p>
                        </dd>
                        <dt>eBGP</dt>
                        <dd>Une session eBGP est une session BGP entre deux routeurs directement connectés qui appartiennent à deux systèmes autonomes différents. Également appelée session BGP externe.</dd>
                        <dt>EGP (Exterior Gateway Protocol)</dt>
                        <dd>Protocole de passerelle extérieure. Synonyme de protocole de routage interdomaine.</dd>
                        <dt>EIGRP (Enhanced Interior Gateway Routing Protocol)</dt>
                        <dd>Le protocole de routage amélioré à l'intérieur d'un domaine est un protocole de routage intradomaine propriétaire souvent utilisé dans les réseaux d'entreprise. EIGRP utilise l'algorithme DUAL décrit dans [Garcia1993].</dd>
                        <dt>trame (frame en anglais)</dt>
                        <dd>Une trame est l'unité de transfert d'informations dans la couche de liaison de données.</dd>
                        <dt>Frame-Relay</dt>
                        <dd>Une technologie de mise en réseau étendue utilisant des circuits déployés par les opérateurs de télécommunications.</dd>
                        <dt>ftp (File Transfer Protocol)</dt>
                        <dd>Le protocole de transfert de fichiers défini dans la RFC 959 était le protocole de facto pour l'échange de fichiers sur Internet avant l'adoption généralisée de HTTP (RFC 2616).</dd>
                        <dt>hosts.txt</dt>
                        <dd>Un fihcier qui contenait initialement la liste de tous les hôtes Internet avec leur adresse IPv4. À mesure que le réseau s'est développé, ce fichier a été remplacé par le DNS, mais chaque hôte conserve toujours un petit fichier hosts.txt qui peut être utilisé lorsque le DNS n'est pas disponible.</dd>
                        <dt>HTML (HyperText Markup Language)</dt>
                        <dd>Le langage de balisage hypertexte spécifie la structure et la syntaxe des documents échangés sur le World Wide Web. HTML est géré par le groupe de travail HTML du W3C.</dd>
                        <dt>HTTTP (HyperText Transport Protocol)</dt>
                        <dd>Le protocole de transport hypertexte est défini dans la RFC 2616.</dd>
                        <dt>hub</dt>
                        <dd>Un concentrateur fonctionnant dans la couche physique</dd>
                        <dt>IANA (Internet Assigned Numbers Authority)</dt>
                        <dd>L'Autorité pour l'attribution des numéros sur Internet est responsable de la coordination de la racine du DNS, de l'adressage IP et d'autres ressouces de protocole Internet.</dd>
                        <dt>iBGP</dt>
                        <dd>Une session iBGP est une session BGP entre deux routeurs appartenant au même système autonome. Également appelée session BGP interne.</dd>
                        <dt>ICAN (Internet Corporation for Assigned Names and Numbers)</dt>
                        <dd>La Société pour l'attribution des noms de domaine et des numéros sur Internet coordonne l'attribution des noms de domaines, des adresses IP et des numéros AS, ainsi que les paramètres de protocole. Elle coordonne également le fonctionnement et l'évolution des serveurs racine du DNS.</dd>
                        <dt>IETF (Internet Engineering Task Force)</dt>
                        <dd>Le groupe de travail sur l'ingénierie de l'Internet est une organisation à but non lucratig qui développe les normes des protocoles utilisés sur Internet. L'IETF couvre principalement les couches de transport et de réseau. Plusieurs protocoles de couche applicative sont également normalisés au sein de l'IETF. Les travaux au sein de l'IETF sont organisés en groupes de travail. La majeure partie du travail est réalisée par échange d'e-mails et il y a trois réunions de l'IETF chaque année. La participation est ouverte à tous. Voir <a href="http://www.ietf.org/" target="_blank">http://www.ietf.org/</a>.</dd>
                        <dt>IGP (Interior Gateway Protocol)</dt>
                        <dd>Protocole de passerelle intérieure. Synonyme de protocoel de routage intradomaine.</dd>
                        <dt>IGRP (Interior Gateway Routing Protocol)</dt>
                        <dd>Le protocole de routage intérieur est un protocole de routage intradomaine propriétaire qui utilise le vecteur de distance. IGRP prend en charge plusieurs métriques pour chaque route, mais a été remplacé par EIGRP.</dd>
                        <dt>IMAP (Internet Message Access Protocol)</dt>
                        <dd>Le protocole d'accès aux messages Internet, défini dans la RFC 3501, est un protocole de niveau application qui permet à un client d'accéder et de manipuler les e-mails stockés sur un serveur. Avec IMAP, les messages électroniques restent sur le serveur et ne sont pas téléchargés sur le client.</dd>
                        <dt>Internet</dt>
                        <dd>Un internet public est un inter-réseau, c'est-à-dire un réseau composé de différents réseaux qui utilisent IPv4 ou IPv6. L'Internet est un inter-réseau très populaire, mais d'autres inter-réseaux ont été utilisés dans le passé.</dd>
                        <dt>Interrogation inverse (inverse query en anglais)</dt>
                        <dd>Pour les serveurs DNS et les résolveurs, une interrogation inverse est une requête pour le nom de domaine correspondant à une adresse IP donnée.</dd>
                        <dt>IP (Internet Protocol)</dt>
                        <dd>Protocole Internet est le terme générique pour le protocole de la couche réseau dans la suite de protocoles TCP/IP. IPv4 est largement utilisé aujourd'hui et IPv6 est censé remplacer IPv4.</dd>
                        <dt>IPv4</dt>
                        <dd>C'est la version 4 du protocole Internet, le protocole de couche réseau sans connexion utilisé dans la plupart de l'Internet aujourd'hui. Les adresses IPv4 sont codées sur 32 bits.</dd>
                        <dt>IPv6</dt>
                        <dd>C'est la version 6 du protocole Internet, le protocole de couche réseau sans connexion qui est destiné à remplacer IPv4. Les adresses IPv6 sont codées sur 128 bits.</dd>
                        <dt>IS-IS (Intermediate System-Intermediate System)</dt>
                        <dd>Un routage intradomaine à état de liens qui était initialement défini pour le protocole CLNP de l'ISO, mais qui a été étendu pour prendre en charge IPv4 et IPv6. IS-IS est souvent utilisé dans les réseaux des ISP. Il est défini dans[ISO10589].</dd>
                        <dt>ISN (Initial Sequence Number)</dt>
                        <dd>Le numéro de séquence initial d'une connexion TCP, choisi par le client (ou le serveur) et placé dans le segment <span class="em">SYN</span> (ou <span class="em">SYN+ACK</span>) lors de l'établissement de la connexion TCP.</dd>
                        <dt>ISO (International Standardization Organisation)</dt>
                        <dd>C'est une agence des Nations Unies basée à Genève qui développe des normes dans différents domaines. Au sein de l'ISO, des représentants des pays votent pour approuver ou rejeter les normes. La majeure partie du travail de développement des normes ISO est effectuée au sein de groupes de travail d'experts. Des informations supplémentaires sur l'ISO peuvent être obtenues sur <a href="http://www.iso.int/" target="_blank">http://www.iso.int/</a>.</dd>
                        <dt>ISO-3166</dt>
                        <dd>Une norme <span class="em">ISO</span> qui définit des codes pour représenter les pays et leurs subdivisions. Voir <a href="http://www.iso.org/iso/country_codes.htm" target="_blank">http://www.iso.org/iso/country_codes.htm</a>.</dd>
                        <dt>ISP (Internet Service Provider)</dt>
                        <dd>Un fournisseur de services Internet, c'est-à-dire un réseau qui fournit un accès à Internet à ses clients.</dd>
                        <dt>ITU (International Telecommunication Union)</dt>
                        <dd>C'est une agence des Nations Unies dont le but est de développer des normes pour l'industrue des télécommunications. Elle a été initialement créée pour standardiser le système téléphonique de base, mais s'est ensuite étendue aux réseaux de données. Les travaux au sein de l'ITU sont principalement réalisés par des spécialistes des réseaux issus de l'industrie des télécommunications (opérateurs et fournisseurs). Voir <a href="http://www.itu.int/" target="_blank">http://www.itu.int/</a> pour plus d'informations.</dd>
                        <dt>IXP (Internet eXchange Point)</dt>
                        <dd>Un lieu où des routeurs appartenant à différents domaines sont connectés au même LAN pour établir des sessions de peering et échanger des paquets. Voir <a href="http://www.euro-ix.net/" target="_blank">http://www.euro-ix.net/</a> ou <a href="http://en.wikipedia.org/wiki/List_of_Internet_exchange_points_by_size" target="_blank">http://en.wikipedia.org/wiki/List_of_Internet_exchange_points_by_size</a> pour une liste partielle des IXP.</dd>
                        <dt>LAN (Local Area Network)</dt>
                        <dd>Un réseau local.</dd>
                        <dt>Leased line</dt>
                        <dd>Une ligne téléphonique disponible en permanence entre deux points.</dd>
                        <dt>MAN (Metropolitan Area Network)</dt>
                        <dd>Un réseau métropolitain.</dd>
                        <dt>MIME (Multipurpose Internet Mail Extensions)</dt>
                        <dd>C'est un ensemble d'extensions au format des messages électroniques, défini dans la RFC 2045, qui permettent d'utiliser des caractères non-ASCII à l'intérieur de messages électroniques. Un message MIME peut être composé de plusieurs parties ayant chacune un format différent.</dd>
                        <dt>Document MIME</dt>
                        <dd>C'est un document encodé en utilisant le format MIME.</dd>
                        <dt>minicomputer</dt>
                        <dd>Un minicomputer est un système multi-utilisateurs qui était généralement utilisé dans les années 1960/1970 pour servir des départements. Consultez l'article correspondant sur Wikipedia pour plus d'informations : <a href="http://en.wikipedia.org/wiki/Minicomputer" target="_blank">http://en.wikipedia.org/wiki/Minicomputer</a>.</dd>
                        <dt>modem</dt>
                        <dd>Un modem (modulateur-démodulateur) est un dispositif qui encode (respectivement décode) des informations numériques en modulant (respectivement démodulant) un signal analogique. Les modems sont fréquemment utilisés pour transmettre des informations numériques via des lignes téléphoniques et des liaisons radio. Consultez <a href="http://en.wikipedia.org/wiki/Modem" target="_blank">http://en.wikipedia.org/wiki/Modem</a> pour un aperçu des différents types de modems.</dd>
                        <dt>MSS</dt>
                        <dd>Une option TCP utilisée pour une entité TCP dans les segments SYN pour indiquer la taille maximale de segment qu'elle peut recevoir.</dd>
                        <dt>multicast</dt>
                        <dd>Un mode de transmission où une information est envoyée efficacement à tous les récepteurs appartenant à un groupe donné.</dd>
                        <dt>nameserver</dt>
                        <dd>Un serveur qui implémente le protocole DNS et peut répondre aux requêtes pour les noms situés dans son propre domaine.</dd>
                        <dt>NAT (Network Address Translator)</dt>
                        <dd>Un traducteur d'adresses réseau est un élément intermédiaire qui traduit les paquets IP.</dd>
                        <dt>NBMA (Non Broadcast Mode Multiple Access Network)</dt>
                        <dd>Un réseau d'accès multiple en mode non diffusé est un sous-réseau qui prend en charge plusieurs hôtes/routeurs mais ne fournit pas de moyen efficace d'envoyer des trames de diffusion à tous les appareils connectés au sous-réseau. Les sous-réseaux ATM en sont un exemple.</dd>
                        <dt>L'ordre des octets réseau (network-byte order)</dt>
                        <dd>Le protocole Internet permet de transporter des séquences d'octets. Ces séquences d'octets sont suffisantes pour transporter des caractères ASCII. L'ordre des octets réseau fait référence à l'encodage Big-Endian pour les entiers de 16 et 32 bits. Voir <a href="http://en.wikipedia.org/wiki/Endianness" target="_blank">http://en.wikipedia.org/wiki/Endianness</a> pour plus d'informations sur l'endianness.</dd>
                        <dt>NFS (Network File System)</dt>
                        <dd>Le système de fichiers en réseau est défini dans la RFC 1094.</dd>
                        <dt>NTP (Network Time Protocol)</dt>
                        <dd>Le protocole de synchronisation du temps réseau est défini dans la RFC 1305.</dd>
                        <dt>OSI (Open Systems Interconnection)</dt>
                        <dd>Un ensemble de normes de réseau développées par l'ISO, y compris le modèle de référence OSI à 7 couches.</dd>
                        <dt>OSPF (Open Shortest Path First)</dt>
                        <dd>Un protocole de routage interne basé sur l'état des liens qui est souvent utilisé dans les réseaux d'entreprise et les réseaux ISP. OSPF est défini dans la RFC 2328 et la RFC 5340.</dd>
                        <dt>packet</dt>
                        <dd>Un paquet est l'unité de transfert d'informations dans la couche réseau.</dd>
                        <dt>PBL (Probleme-Based Learning)</dt>
                        <dd>L'apprentissage basé sur les problèmes est une approche pédagogique qui repose sur des problèmes.</dd>
                        <dt>POP (Post Office Protocol)</dt>
                        <dd>Le protocole de la Poste, défini dans la RFC 1939, est un protocole de niveau application qui permet à un client de télécharger des messages électroniques stockés sur un serveur.</dd>
                        <dt>resolver</dt>
                        <dd>Un serveur qui implémente le protocole DNS et peut résoudre des requêtes. Un resolver sert généralement un ensemble de clients (par exemple, tous les hôtes d'un campus ou tous les clients d'un FAI). Il envoie des requêtes DNS à des serveurs de noms partout au nom de ses clients et stocke les réponses reçues dans son cache. Un resolver doit connaître les adresses IP des serveurs de noms racine.</dd>
                        <dt>RIP (Routing Information Protocol)</dt>
                        <dd>Le protocole d'information de routage est un protocole de routage interne basé sur les vecteurs de distance qui est parfois utilisé dans les réseaux d'entreprise. RIP est défini dans la RFC 2453.</dd>
                        <dt>RIR (Regional Internet Registry)</dt>
                        <dd>Le registre Internet régional est une organisation qui gère les adresses IP et les numéros AS au nom de l'IANA.</dd>
                        <dt>root nameserver</dt>
                        <dd>Un serveur de noms racine est responsable de la racine de la hiérarchie des noms de domaine. Il existe actuellement une douzaine de serveurs de noms racine et chaque résolveur DNS. Voir <a href="http://www.root-servers.org/" target="_blank">http://www.root-servers.org/</a> pour plus d'informations sur le fonctionnement de ces serveurs racine</dd>
                        <dt>RTT (round-trip-time)</dt>
                        <dd>Le temps aller-retour est le délai entre la transmission d'un segment et la réception de l'accusé de réception correspondant dans un protocole de transport.</dd>
                        <dt>routeur</dt>
                        <dd>Un relais fonctionnant dans la couche réseau.</dd>
                        <dt>RPC (Remote Procedure Calls)</dt>
                        <dd>Plusieurs types d'appels de procédure à distance ont été définis. Le mécanisme RPC défini dans la RFC 5531 est utilisé par des applications telles que NFS.</dd>
                        <dt>SDU (Service Data Unit)</dt>
                        <dd>Une unité de données de service est l'unité d'information transférée entre les applications.</dd>
                        <dt>Segment</dt>
                        <dd>Un segment est l'unité de transfert d'information dans la couche de transport.</dd>
                        <dt>SMTP (Simple Mail Transfer Protocol)</dt>
                        <dd>Le protocle simple de transfert de courrier électronique est défini dans la RFC 821.</dd>
                        <dt>SNMP (Simple Network Management Protocol)</dt>
                        <dd>Le protocole somple de gestion de réseau est un protocole de gestion défini les réseaux TCP/IP.</dd>
                        <dt>socket</dt>
                        <dd>Une interface de programmation de bas niveau initialment définie sur Berkeley Unix pour permettre aux programmeurs de développer des clients et des serveurs.</dd>
                        <dt>spoofed packet</dt>
                        <dd>Un paquet est considéré comme étant falsifié lorsqu'il est envoyé avec une adresse source différente de celle de l'expéditeur.</dd>
                        <dt>SSH (Secure Shell)</dt>
                        <dd>Le protocole de couche de transport Secure Shell est défini dans la RFC 4253.</dd>
                        <dt>standard query</dt>
                        <dd>Pour les serveurs DNS et les résolveurs, une requête standard est une requête pour un enregistrement <span class="em">A</span> ou <span class="em">AAAA</span>. Une telle requête renvoie généralemnt une adresse IP.</dd>
                        <dt>switch</dt>
                        <dd>Un relais opérant dans la couche de liaison de données.</dd>
                        <dt>SYN cookie</dt>
                        <dd>Les cookies SYN sont une technique utilisée pour calculer le numéro de séquence initial (ISN).</dd>
                        <dt>TCB (Transmission Control Block)</dt>
                        <dd>Le bloc de contrôle de transmission (TCB) est l'ensemble des variables maintenues pour chaque connexion TCP établie par une implémentation TCP.</dd>
                        <dt>TCP (Transmission Control Protocol)</dt>
                        <dd>Le protocole de contrôle de transmission (TCP) est un protocole de la couche de transport dans la suite de protocoles TCP/IP qui fournit un service de connexion fiable en flux d'octets sur IP.</dd>
                        <dt>TCP/IP</dt>
                        <dd>Cela fait référence aux protocoles <span class="em">TCP</span> et <span class="em">IP</span>.</dd>
                        <dt>telnet</dt>
                        <dd>Le protocole telnet est défini dans la RFC 854.</dd>
                        <dt>TLD (Top-level domain name)</dt>
                        <dd>Il exits deux types de nom de domaine de premier niveau (TLD). Les ccTLD correspondent à un code pays ISO-3166 composé de deux lettres. Les gTLD sont des TLD génériques qui ne pas assignés à un pays spécifique.</dd>
                        <dt>TLS (Transport Layer Security)</dt>
                        <dd>Le TLS, défini dans la RFC 5246, est un protocole cryptographique utilisé pour assurer la sécurité des communications des applications Internet. Ce protocole est utilisé au-dessus du service de transport, mais une description détaillée est hors du cadre de ce livre.</dd>
                        <dt>UDP (User Datagram Protocol)</dt>
                        <dd>Protocole de la couche de transport dans la suite de protocole TCP/IP qui fournit un service de connexion sans fiabilité incluant un mécanisme de détection de cooruption.</dd>
                        <dt>unicast</dt>
                        <dd>Mode de transmission où une information est envoyée d'une source à un destinataire.</dd>
                        <dt>vnc (Virtual Network Computing)</dt>
                        <dd>Une application en réseau qui permet d'accéder à distance à l'interface utilisateur graphique (GUI) d'un ordinateur. Voir <a href="http://en.wikipedia.org/wiki/Virtual_Network_Computing" target="_blank">http://en.wikipedia.org/wiki/Virtual_Network_Computing</a>.</dd>
                        <dt>W3C (World Wide Web Consortium)</dt>
                        <dd>Il a été créé pour standardiser les protocoles et mécanismes utilisés dans le World Wide Web mondial. Il est donc axé sur une partie de la couche application. Voir <a href="http://www.w3c.org/" target="_blank">http://www.w3c.org/</a>.</dd>
                        <dt>WAN (Wide Area Network)</dt>
                        <dd>Réseau étendu.</dd>
                        <dt>X.25</dt>
                        <dd>Une technologie de réseau étendu utilisant des circuits virtuels qui était déployée par les opérateurs de télécommunications.</dd>
                        <dt>X11</dt>
                        <dd>Le système XWindow et les protocoles associés sont définis dans [SG1990].</dd>
                        <dt>XML (eXtensible Markup Language)</dt>
                        <dd>Le langage de balisage extensible (XML) est un format de texte flexible dérivé du SGML. Il a été initialement conçu pour l'industrie de l'édition électroniqe, mais il est maintenant utilisé par une grande variété d'applications nécessitant l'échange de données structurées. Les spécifications XML sont maintenues par plusieurs groupes de travail du W3C.</dd>
                    </dl>
                    <h4>7.2 Bibliographie :</h4>
                    <p>Dans la mesure du possible, la biliographie inclut des liens hypertexte stables vers les références citées.</p>
                    <h4>7.3 Indexs et tables :</h4>
                    <ul>
                        <li>
                            <p>genindex</p>
                        </li>
                        <li>
                            <p>search</p>
                        </li>
                    </ul>
                </article>
            <!-- fin de section -->
            </section>
        </main>
        <footer role="contentinfo">
            <div>
                <address>
                    Professeurs :
                    <a class="mail" href="mailto:olivier.choquet@vinci.be">Choquet Olivier</a>
                    <a class="mail" href="mailto:driss.vandenheede.techinfo@gmail.com">Vandenheede Driss</a>
                </address>
            </div>
            <div>
                <p>Si vous détectez une erreur, une faute d'orthographe, n'hésitez pas à envoyer un mail aux adresses juste à côté avec le nom de la page et l'erreur détectée.</p>
            </div>
            <div id="version">
                <p>Syllabus HTML</p>
                <p>Version 5.0</p>
            </div>
        </footer>
    </body>
    <script src="../pre.js"></script>
</html>